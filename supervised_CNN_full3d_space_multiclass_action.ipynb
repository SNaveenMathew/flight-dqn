{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section: Initialization\n",
    "\n",
    "- setting values for global variables\n",
    "- loading required packages\n",
    "- reading files\n",
    "- setting up logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for supervised learning using previous state to predict current action\n",
    "# Code from gym-flight to be reused as much as possible\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../gym-flight/')\n",
    "import gym_flight\n",
    "from gym_flight.utils.numpy_util import Sparse3dArray\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from multiclass import *\n",
    "from full3d_util import get_last_ts_data, create_space_ts, get_range_df, get_action_discrete, get_env_action_aircraft, get_X_Y, fix_XY, unlist, bind, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(\n",
    "    filename='log.txt',\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")\n",
    "x_length = 100\n",
    "y_length = 100\n",
    "z_length = 100\n",
    "ts_range = 40\n",
    "time_dim = 1\n",
    "state_dim = [x_length, y_length, z_length]\n",
    "action_size = 36\n",
    "learning_rate = 0.00025\n",
    "flight_data_path = \"../gym-flight/data/processed_jfk.csv\"\n",
    "dtype_dict = {\"id\": str, \"ts\": np.int16, \"lat\": np.float32, \"lon\": np.float32, \"altitude\": np.float32, \"speed\": np.float32, \"x\": np.int16, \"y\": np.int16, \"z\": np.int16, \"is_landing\": np.int8}\n",
    "flight_data = pd.read_csv(flight_data_path, dtype = dtype_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section on creating space for timestamp and aircraft-timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('ts_full3d_space.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiprocessing the creation of space at unique timestamp and caching the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import pickle\n",
    "splits = mp.cpu_count() - 1\n",
    "if os.path.isfile('ts_full3d_space.bin'):\n",
    "    space_X = pickle.load(open('ts_full3d_space.bin', 'rb'))\n",
    "else:\n",
    "    p = mp.Pool(processes = splits)\n",
    "    # Arranging in reverse order to speed up computation\n",
    "    rng = list(reversed(range(min(flight_data['ts']), max(flight_data['ts']) + 1)))\n",
    "    split_rng = np.array_split(rng, splits)\n",
    "    pool_results = p.map(get_range_df, split_rng)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    space_X = np.concatenate(pool_results, axis = 0)\n",
    "    space_X = np.flip(space_X, axis = 0)\n",
    "    pickle.dump(space_X, open(\"ts_full3d_space.bin\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1459"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(space_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calls functions to obtain the sparse array of environments for each aircraft-timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('X_train_full3d_space_multiclass_action.bin'):\n",
    "    X_train = pickle.load(open('X_train_full3d_space_multiclass_action.bin', 'rb'))\n",
    "    Y_train = pickle.load(open('Y_train_full3d_space_multiclass_action.bin', 'rb'))\n",
    "    X_test = pickle.load(open('X_test_full3d_space_multiclass_action.bin', 'rb'))\n",
    "    Y_test = pickle.load(open('Y_test_full3d_space_multiclass_action.bin', 'rb'))\n",
    "else:\n",
    "    uniq_id = flight_data['id'].unique()\n",
    "    #     uniq_id = uniq_id[:50]\n",
    "    XY = get_X_Y(uniq_id, flight_data, x_length, y_length, z_length, space_X, discrete_action = True)\n",
    "    X, Y = fix_XY(XY)\n",
    "    X_train, Y_train, X_test, Y_test = train_test_split(X, Y)\n",
    "    pickle.dump(X_train, open(\"X_train_full3d_space_multiclass_action.bin\", \"wb\"))\n",
    "    pickle.dump(Y_train, open(\"Y_train_full3d_space_multiclass_action.bin\", \"wb\"))\n",
    "    pickle.dump(X_test, open(\"X_test_full3d_space_multiclass_action.bin\", \"wb\"))\n",
    "    pickle.dump(Y_test, open(\"Y_test_full3d_space_multiclass_action.bin\", \"wb\"))\n",
    "\n",
    "train_ac_index = [len(X[0]) for X in X_train]\n",
    "test_ac_index = [len(X[0]) for X in X_test]\n",
    "X_train = unlist(unlist(X_train))\n",
    "Y_train = bind(unlist(Y_train))\n",
    "Y_train = Y_train.astype(np.int16)\n",
    "X_test = unlist(unlist(X_test))\n",
    "Y_test = bind(unlist(Y_test))\n",
    "Y_test = Y_test.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section on CNN, prediction, training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/atc/notebooks/flight-dqn/multiclass.py:15: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv3d instead.\n",
      "WARNING:tensorflow:From /home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/atc/notebooks/flight-dqn/multiclass.py:24: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /home/atc/notebooks/flight-dqn/multiclass.py:27: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "state_dim = [x_length, y_length, z_length]\n",
    "cnn = CNN_multiclass(state_dim, time_dim, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section on using the CNN class and data for training, prediction and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# unique_ts = flight_data['ts'].unique()\n",
    "# for i in range(len(unique_ts)):\n",
    "#     train_X.append(create_space_ts(ts = unique_ts.iloc[i], flight_data = flight_data, x_length = x_length, y_length = y_length, z_length = z_length, ts_range = ts_range))\n",
    "\n",
    "# Row binding all the arrays\n",
    "# train_X = np.concatenate(train_X, axis = 0)\n",
    "perc = 0\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "num_batches = len(X_train)//batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4459"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1/4459:  Train loss: 3.5838890075683594  Test loss: 3.581315279006958 \n",
      "Epoch: 1/10:  mini-batch 2/4459:  Train loss: 3.5826973915100098  Test loss: 3.5760655403137207 \n",
      "Epoch: 1/10:  mini-batch 3/4459:  Train loss: 3.584693670272827  Test loss: 3.575218677520752 \n",
      "Epoch: 1/10:  mini-batch 4/4459:  Train loss: 3.5847012996673584  Test loss: 3.575793743133545 \n",
      "Epoch: 1/10:  mini-batch 5/4459:  Train loss: 3.5747973918914795  Test loss: 3.574061870574951 \n",
      "Epoch: 1/10:  mini-batch 6/4459:  Train loss: 3.5790867805480957  Test loss: 3.570615530014038 \n",
      "Epoch: 1/10:  mini-batch 7/4459:  Train loss: 3.5394880771636963  Test loss: 3.564418315887451 \n",
      "Epoch: 1/10:  mini-batch 8/4459:  Train loss: 3.557560920715332  Test loss: 3.555276393890381 \n",
      "Epoch: 1/10:  mini-batch 9/4459:  Train loss: 3.5063180923461914  Test loss: 3.5420680046081543 \n",
      "Epoch: 1/10:  mini-batch 10/4459:  Train loss: 3.424839496612549  Test loss: 3.52697491645813 \n",
      "Epoch: 1/10:  mini-batch 11/4459:  Train loss: 3.291048049926758  Test loss: 3.5248658657073975 \n",
      "Epoch: 1/10:  mini-batch 12/4459:  Train loss: 3.4558498859405518  Test loss: 3.5598716735839844 \n",
      "Epoch: 1/10:  mini-batch 13/4459:  Train loss: 3.32991361618042  Test loss: 3.662527561187744 \n",
      "Epoch: 1/10:  mini-batch 14/4459:  Train loss: 2.96691632270813  Test loss: 3.879483222961426 \n",
      "Epoch: 1/10:  mini-batch 15/4459:  Train loss: 3.2694995403289795  Test loss: 4.087907791137695 \n",
      "Epoch: 1/10:  mini-batch 16/4459:  Train loss: 3.2829980850219727  Test loss: 4.069024085998535 \n",
      "Epoch: 1/10:  mini-batch 17/4459:  Train loss: 4.146688938140869  Test loss: 3.8454368114471436 \n",
      "Epoch: 1/10:  mini-batch 18/4459:  Train loss: 3.7614593505859375  Test loss: 3.6580684185028076 \n",
      "Epoch: 1/10:  mini-batch 19/4459:  Train loss: 4.1378889083862305  Test loss: 3.5406534671783447 \n",
      "Epoch: 1/10:  mini-batch 20/4459:  Train loss: 3.2791943550109863  Test loss: 3.5005311965942383 \n",
      "Epoch: 1/10:  mini-batch 21/4459:  Train loss: 3.322746515274048  Test loss: 3.4907336235046387 \n",
      "Epoch: 1/10:  mini-batch 22/4459:  Train loss: 3.373824119567871  Test loss: 3.4913363456726074 \n",
      "Epoch: 1/10:  mini-batch 23/4459:  Train loss: 3.3788492679595947  Test loss: 3.4948089122772217 \n",
      "Epoch: 1/10:  mini-batch 24/4459:  Train loss: 3.3938634395599365  Test loss: 3.4978466033935547 \n",
      "Epoch: 1/10:  mini-batch 25/4459:  Train loss: 3.369816541671753  Test loss: 3.499113082885742 \n",
      "Epoch: 1/10:  mini-batch 26/4459:  Train loss: 3.349287271499634  Test loss: 3.498218059539795 \n",
      "Epoch: 1/10:  mini-batch 27/4459:  Train loss: 3.29286789894104  Test loss: 3.495414972305298 \n",
      "Epoch: 1/10:  mini-batch 28/4459:  Train loss: 3.418903350830078  Test loss: 3.4912376403808594 \n",
      "Epoch: 1/10:  mini-batch 29/4459:  Train loss: 3.6028943061828613  Test loss: 3.4863977432250977 \n",
      "Epoch: 1/10:  mini-batch 30/4459:  Train loss: 3.340956926345825  Test loss: 3.480800151824951 \n",
      "Epoch: 1/10:  mini-batch 31/4459:  Train loss: 3.364391803741455  Test loss: 3.475374460220337 \n",
      "Epoch: 1/10:  mini-batch 32/4459:  Train loss: 3.2994322776794434  Test loss: 3.4713387489318848 \n",
      "Epoch: 1/10:  mini-batch 33/4459:  Train loss: 3.394622802734375  Test loss: 3.4707984924316406 \n",
      "Epoch: 1/10:  mini-batch 34/4459:  Train loss: 3.362868309020996  Test loss: 3.4742674827575684 \n",
      "Epoch: 1/10:  mini-batch 35/4459:  Train loss: 3.212409257888794  Test loss: 3.4846959114074707 \n",
      "Epoch: 1/10:  mini-batch 36/4459:  Train loss: 3.3297910690307617  Test loss: 3.503643035888672 \n",
      "Epoch: 1/10:  mini-batch 37/4459:  Train loss: 3.029866933822632  Test loss: 3.538644313812256 \n",
      "Epoch: 1/10:  mini-batch 38/4459:  Train loss: 3.4208126068115234  Test loss: 3.5736000537872314 \n",
      "Epoch: 1/10:  mini-batch 39/4459:  Train loss: 2.961697578430176  Test loss: 3.628917694091797 \n",
      "Epoch: 1/10:  mini-batch 40/4459:  Train loss: 2.8945446014404297  Test loss: 3.7076199054718018 \n",
      "Epoch: 1/10:  mini-batch 41/4459:  Train loss: 2.7298059463500977  Test loss: 3.844550848007202 \n",
      "Epoch: 1/10:  mini-batch 42/4459:  Train loss: 3.017869472503662  Test loss: 3.987977981567383 \n",
      "Epoch: 1/10:  mini-batch 43/4459:  Train loss: 3.1405270099639893  Test loss: 4.0604352951049805 \n",
      "Epoch: 1/10:  mini-batch 44/4459:  Train loss: 3.085209608078003  Test loss: 4.087363243103027 \n",
      "Epoch: 1/10:  mini-batch 45/4459:  Train loss: 2.9479804039001465  Test loss: 4.098176956176758 \n",
      "Epoch: 1/10:  mini-batch 46/4459:  Train loss: 2.6962900161743164  Test loss: 4.091189861297607 \n",
      "Epoch: 1/10:  mini-batch 47/4459:  Train loss: 2.094259023666382  Test loss: 4.160687446594238 \n",
      "Epoch: 1/10:  mini-batch 48/4459:  Train loss: 2.609394073486328  Test loss: 4.240532875061035 \n",
      "Epoch: 1/10:  mini-batch 49/4459:  Train loss: 3.198823928833008  Test loss: 4.237615585327148 \n",
      "Epoch: 1/10:  mini-batch 50/4459:  Train loss: 2.9545464515686035  Test loss: 4.167961120605469 \n",
      "Epoch: 1/10:  mini-batch 51/4459:  Train loss: 2.356895685195923  Test loss: 4.143306255340576 \n",
      "Epoch: 1/10:  mini-batch 52/4459:  Train loss: 2.782573938369751  Test loss: 4.101014614105225 \n",
      "Epoch: 1/10:  mini-batch 53/4459:  Train loss: 2.4712235927581787  Test loss: 4.085386276245117 \n",
      "Epoch: 1/10:  mini-batch 54/4459:  Train loss: 2.867518424987793  Test loss: 4.044749736785889 \n",
      "Epoch: 1/10:  mini-batch 55/4459:  Train loss: 2.61447811126709  Test loss: 4.02215051651001 \n",
      "Epoch: 1/10:  mini-batch 56/4459:  Train loss: 2.3724863529205322  Test loss: 4.041058540344238 \n",
      "Epoch: 1/10:  mini-batch 57/4459:  Train loss: 2.566572666168213  Test loss: 4.0683770179748535 \n",
      "Epoch: 1/10:  mini-batch 58/4459:  Train loss: 2.3645243644714355  Test loss: 4.132626533508301 \n",
      "Epoch: 1/10:  mini-batch 59/4459:  Train loss: 3.0606133937835693  Test loss: 4.127721309661865 \n",
      "Epoch: 1/10:  mini-batch 60/4459:  Train loss: 2.2949254512786865  Test loss: 4.186418533325195 \n",
      "Epoch: 1/10:  mini-batch 61/4459:  Train loss: 3.0889029502868652  Test loss: 4.17021369934082 \n",
      "Epoch: 1/10:  mini-batch 62/4459:  Train loss: 3.791065216064453  Test loss: 4.0232977867126465 \n",
      "Epoch: 1/10:  mini-batch 63/4459:  Train loss: 2.1976847648620605  Test loss: 3.948012590408325 \n",
      "Epoch: 1/10:  mini-batch 64/4459:  Train loss: 3.113563060760498  Test loss: 3.8636748790740967 \n",
      "Epoch: 1/10:  mini-batch 65/4459:  Train loss: 2.9583137035369873  Test loss: 3.7940001487731934 \n",
      "Epoch: 1/10:  mini-batch 66/4459:  Train loss: 2.741844654083252  Test loss: 3.755441665649414 \n",
      "Epoch: 1/10:  mini-batch 67/4459:  Train loss: 2.7319459915161133  Test loss: 3.7443339824676514 \n",
      "Epoch: 1/10:  mini-batch 68/4459:  Train loss: 2.5802741050720215  Test loss: 3.7624895572662354 \n",
      "Epoch: 1/10:  mini-batch 69/4459:  Train loss: 2.7078731060028076  Test loss: 3.799856662750244 \n",
      "Epoch: 1/10:  mini-batch 70/4459:  Train loss: 2.4162604808807373  Test loss: 3.873689651489258 \n",
      "Epoch: 1/10:  mini-batch 71/4459:  Train loss: 2.2359185218811035  Test loss: 4.0005316734313965 \n",
      "Epoch: 1/10:  mini-batch 72/4459:  Train loss: 2.49078369140625  Test loss: 4.167973518371582 \n",
      "Epoch: 1/10:  mini-batch 73/4459:  Train loss: 2.852792263031006  Test loss: 4.337526321411133 \n",
      "Epoch: 1/10:  mini-batch 74/4459:  Train loss: 2.15556263923645  Test loss: 4.561511516571045 \n",
      "Epoch: 1/10:  mini-batch 75/4459:  Train loss: 2.984652042388916  Test loss: 4.678798198699951 \n",
      "Epoch: 1/10:  mini-batch 76/4459:  Train loss: 3.246852397918701  Test loss: 4.634793281555176 \n",
      "Epoch: 1/10:  mini-batch 77/4459:  Train loss: 2.560102939605713  Test loss: 4.560342311859131 \n",
      "Epoch: 1/10:  mini-batch 78/4459:  Train loss: 1.9758198261260986  Test loss: 4.54164457321167 \n",
      "Epoch: 1/10:  mini-batch 79/4459:  Train loss: 2.729537010192871  Test loss: 4.497664451599121 \n",
      "Epoch: 1/10:  mini-batch 80/4459:  Train loss: 2.9067375659942627  Test loss: 4.416107654571533 \n",
      "Epoch: 1/10:  mini-batch 81/4459:  Train loss: 2.8632936477661133  Test loss: 4.305519104003906 \n",
      "Epoch: 1/10:  mini-batch 82/4459:  Train loss: 2.4121298789978027  Test loss: 4.242256164550781 \n",
      "Epoch: 1/10:  mini-batch 83/4459:  Train loss: 2.3826959133148193  Test loss: 4.217323303222656 \n",
      "Epoch: 1/10:  mini-batch 84/4459:  Train loss: 2.4672317504882812  Test loss: 4.218822479248047 \n",
      "Epoch: 1/10:  mini-batch 85/4459:  Train loss: 2.3086161613464355  Test loss: 4.256810188293457 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 86/4459:  Train loss: 2.7636795043945312  Test loss: 4.28445291519165 \n",
      "Epoch: 1/10:  mini-batch 87/4459:  Train loss: 2.0503787994384766  Test loss: 4.3655853271484375 \n",
      "Epoch: 1/10:  mini-batch 88/4459:  Train loss: 2.361149787902832  Test loss: 4.476391792297363 \n",
      "Epoch: 1/10:  mini-batch 89/4459:  Train loss: 2.503573179244995  Test loss: 4.589815139770508 \n",
      "Epoch: 1/10:  mini-batch 90/4459:  Train loss: 2.6895573139190674  Test loss: 4.667757987976074 \n",
      "Epoch: 1/10:  mini-batch 91/4459:  Train loss: 2.7669246196746826  Test loss: 4.691670894622803 \n",
      "Epoch: 1/10:  mini-batch 92/4459:  Train loss: 2.1918773651123047  Test loss: 4.73465633392334 \n",
      "Epoch: 1/10:  mini-batch 93/4459:  Train loss: 2.2633485794067383  Test loss: 4.794447898864746 \n",
      "Epoch: 1/10:  mini-batch 94/4459:  Train loss: 3.5106050968170166  Test loss: 4.693515300750732 \n",
      "Epoch: 1/10:  mini-batch 95/4459:  Train loss: 2.367588996887207  Test loss: 4.6100993156433105 \n",
      "Epoch: 1/10:  mini-batch 96/4459:  Train loss: 1.942674994468689  Test loss: 4.598945140838623 \n",
      "Epoch: 1/10:  mini-batch 97/4459:  Train loss: 2.738962411880493  Test loss: 4.5620198249816895 \n",
      "Epoch: 1/10:  mini-batch 98/4459:  Train loss: 2.811953544616699  Test loss: 4.489837169647217 \n",
      "Epoch: 1/10:  mini-batch 99/4459:  Train loss: 1.8351292610168457  Test loss: 4.500059127807617 \n",
      "Epoch: 1/10:  mini-batch 100/4459:  Train loss: 2.1967179775238037  Test loss: 4.543903827667236 \n",
      "Epoch: 1/10:  mini-batch 101/4459:  Train loss: 2.9627232551574707  Test loss: 4.521750450134277 \n",
      "Epoch: 1/10:  mini-batch 102/4459:  Train loss: 2.5762276649475098  Test loss: 4.496879577636719 \n",
      "Epoch: 1/10:  mini-batch 103/4459:  Train loss: 2.763740062713623  Test loss: 4.449222564697266 \n",
      "Epoch: 1/10:  mini-batch 104/4459:  Train loss: 2.3000354766845703  Test loss: 4.433464050292969 \n",
      "Epoch: 1/10:  mini-batch 105/4459:  Train loss: 2.752079486846924  Test loss: 4.399294853210449 \n",
      "Epoch: 1/10:  mini-batch 106/4459:  Train loss: 2.6450438499450684  Test loss: 4.363717555999756 \n",
      "Epoch: 1/10:  mini-batch 107/4459:  Train loss: 3.0428826808929443  Test loss: 4.300833225250244 \n",
      "Epoch: 1/10:  mini-batch 108/4459:  Train loss: 2.421738386154175  Test loss: 4.267660140991211 \n",
      "Epoch: 1/10:  mini-batch 109/4459:  Train loss: 1.8534594774246216  Test loss: 4.313363552093506 \n",
      "Epoch: 1/10:  mini-batch 110/4459:  Train loss: 1.9152257442474365  Test loss: 4.428998947143555 \n",
      "Epoch: 1/10:  mini-batch 111/4459:  Train loss: 2.2214486598968506  Test loss: 4.580197334289551 \n",
      "Epoch: 1/10:  mini-batch 112/4459:  Train loss: 2.6515536308288574  Test loss: 4.692241668701172 \n",
      "Epoch: 1/10:  mini-batch 113/4459:  Train loss: 2.8189401626586914  Test loss: 4.742420196533203 \n",
      "Epoch: 1/10:  mini-batch 114/4459:  Train loss: 2.1967310905456543  Test loss: 4.818019866943359 \n",
      "Epoch: 1/10:  mini-batch 115/4459:  Train loss: 1.9857676029205322  Test loss: 4.957736492156982 \n",
      "Epoch: 1/10:  mini-batch 116/4459:  Train loss: 2.906975746154785  Test loss: 4.9665913581848145 \n",
      "Epoch: 1/10:  mini-batch 117/4459:  Train loss: 2.3152928352355957  Test loss: 4.9476094245910645 \n",
      "Epoch: 1/10:  mini-batch 118/4459:  Train loss: 3.3273673057556152  Test loss: 4.774084091186523 \n",
      "Epoch: 1/10:  mini-batch 119/4459:  Train loss: 3.4155099391937256  Test loss: 4.506888389587402 \n",
      "Epoch: 1/10:  mini-batch 120/4459:  Train loss: 2.175246477127075  Test loss: 4.337062358856201 \n",
      "Epoch: 1/10:  mini-batch 121/4459:  Train loss: 2.424025058746338  Test loss: 4.218958854675293 \n",
      "Epoch: 1/10:  mini-batch 122/4459:  Train loss: 2.793417453765869  Test loss: 4.116811275482178 \n",
      "Epoch: 1/10:  mini-batch 123/4459:  Train loss: 2.641899585723877  Test loss: 4.046558856964111 \n",
      "Epoch: 1/10:  mini-batch 124/4459:  Train loss: 2.7151689529418945  Test loss: 3.999763250350952 \n",
      "Epoch: 1/10:  mini-batch 125/4459:  Train loss: 2.8106651306152344  Test loss: 3.966705322265625 \n",
      "Epoch: 1/10:  mini-batch 126/4459:  Train loss: 3.1211471557617188  Test loss: 3.928152322769165 \n",
      "Epoch: 1/10:  mini-batch 127/4459:  Train loss: 2.573148727416992  Test loss: 3.919034004211426 \n",
      "Epoch: 1/10:  mini-batch 128/4459:  Train loss: 3.067869186401367  Test loss: 3.906167984008789 \n",
      "Epoch: 1/10:  mini-batch 129/4459:  Train loss: 2.5618515014648438  Test loss: 3.918724775314331 \n",
      "Epoch: 1/10:  mini-batch 130/4459:  Train loss: 2.614833116531372  Test loss: 3.9522018432617188 \n",
      "Epoch: 1/10:  mini-batch 131/4459:  Train loss: 2.8629612922668457  Test loss: 3.9898550510406494 \n",
      "Epoch: 1/10:  mini-batch 132/4459:  Train loss: 2.705388307571411  Test loss: 4.0452752113342285 \n",
      "Epoch: 1/10:  mini-batch 133/4459:  Train loss: 2.536961555480957  Test loss: 4.122623443603516 \n",
      "Epoch: 1/10:  mini-batch 134/4459:  Train loss: 2.7908473014831543  Test loss: 4.199542045593262 \n",
      "Epoch: 1/10:  mini-batch 135/4459:  Train loss: 2.73213791847229  Test loss: 4.274513244628906 \n",
      "Epoch: 1/10:  mini-batch 136/4459:  Train loss: 2.6431050300598145  Test loss: 4.348568439483643 \n",
      "Epoch: 1/10:  mini-batch 137/4459:  Train loss: 2.772491455078125  Test loss: 4.403186798095703 \n",
      "Epoch: 1/10:  mini-batch 138/4459:  Train loss: 2.4994754791259766  Test loss: 4.473950386047363 \n",
      "Epoch: 1/10:  mini-batch 139/4459:  Train loss: 2.2802157402038574  Test loss: 4.56566858291626 \n",
      "Epoch: 1/10:  mini-batch 140/4459:  Train loss: 2.734179735183716  Test loss: 4.639174461364746 \n",
      "Epoch: 1/10:  mini-batch 141/4459:  Train loss: 2.2959532737731934  Test loss: 4.74815034866333 \n",
      "Epoch: 1/10:  mini-batch 142/4459:  Train loss: 2.223921298980713  Test loss: 4.874905586242676 \n",
      "Epoch: 1/10:  mini-batch 143/4459:  Train loss: 2.436558723449707  Test loss: 4.989739418029785 \n",
      "Epoch: 1/10:  mini-batch 144/4459:  Train loss: 2.2160019874572754  Test loss: 5.120023727416992 \n",
      "Epoch: 1/10:  mini-batch 145/4459:  Train loss: 2.315784454345703  Test loss: 5.23054313659668 \n",
      "Epoch: 1/10:  mini-batch 146/4459:  Train loss: 2.0249693393707275  Test loss: 5.353930473327637 \n",
      "Epoch: 1/10:  mini-batch 147/4459:  Train loss: 3.2367444038391113  Test loss: 5.269087791442871 \n",
      "Epoch: 1/10:  mini-batch 148/4459:  Train loss: 2.7001523971557617  Test loss: 5.103729248046875 \n",
      "Epoch: 1/10:  mini-batch 149/4459:  Train loss: 2.7929797172546387  Test loss: 4.900294303894043 \n",
      "Epoch: 1/10:  mini-batch 150/4459:  Train loss: 3.2222695350646973  Test loss: 4.629034519195557 \n",
      "Epoch: 1/10:  mini-batch 151/4459:  Train loss: 3.292693614959717  Test loss: 4.356634616851807 \n",
      "Epoch: 1/10:  mini-batch 152/4459:  Train loss: 3.4265682697296143  Test loss: 4.112102508544922 \n",
      "Epoch: 1/10:  mini-batch 153/4459:  Train loss: 3.115175724029541  Test loss: 3.9397263526916504 \n",
      "Epoch: 1/10:  mini-batch 154/4459:  Train loss: 2.892256259918213  Test loss: 3.838019609451294 \n",
      "Epoch: 1/10:  mini-batch 155/4459:  Train loss: 2.9101357460021973  Test loss: 3.778198480606079 \n",
      "Epoch: 1/10:  mini-batch 156/4459:  Train loss: 2.8232383728027344  Test loss: 3.7484514713287354 \n",
      "Epoch: 1/10:  mini-batch 157/4459:  Train loss: 2.856159210205078  Test loss: 3.737987518310547 \n",
      "Epoch: 1/10:  mini-batch 158/4459:  Train loss: 2.9576587677001953  Test loss: 3.7387502193450928 \n",
      "Epoch: 1/10:  mini-batch 159/4459:  Train loss: 2.9198875427246094  Test loss: 3.7495105266571045 \n",
      "Epoch: 1/10:  mini-batch 160/4459:  Train loss: 3.3933680057525635  Test loss: 3.7603297233581543 \n",
      "Epoch: 1/10:  mini-batch 161/4459:  Train loss: 2.7541232109069824  Test loss: 3.7855570316314697 \n",
      "Epoch: 1/10:  mini-batch 162/4459:  Train loss: 2.7578330039978027  Test loss: 3.8269083499908447 \n",
      "Epoch: 1/10:  mini-batch 163/4459:  Train loss: 2.7719571590423584  Test loss: 3.8870136737823486 \n",
      "Epoch: 1/10:  mini-batch 164/4459:  Train loss: 2.8860864639282227  Test loss: 3.9644320011138916 \n",
      "Epoch: 1/10:  mini-batch 165/4459:  Train loss: 2.6829628944396973  Test loss: 4.065888404846191 \n",
      "Epoch: 1/10:  mini-batch 166/4459:  Train loss: 2.840343713760376  Test loss: 4.189897537231445 \n",
      "Epoch: 1/10:  mini-batch 167/4459:  Train loss: 2.525331974029541  Test loss: 4.347898483276367 \n",
      "Epoch: 1/10:  mini-batch 168/4459:  Train loss: 2.6225662231445312  Test loss: 4.527871131896973 \n",
      "Epoch: 1/10:  mini-batch 169/4459:  Train loss: 2.3906333446502686  Test loss: 4.736100196838379 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 170/4459:  Train loss: 2.57767391204834  Test loss: 4.942663192749023 \n",
      "Epoch: 1/10:  mini-batch 171/4459:  Train loss: 2.6753015518188477  Test loss: 5.1206440925598145 \n",
      "Epoch: 1/10:  mini-batch 172/4459:  Train loss: 2.528700828552246  Test loss: 5.270549297332764 \n",
      "Epoch: 1/10:  mini-batch 173/4459:  Train loss: 2.5182907581329346  Test loss: 5.384564399719238 \n",
      "Epoch: 1/10:  mini-batch 174/4459:  Train loss: 2.585728168487549  Test loss: 5.441375732421875 \n",
      "Epoch: 1/10:  mini-batch 175/4459:  Train loss: 2.5374345779418945  Test loss: 5.448753356933594 \n",
      "Epoch: 1/10:  mini-batch 176/4459:  Train loss: 2.5578830242156982  Test loss: 5.398690700531006 \n",
      "Epoch: 1/10:  mini-batch 177/4459:  Train loss: 2.4982380867004395  Test loss: 5.318838119506836 \n",
      "Epoch: 1/10:  mini-batch 178/4459:  Train loss: 2.7446742057800293  Test loss: 5.187656879425049 \n",
      "Epoch: 1/10:  mini-batch 179/4459:  Train loss: 2.4412002563476562  Test loss: 5.058216571807861 \n",
      "Epoch: 1/10:  mini-batch 180/4459:  Train loss: 2.814417839050293  Test loss: 4.904143333435059 \n",
      "Epoch: 1/10:  mini-batch 181/4459:  Train loss: 2.4952473640441895  Test loss: 4.774660110473633 \n",
      "Epoch: 1/10:  mini-batch 182/4459:  Train loss: 2.534212350845337  Test loss: 4.6664509773254395 \n",
      "Epoch: 1/10:  mini-batch 183/4459:  Train loss: 2.3872885704040527  Test loss: 4.591870307922363 \n",
      "Epoch: 1/10:  mini-batch 184/4459:  Train loss: 2.5011401176452637  Test loss: 4.538105010986328 \n",
      "Epoch: 1/10:  mini-batch 185/4459:  Train loss: 2.6509971618652344  Test loss: 4.49556827545166 \n",
      "Epoch: 1/10:  mini-batch 186/4459:  Train loss: 2.7539124488830566  Test loss: 4.453192710876465 \n",
      "Epoch: 1/10:  mini-batch 187/4459:  Train loss: 3.0123302936553955  Test loss: 4.403765678405762 \n",
      "Epoch: 1/10:  mini-batch 188/4459:  Train loss: 3.060089588165283  Test loss: 4.347600936889648 \n",
      "Epoch: 1/10:  mini-batch 189/4459:  Train loss: 2.506427764892578  Test loss: 4.32691764831543 \n",
      "Epoch: 1/10:  mini-batch 190/4459:  Train loss: 2.8872671127319336  Test loss: 4.316058158874512 \n",
      "Epoch: 1/10:  mini-batch 191/4459:  Train loss: 2.826509952545166  Test loss: 4.311111927032471 \n",
      "Epoch: 1/10:  mini-batch 192/4459:  Train loss: 3.8047428131103516  Test loss: 4.25233793258667 \n",
      "Epoch: 1/10:  mini-batch 193/4459:  Train loss: 3.003687858581543  Test loss: 4.1959967613220215 \n",
      "Epoch: 1/10:  mini-batch 194/4459:  Train loss: 2.4472880363464355  Test loss: 4.172242164611816 \n",
      "Epoch: 1/10:  mini-batch 195/4459:  Train loss: 3.116739273071289  Test loss: 4.142077445983887 \n",
      "Epoch: 1/10:  mini-batch 196/4459:  Train loss: 3.669252634048462  Test loss: 4.090176582336426 \n",
      "Epoch: 1/10:  mini-batch 197/4459:  Train loss: 3.23219895362854  Test loss: 4.036567211151123 \n",
      "Epoch: 1/10:  mini-batch 198/4459:  Train loss: 3.987218141555786  Test loss: 3.959270477294922 \n",
      "Epoch: 1/10:  mini-batch 199/4459:  Train loss: 3.2763829231262207  Test loss: 3.8964390754699707 \n",
      "Epoch: 1/10:  mini-batch 200/4459:  Train loss: 2.750771999359131  Test loss: 3.859489679336548 \n",
      "Epoch: 1/10:  mini-batch 201/4459:  Train loss: 2.910923480987549  Test loss: 3.8386292457580566 \n",
      "Epoch: 1/10:  mini-batch 202/4459:  Train loss: 2.8408265113830566  Test loss: 3.8314149379730225 \n",
      "Epoch: 1/10:  mini-batch 203/4459:  Train loss: 2.83735990524292  Test loss: 3.835712194442749 \n",
      "Epoch: 1/10:  mini-batch 204/4459:  Train loss: 2.7998499870300293  Test loss: 3.851465940475464 \n",
      "Epoch: 1/10:  mini-batch 205/4459:  Train loss: 3.1048104763031006  Test loss: 3.8696532249450684 \n",
      "Epoch: 1/10:  mini-batch 206/4459:  Train loss: 3.7838358879089355  Test loss: 3.8716940879821777 \n",
      "Epoch: 1/10:  mini-batch 207/4459:  Train loss: 3.829277992248535  Test loss: 3.8586320877075195 \n",
      "Epoch: 1/10:  mini-batch 208/4459:  Train loss: 3.705280303955078  Test loss: 3.83646297454834 \n",
      "Epoch: 1/10:  mini-batch 209/4459:  Train loss: 3.6712942123413086  Test loss: 3.809816837310791 \n",
      "Epoch: 1/10:  mini-batch 210/4459:  Train loss: 3.2325360774993896  Test loss: 3.7896924018859863 \n",
      "Epoch: 1/10:  mini-batch 211/4459:  Train loss: 3.055757999420166  Test loss: 3.777494430541992 \n",
      "Epoch: 1/10:  mini-batch 212/4459:  Train loss: 3.4864931106567383  Test loss: 3.7639260292053223 \n",
      "Epoch: 1/10:  mini-batch 213/4459:  Train loss: 3.5631027221679688  Test loss: 3.750248908996582 \n",
      "Epoch: 1/10:  mini-batch 214/4459:  Train loss: 3.2615790367126465  Test loss: 3.741395950317383 \n",
      "Epoch: 1/10:  mini-batch 215/4459:  Train loss: 3.5256595611572266  Test loss: 3.7309165000915527 \n",
      "Epoch: 1/10:  mini-batch 216/4459:  Train loss: 3.393160820007324  Test loss: 3.7223029136657715 \n",
      "Epoch: 1/10:  mini-batch 217/4459:  Train loss: 3.2111053466796875  Test loss: 3.718306541442871 \n",
      "Epoch: 1/10:  mini-batch 218/4459:  Train loss: 2.9733223915100098  Test loss: 3.7211856842041016 \n",
      "Epoch: 1/10:  mini-batch 219/4459:  Train loss: 3.310649871826172  Test loss: 3.7250447273254395 \n",
      "Epoch: 1/10:  mini-batch 220/4459:  Train loss: 3.3648033142089844  Test loss: 3.7296948432922363 \n",
      "Epoch: 1/10:  mini-batch 221/4459:  Train loss: 3.4225387573242188  Test loss: 3.734020948410034 \n",
      "Epoch: 1/10:  mini-batch 222/4459:  Train loss: 3.5324342250823975  Test loss: 3.735431432723999 \n",
      "Epoch: 1/10:  mini-batch 223/4459:  Train loss: 3.1116228103637695  Test loss: 3.741281032562256 \n",
      "Epoch: 1/10:  mini-batch 224/4459:  Train loss: 2.913752555847168  Test loss: 3.7547783851623535 \n",
      "Epoch: 1/10:  mini-batch 225/4459:  Train loss: 2.9851062297821045  Test loss: 3.7742743492126465 \n",
      "Epoch: 1/10:  mini-batch 226/4459:  Train loss: 2.9406614303588867  Test loss: 3.8004331588745117 \n",
      "Epoch: 1/10:  mini-batch 227/4459:  Train loss: 2.8132126331329346  Test loss: 3.8376452922821045 \n",
      "Epoch: 1/10:  mini-batch 228/4459:  Train loss: 2.815265655517578  Test loss: 3.8877711296081543 \n",
      "Epoch: 1/10:  mini-batch 229/4459:  Train loss: 2.919677972793579  Test loss: 3.9481191635131836 \n",
      "Epoch: 1/10:  mini-batch 230/4459:  Train loss: 2.7415366172790527  Test loss: 4.025012016296387 \n",
      "Epoch: 1/10:  mini-batch 231/4459:  Train loss: 2.710545539855957  Test loss: 4.121056079864502 \n",
      "Epoch: 1/10:  mini-batch 232/4459:  Train loss: 2.7154958248138428  Test loss: 4.234731197357178 \n",
      "Epoch: 1/10:  mini-batch 233/4459:  Train loss: 2.834141254425049  Test loss: 4.358080863952637 \n",
      "Epoch: 1/10:  mini-batch 234/4459:  Train loss: 3.7719836235046387  Test loss: 4.430476665496826 \n",
      "Epoch: 1/10:  mini-batch 235/4459:  Train loss: 3.9130778312683105  Test loss: 4.4373931884765625 \n",
      "Epoch: 1/10:  mini-batch 236/4459:  Train loss: 4.459214687347412  Test loss: 4.353341579437256 \n",
      "Epoch: 1/10:  mini-batch 237/4459:  Train loss: 3.9734349250793457  Test loss: 4.233539581298828 \n",
      "Epoch: 1/10:  mini-batch 238/4459:  Train loss: 4.200902938842773  Test loss: 4.094447612762451 \n",
      "Epoch: 1/10:  mini-batch 239/4459:  Train loss: 4.17706298828125  Test loss: 3.962963581085205 \n",
      "Epoch: 1/10:  mini-batch 240/4459:  Train loss: 4.222896575927734  Test loss: 3.8506855964660645 \n",
      "Epoch: 1/10:  mini-batch 241/4459:  Train loss: 3.761354923248291  Test loss: 3.76943302154541 \n",
      "Epoch: 1/10:  mini-batch 242/4459:  Train loss: 3.7283811569213867  Test loss: 3.711850643157959 \n",
      "Epoch: 1/10:  mini-batch 243/4459:  Train loss: 3.744175910949707  Test loss: 3.671203136444092 \n",
      "Epoch: 1/10:  mini-batch 244/4459:  Train loss: 3.729962110519409  Test loss: 3.643049716949463 \n",
      "Epoch: 1/10:  mini-batch 245/4459:  Train loss: 3.648986577987671  Test loss: 3.623642921447754 \n",
      "Epoch: 1/10:  mini-batch 246/4459:  Train loss: 3.5601437091827393  Test loss: 3.6101741790771484 \n",
      "Epoch: 1/10:  mini-batch 247/4459:  Train loss: 3.545337438583374  Test loss: 3.6008522510528564 \n",
      "Epoch: 1/10:  mini-batch 248/4459:  Train loss: 3.5356173515319824  Test loss: 3.594187021255493 \n",
      "Epoch: 1/10:  mini-batch 249/4459:  Train loss: 3.48602294921875  Test loss: 3.5893754959106445 \n",
      "Epoch: 1/10:  mini-batch 250/4459:  Train loss: 3.593153953552246  Test loss: 3.5855765342712402 \n",
      "Epoch: 1/10:  mini-batch 251/4459:  Train loss: 3.5731825828552246  Test loss: 3.582731246948242 \n",
      "Epoch: 1/10:  mini-batch 252/4459:  Train loss: 3.50308895111084  Test loss: 3.580571413040161 \n",
      "Epoch: 1/10:  mini-batch 253/4459:  Train loss: 3.6320815086364746  Test loss: 3.5787601470947266 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 254/4459:  Train loss: 3.6145148277282715  Test loss: 3.5773234367370605 \n",
      "Epoch: 1/10:  mini-batch 255/4459:  Train loss: 3.4526467323303223  Test loss: 3.5763309001922607 \n",
      "Epoch: 1/10:  mini-batch 256/4459:  Train loss: 3.5328731536865234  Test loss: 3.575489044189453 \n",
      "Epoch: 1/10:  mini-batch 257/4459:  Train loss: 3.6433019638061523  Test loss: 3.574758529663086 \n",
      "Epoch: 1/10:  mini-batch 258/4459:  Train loss: 3.606125831604004  Test loss: 3.574162006378174 \n",
      "Epoch: 1/10:  mini-batch 259/4459:  Train loss: 3.534118175506592  Test loss: 3.573662757873535 \n",
      "Epoch: 1/10:  mini-batch 260/4459:  Train loss: 3.52670955657959  Test loss: 3.573364496231079 \n",
      "Epoch: 1/10:  mini-batch 261/4459:  Train loss: 3.479160785675049  Test loss: 3.5731165409088135 \n",
      "Epoch: 1/10:  mini-batch 262/4459:  Train loss: 3.6229052543640137  Test loss: 3.5727498531341553 \n",
      "Epoch: 1/10:  mini-batch 263/4459:  Train loss: 3.58607816696167  Test loss: 3.572467088699341 \n",
      "Epoch: 1/10:  mini-batch 264/4459:  Train loss: 3.5765833854675293  Test loss: 3.572139263153076 \n",
      "Epoch: 1/10:  mini-batch 265/4459:  Train loss: 3.5668928623199463  Test loss: 3.5718019008636475 \n",
      "Epoch: 1/10:  mini-batch 266/4459:  Train loss: 3.563255786895752  Test loss: 3.571445941925049 \n",
      "Epoch: 1/10:  mini-batch 267/4459:  Train loss: 3.482029914855957  Test loss: 3.5710697174072266 \n",
      "Epoch: 1/10:  mini-batch 268/4459:  Train loss: 3.6008694171905518  Test loss: 3.5706276893615723 \n",
      "Epoch: 1/10:  mini-batch 269/4459:  Train loss: 3.590634822845459  Test loss: 3.5702223777770996 \n",
      "Epoch: 1/10:  mini-batch 270/4459:  Train loss: 3.580836057662964  Test loss: 3.5698766708374023 \n",
      "Epoch: 1/10:  mini-batch 271/4459:  Train loss: 3.5302164554595947  Test loss: 3.569572925567627 \n",
      "Epoch: 1/10:  mini-batch 272/4459:  Train loss: 3.5967862606048584  Test loss: 3.569211483001709 \n",
      "Epoch: 1/10:  mini-batch 273/4459:  Train loss: 3.54874587059021  Test loss: 3.5688467025756836 \n",
      "Epoch: 1/10:  mini-batch 274/4459:  Train loss: 3.4580018520355225  Test loss: 3.568408727645874 \n",
      "Epoch: 1/10:  mini-batch 275/4459:  Train loss: 3.559145927429199  Test loss: 3.5679402351379395 \n",
      "Epoch: 1/10:  mini-batch 276/4459:  Train loss: 3.5662355422973633  Test loss: 3.5675740242004395 \n",
      "Epoch: 1/10:  mini-batch 277/4459:  Train loss: 3.586456775665283  Test loss: 3.5672624111175537 \n",
      "Epoch: 1/10:  mini-batch 278/4459:  Train loss: 3.553757667541504  Test loss: 3.5669264793395996 \n",
      "Epoch: 1/10:  mini-batch 279/4459:  Train loss: 3.6453585624694824  Test loss: 3.566549062728882 \n",
      "Epoch: 1/10:  mini-batch 280/4459:  Train loss: 3.565844774246216  Test loss: 3.5660691261291504 \n",
      "Epoch: 1/10:  mini-batch 281/4459:  Train loss: 3.573869466781616  Test loss: 3.5655341148376465 \n",
      "Epoch: 1/10:  mini-batch 282/4459:  Train loss: 3.652536392211914  Test loss: 3.5650229454040527 \n",
      "Epoch: 1/10:  mini-batch 283/4459:  Train loss: 3.524111032485962  Test loss: 3.564558506011963 \n",
      "Epoch: 1/10:  mini-batch 284/4459:  Train loss: 3.629399299621582  Test loss: 3.5640695095062256 \n",
      "Epoch: 1/10:  mini-batch 285/4459:  Train loss: 3.5811121463775635  Test loss: 3.56364107131958 \n",
      "Epoch: 1/10:  mini-batch 286/4459:  Train loss: 3.5723936557769775  Test loss: 3.5631284713745117 \n",
      "Epoch: 1/10:  mini-batch 287/4459:  Train loss: 3.510366916656494  Test loss: 3.5625929832458496 \n",
      "Epoch: 1/10:  mini-batch 288/4459:  Train loss: 3.4782419204711914  Test loss: 3.562028408050537 \n",
      "Epoch: 1/10:  mini-batch 289/4459:  Train loss: 3.4736881256103516  Test loss: 3.561368465423584 \n",
      "Epoch: 1/10:  mini-batch 290/4459:  Train loss: 3.4448282718658447  Test loss: 3.560603618621826 \n",
      "Epoch: 1/10:  mini-batch 291/4459:  Train loss: 3.536562442779541  Test loss: 3.5596771240234375 \n",
      "Epoch: 1/10:  mini-batch 292/4459:  Train loss: 3.4499170780181885  Test loss: 3.558681011199951 \n",
      "Epoch: 1/10:  mini-batch 293/4459:  Train loss: 3.5611233711242676  Test loss: 3.5576744079589844 \n",
      "Epoch: 1/10:  mini-batch 294/4459:  Train loss: 3.5747203826904297  Test loss: 3.5566468238830566 \n",
      "Epoch: 1/10:  mini-batch 295/4459:  Train loss: 3.53462815284729  Test loss: 3.555788040161133 \n",
      "Epoch: 1/10:  mini-batch 296/4459:  Train loss: 3.5245609283447266  Test loss: 3.5548927783966064 \n",
      "Epoch: 1/10:  mini-batch 297/4459:  Train loss: 3.5411949157714844  Test loss: 3.553927183151245 \n",
      "Epoch: 1/10:  mini-batch 298/4459:  Train loss: 3.5178284645080566  Test loss: 3.552947998046875 \n",
      "Epoch: 1/10:  mini-batch 299/4459:  Train loss: 3.5062198638916016  Test loss: 3.5518319606781006 \n",
      "Epoch: 1/10:  mini-batch 300/4459:  Train loss: 3.599339723587036  Test loss: 3.550814151763916 \n",
      "Epoch: 1/10:  mini-batch 301/4459:  Train loss: 3.6371090412139893  Test loss: 3.549689769744873 \n",
      "Epoch: 1/10:  mini-batch 302/4459:  Train loss: 3.5160651206970215  Test loss: 3.548821449279785 \n",
      "Epoch: 1/10:  mini-batch 303/4459:  Train loss: 3.58126163482666  Test loss: 3.5478367805480957 \n",
      "Epoch: 1/10:  mini-batch 304/4459:  Train loss: 3.520399808883667  Test loss: 3.546760320663452 \n",
      "Epoch: 1/10:  mini-batch 305/4459:  Train loss: 3.5965723991394043  Test loss: 3.5456666946411133 \n",
      "Epoch: 1/10:  mini-batch 306/4459:  Train loss: 3.500990867614746  Test loss: 3.544595956802368 \n",
      "Epoch: 1/10:  mini-batch 307/4459:  Train loss: 3.5416760444641113  Test loss: 3.543642044067383 \n",
      "Epoch: 1/10:  mini-batch 308/4459:  Train loss: 3.4958202838897705  Test loss: 3.542738914489746 \n",
      "Epoch: 1/10:  mini-batch 309/4459:  Train loss: 3.521148204803467  Test loss: 3.542079448699951 \n",
      "Epoch: 1/10:  mini-batch 310/4459:  Train loss: 3.400981903076172  Test loss: 3.541250705718994 \n",
      "Epoch: 1/10:  mini-batch 311/4459:  Train loss: 3.517728328704834  Test loss: 3.540862560272217 \n",
      "Epoch: 1/10:  mini-batch 312/4459:  Train loss: 3.4384751319885254  Test loss: 3.5403614044189453 \n",
      "Epoch: 1/10:  mini-batch 313/4459:  Train loss: 3.4342315196990967  Test loss: 3.5399909019470215 \n",
      "Epoch: 1/10:  mini-batch 314/4459:  Train loss: 3.494563579559326  Test loss: 3.539761543273926 \n",
      "Epoch: 1/10:  mini-batch 315/4459:  Train loss: 3.5133581161499023  Test loss: 3.539607048034668 \n",
      "Epoch: 1/10:  mini-batch 316/4459:  Train loss: 3.4956302642822266  Test loss: 3.539625644683838 \n",
      "Epoch: 1/10:  mini-batch 317/4459:  Train loss: 3.4243412017822266  Test loss: 3.5398099422454834 \n",
      "Epoch: 1/10:  mini-batch 318/4459:  Train loss: 3.613457441329956  Test loss: 3.5402979850769043 \n",
      "Epoch: 1/10:  mini-batch 319/4459:  Train loss: 3.5377156734466553  Test loss: 3.5412437915802 \n",
      "Epoch: 1/10:  mini-batch 320/4459:  Train loss: 3.2619481086730957  Test loss: 3.541743040084839 \n",
      "Epoch: 1/10:  mini-batch 321/4459:  Train loss: 3.4371285438537598  Test loss: 3.5419163703918457 \n",
      "Epoch: 1/10:  mini-batch 322/4459:  Train loss: 3.3808820247650146  Test loss: 3.5430846214294434 \n",
      "Epoch: 1/10:  mini-batch 323/4459:  Train loss: 3.467198371887207  Test loss: 3.5441927909851074 \n",
      "Epoch: 1/10:  mini-batch 324/4459:  Train loss: 3.491812229156494  Test loss: 3.5449790954589844 \n",
      "Epoch: 1/10:  mini-batch 325/4459:  Train loss: 3.383199453353882  Test loss: 3.5456933975219727 \n",
      "Epoch: 1/10:  mini-batch 326/4459:  Train loss: 3.658952236175537  Test loss: 3.5457324981689453 \n",
      "Epoch: 1/10:  mini-batch 327/4459:  Train loss: 3.495689868927002  Test loss: 3.545255184173584 \n",
      "Epoch: 1/10:  mini-batch 328/4459:  Train loss: 3.3760621547698975  Test loss: 3.5444626808166504 \n",
      "Epoch: 1/10:  mini-batch 329/4459:  Train loss: 3.386660575866699  Test loss: 3.5437726974487305 \n",
      "Epoch: 1/10:  mini-batch 330/4459:  Train loss: 3.5827131271362305  Test loss: 3.542799949645996 \n",
      "Epoch: 1/10:  mini-batch 331/4459:  Train loss: 3.411224365234375  Test loss: 3.542109251022339 \n",
      "Epoch: 1/10:  mini-batch 332/4459:  Train loss: 3.369406223297119  Test loss: 3.541884660720825 \n",
      "Epoch: 1/10:  mini-batch 333/4459:  Train loss: 3.3669724464416504  Test loss: 3.541820764541626 \n",
      "Epoch: 1/10:  mini-batch 334/4459:  Train loss: 3.4941184520721436  Test loss: 3.540745973587036 \n",
      "Epoch: 1/10:  mini-batch 335/4459:  Train loss: 3.557582378387451  Test loss: 3.539083957672119 \n",
      "Epoch: 1/10:  mini-batch 336/4459:  Train loss: 3.6120765209198  Test loss: 3.536677598953247 \n",
      "Epoch: 1/10:  mini-batch 337/4459:  Train loss: 3.436528205871582  Test loss: 3.534298896789551 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 338/4459:  Train loss: 3.601224422454834  Test loss: 3.530951976776123 \n",
      "Epoch: 1/10:  mini-batch 339/4459:  Train loss: 3.4151647090911865  Test loss: 3.527974843978882 \n",
      "Epoch: 1/10:  mini-batch 340/4459:  Train loss: 3.626239776611328  Test loss: 3.524399995803833 \n",
      "Epoch: 1/10:  mini-batch 341/4459:  Train loss: 3.14350962638855  Test loss: 3.5218019485473633 \n",
      "Epoch: 1/10:  mini-batch 342/4459:  Train loss: 3.4967689514160156  Test loss: 3.519287109375 \n",
      "Epoch: 1/10:  mini-batch 343/4459:  Train loss: 3.6646931171417236  Test loss: 3.516209602355957 \n",
      "Epoch: 1/10:  mini-batch 344/4459:  Train loss: 3.668013095855713  Test loss: 3.5131802558898926 \n",
      "Epoch: 1/10:  mini-batch 345/4459:  Train loss: 3.453693389892578  Test loss: 3.5103988647460938 \n",
      "Epoch: 1/10:  mini-batch 346/4459:  Train loss: 3.188122272491455  Test loss: 3.508371353149414 \n",
      "Epoch: 1/10:  mini-batch 347/4459:  Train loss: 3.395141124725342  Test loss: 3.5060503482818604 \n",
      "Epoch: 1/10:  mini-batch 348/4459:  Train loss: 3.347837448120117  Test loss: 3.504459857940674 \n",
      "Epoch: 1/10:  mini-batch 349/4459:  Train loss: 3.4112789630889893  Test loss: 3.503230571746826 \n",
      "Epoch: 1/10:  mini-batch 350/4459:  Train loss: 3.3739254474639893  Test loss: 3.5012898445129395 \n",
      "Epoch: 1/10:  mini-batch 351/4459:  Train loss: 3.558563709259033  Test loss: 3.499302387237549 \n",
      "Epoch: 1/10:  mini-batch 352/4459:  Train loss: 3.344050884246826  Test loss: 3.4978251457214355 \n",
      "Epoch: 1/10:  mini-batch 353/4459:  Train loss: 3.469517469406128  Test loss: 3.4963109493255615 \n",
      "Epoch: 1/10:  mini-batch 354/4459:  Train loss: 3.3174033164978027  Test loss: 3.494765043258667 \n",
      "Epoch: 1/10:  mini-batch 355/4459:  Train loss: 3.4802451133728027  Test loss: 3.492764949798584 \n",
      "Epoch: 1/10:  mini-batch 356/4459:  Train loss: 3.5104293823242188  Test loss: 3.490229845046997 \n",
      "Epoch: 1/10:  mini-batch 357/4459:  Train loss: 3.3760123252868652  Test loss: 3.4880459308624268 \n",
      "Epoch: 1/10:  mini-batch 358/4459:  Train loss: 3.4871363639831543  Test loss: 3.485044479370117 \n",
      "Epoch: 1/10:  mini-batch 359/4459:  Train loss: 3.4747867584228516  Test loss: 3.4824655055999756 \n",
      "Epoch: 1/10:  mini-batch 360/4459:  Train loss: 3.3907346725463867  Test loss: 3.4799671173095703 \n",
      "Epoch: 1/10:  mini-batch 361/4459:  Train loss: 3.3486480712890625  Test loss: 3.477574586868286 \n",
      "Epoch: 1/10:  mini-batch 362/4459:  Train loss: 3.6026878356933594  Test loss: 3.4751245975494385 \n",
      "Epoch: 1/10:  mini-batch 363/4459:  Train loss: 3.4883179664611816  Test loss: 3.472423791885376 \n",
      "Epoch: 1/10:  mini-batch 364/4459:  Train loss: 3.7167601585388184  Test loss: 3.4703025817871094 \n",
      "Epoch: 1/10:  mini-batch 365/4459:  Train loss: 3.0637011528015137  Test loss: 3.4697766304016113 \n",
      "Epoch: 1/10:  mini-batch 366/4459:  Train loss: 3.2698516845703125  Test loss: 3.470247507095337 \n",
      "Epoch: 1/10:  mini-batch 367/4459:  Train loss: 3.367913246154785  Test loss: 3.4699490070343018 \n",
      "Epoch: 1/10:  mini-batch 368/4459:  Train loss: 3.4491186141967773  Test loss: 3.469181776046753 \n",
      "Epoch: 1/10:  mini-batch 369/4459:  Train loss: 3.1652040481567383  Test loss: 3.4692554473876953 \n",
      "Epoch: 1/10:  mini-batch 370/4459:  Train loss: 3.13649582862854  Test loss: 3.4702646732330322 \n",
      "Epoch: 1/10:  mini-batch 371/4459:  Train loss: 3.5774948596954346  Test loss: 3.4711966514587402 \n",
      "Epoch: 1/10:  mini-batch 372/4459:  Train loss: 3.340195655822754  Test loss: 3.471700668334961 \n",
      "Epoch: 1/10:  mini-batch 373/4459:  Train loss: 3.2877371311187744  Test loss: 3.4723525047302246 \n",
      "Epoch: 1/10:  mini-batch 374/4459:  Train loss: 3.427186965942383  Test loss: 3.4731357097625732 \n",
      "Epoch: 1/10:  mini-batch 375/4459:  Train loss: 3.18961238861084  Test loss: 3.4752049446105957 \n",
      "Epoch: 1/10:  mini-batch 376/4459:  Train loss: 3.4946765899658203  Test loss: 3.4757580757141113 \n",
      "Epoch: 1/10:  mini-batch 377/4459:  Train loss: 3.240260124206543  Test loss: 3.476351737976074 \n",
      "Epoch: 1/10:  mini-batch 378/4459:  Train loss: 3.215718984603882  Test loss: 3.47776460647583 \n",
      "Epoch: 1/10:  mini-batch 379/4459:  Train loss: 3.3208813667297363  Test loss: 3.479841947555542 \n",
      "Epoch: 1/10:  mini-batch 380/4459:  Train loss: 3.694530725479126  Test loss: 3.4780898094177246 \n",
      "Epoch: 1/10:  mini-batch 381/4459:  Train loss: 3.3658275604248047  Test loss: 3.4754233360290527 \n",
      "Epoch: 1/10:  mini-batch 382/4459:  Train loss: 3.3323755264282227  Test loss: 3.473287582397461 \n",
      "Epoch: 1/10:  mini-batch 383/4459:  Train loss: 3.60379695892334  Test loss: 3.4672691822052 \n",
      "Epoch: 1/10:  mini-batch 384/4459:  Train loss: 3.824801445007324  Test loss: 3.4600541591644287 \n",
      "Epoch: 1/10:  mini-batch 385/4459:  Train loss: 3.5545618534088135  Test loss: 3.4535512924194336 \n",
      "Epoch: 1/10:  mini-batch 386/4459:  Train loss: 3.3745806217193604  Test loss: 3.4486238956451416 \n",
      "Epoch: 1/10:  mini-batch 387/4459:  Train loss: 3.666729688644409  Test loss: 3.4445931911468506 \n",
      "Epoch: 1/10:  mini-batch 388/4459:  Train loss: 3.028088092803955  Test loss: 3.442336082458496 \n",
      "Epoch: 1/10:  mini-batch 389/4459:  Train loss: 3.611356019973755  Test loss: 3.440661907196045 \n",
      "Epoch: 1/10:  mini-batch 390/4459:  Train loss: 3.590876340866089  Test loss: 3.4392151832580566 \n",
      "Epoch: 1/10:  mini-batch 391/4459:  Train loss: 3.326106071472168  Test loss: 3.4380040168762207 \n",
      "Epoch: 1/10:  mini-batch 392/4459:  Train loss: 3.229482650756836  Test loss: 3.437211513519287 \n",
      "Epoch: 1/10:  mini-batch 393/4459:  Train loss: 3.4565885066986084  Test loss: 3.4363625049591064 \n",
      "Epoch: 1/10:  mini-batch 394/4459:  Train loss: 3.293376922607422  Test loss: 3.4363720417022705 \n",
      "Epoch: 1/10:  mini-batch 395/4459:  Train loss: 3.0927932262420654  Test loss: 3.435802698135376 \n",
      "Epoch: 1/10:  mini-batch 396/4459:  Train loss: 3.6721267700195312  Test loss: 3.435227394104004 \n",
      "Epoch: 1/10:  mini-batch 397/4459:  Train loss: 3.3556113243103027  Test loss: 3.4349100589752197 \n",
      "Epoch: 1/10:  mini-batch 398/4459:  Train loss: 3.3936846256256104  Test loss: 3.4346065521240234 \n",
      "Epoch: 1/10:  mini-batch 399/4459:  Train loss: 3.524616241455078  Test loss: 3.434922695159912 \n",
      "Epoch: 1/10:  mini-batch 400/4459:  Train loss: 3.3240301609039307  Test loss: 3.4342472553253174 \n",
      "Epoch: 1/10:  mini-batch 401/4459:  Train loss: 3.3660528659820557  Test loss: 3.434016704559326 \n",
      "Epoch: 1/10:  mini-batch 402/4459:  Train loss: 3.3561179637908936  Test loss: 3.433579921722412 \n",
      "Epoch: 1/10:  mini-batch 403/4459:  Train loss: 3.223879814147949  Test loss: 3.4331488609313965 \n",
      "Epoch: 1/10:  mini-batch 404/4459:  Train loss: 3.497671127319336  Test loss: 3.4322304725646973 \n",
      "Epoch: 1/10:  mini-batch 405/4459:  Train loss: 3.2347373962402344  Test loss: 3.431598424911499 \n",
      "Epoch: 1/10:  mini-batch 406/4459:  Train loss: 3.3966064453125  Test loss: 3.4311296939849854 \n",
      "Epoch: 1/10:  mini-batch 407/4459:  Train loss: 3.4390125274658203  Test loss: 3.430710554122925 \n",
      "Epoch: 1/10:  mini-batch 408/4459:  Train loss: 3.3114914894104004  Test loss: 3.42995285987854 \n",
      "Epoch: 1/10:  mini-batch 409/4459:  Train loss: 3.170510768890381  Test loss: 3.4294607639312744 \n",
      "Epoch: 1/10:  mini-batch 410/4459:  Train loss: 3.209357976913452  Test loss: 3.42952299118042 \n",
      "Epoch: 1/10:  mini-batch 411/4459:  Train loss: 2.9662351608276367  Test loss: 3.4306652545928955 \n",
      "Epoch: 1/10:  mini-batch 412/4459:  Train loss: 3.3994815349578857  Test loss: 3.431832790374756 \n",
      "Epoch: 1/10:  mini-batch 413/4459:  Train loss: 3.670614242553711  Test loss: 3.432372570037842 \n",
      "Epoch: 1/10:  mini-batch 414/4459:  Train loss: 3.5654234886169434  Test loss: 3.4328877925872803 \n",
      "Epoch: 1/10:  mini-batch 415/4459:  Train loss: 3.5844855308532715  Test loss: 3.432649850845337 \n",
      "Epoch: 1/10:  mini-batch 416/4459:  Train loss: 3.7454121112823486  Test loss: 3.4311861991882324 \n",
      "Epoch: 1/10:  mini-batch 417/4459:  Train loss: 3.4934256076812744  Test loss: 3.428783893585205 \n",
      "Epoch: 1/10:  mini-batch 418/4459:  Train loss: 3.3990752696990967  Test loss: 3.426717758178711 \n",
      "Epoch: 1/10:  mini-batch 419/4459:  Train loss: 2.9967947006225586  Test loss: 3.425623893737793 \n",
      "Epoch: 1/10:  mini-batch 420/4459:  Train loss: 3.33164644241333  Test loss: 3.4246392250061035 \n",
      "Epoch: 1/10:  mini-batch 421/4459:  Train loss: 3.240955352783203  Test loss: 3.423815965652466 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 422/4459:  Train loss: 3.267827033996582  Test loss: 3.4234089851379395 \n",
      "Epoch: 1/10:  mini-batch 423/4459:  Train loss: 3.3204212188720703  Test loss: 3.422733783721924 \n",
      "Epoch: 1/10:  mini-batch 424/4459:  Train loss: 3.265443801879883  Test loss: 3.4223523139953613 \n",
      "Epoch: 1/10:  mini-batch 425/4459:  Train loss: 3.3717269897460938  Test loss: 3.421783447265625 \n",
      "Epoch: 1/10:  mini-batch 426/4459:  Train loss: 3.570769786834717  Test loss: 3.4209649562835693 \n",
      "Epoch: 1/10:  mini-batch 427/4459:  Train loss: 3.444324016571045  Test loss: 3.420459270477295 \n",
      "Epoch: 1/10:  mini-batch 428/4459:  Train loss: 3.305354118347168  Test loss: 3.419515609741211 \n",
      "Epoch: 1/10:  mini-batch 429/4459:  Train loss: 3.640841484069824  Test loss: 3.41776442527771 \n",
      "Epoch: 1/10:  mini-batch 430/4459:  Train loss: 3.0629541873931885  Test loss: 3.4168169498443604 \n",
      "Epoch: 1/10:  mini-batch 431/4459:  Train loss: 3.539127826690674  Test loss: 3.4160051345825195 \n",
      "Epoch: 1/10:  mini-batch 432/4459:  Train loss: 3.5008621215820312  Test loss: 3.4151835441589355 \n",
      "Epoch: 1/10:  mini-batch 433/4459:  Train loss: 3.5109527111053467  Test loss: 3.4153711795806885 \n",
      "Epoch: 1/10:  mini-batch 434/4459:  Train loss: 3.539220094680786  Test loss: 3.415440559387207 \n",
      "Epoch: 1/10:  mini-batch 435/4459:  Train loss: 3.3112645149230957  Test loss: 3.4155149459838867 \n",
      "Epoch: 1/10:  mini-batch 436/4459:  Train loss: 3.3858790397644043  Test loss: 3.4151611328125 \n",
      "Epoch: 1/10:  mini-batch 437/4459:  Train loss: 3.4374747276306152  Test loss: 3.4147205352783203 \n",
      "Epoch: 1/10:  mini-batch 438/4459:  Train loss: 3.4273557662963867  Test loss: 3.4143128395080566 \n",
      "Epoch: 1/10:  mini-batch 439/4459:  Train loss: 3.269137144088745  Test loss: 3.4139480590820312 \n",
      "Epoch: 1/10:  mini-batch 440/4459:  Train loss: 3.3907151222229004  Test loss: 3.4139440059661865 \n",
      "Epoch: 1/10:  mini-batch 441/4459:  Train loss: 3.501863956451416  Test loss: 3.41416072845459 \n",
      "Epoch: 1/10:  mini-batch 442/4459:  Train loss: 3.3484206199645996  Test loss: 3.414400100708008 \n",
      "Epoch: 1/10:  mini-batch 443/4459:  Train loss: 3.185222864151001  Test loss: 3.4146740436553955 \n",
      "Epoch: 1/10:  mini-batch 444/4459:  Train loss: 3.116464853286743  Test loss: 3.4151129722595215 \n",
      "Epoch: 1/10:  mini-batch 445/4459:  Train loss: 3.179868698120117  Test loss: 3.415123462677002 \n",
      "Epoch: 1/10:  mini-batch 446/4459:  Train loss: 3.3507237434387207  Test loss: 3.4146528244018555 \n",
      "Epoch: 1/10:  mini-batch 447/4459:  Train loss: 3.5296576023101807  Test loss: 3.413909912109375 \n",
      "Epoch: 1/10:  mini-batch 448/4459:  Train loss: 3.249164581298828  Test loss: 3.4133870601654053 \n",
      "Epoch: 1/10:  mini-batch 449/4459:  Train loss: 3.198992967605591  Test loss: 3.4130258560180664 \n",
      "Epoch: 1/10:  mini-batch 450/4459:  Train loss: 3.2491843700408936  Test loss: 3.4123668670654297 \n",
      "Epoch: 1/10:  mini-batch 451/4459:  Train loss: 3.0882463455200195  Test loss: 3.4120702743530273 \n",
      "Epoch: 1/10:  mini-batch 452/4459:  Train loss: 3.579767942428589  Test loss: 3.411916732788086 \n",
      "Epoch: 1/10:  mini-batch 453/4459:  Train loss: 3.270521640777588  Test loss: 3.4118123054504395 \n",
      "Epoch: 1/10:  mini-batch 454/4459:  Train loss: 3.0901894569396973  Test loss: 3.412259340286255 \n",
      "Epoch: 1/10:  mini-batch 455/4459:  Train loss: 3.1416707038879395  Test loss: 3.413386344909668 \n",
      "Epoch: 1/10:  mini-batch 456/4459:  Train loss: 3.340982437133789  Test loss: 3.4143218994140625 \n",
      "Epoch: 1/10:  mini-batch 457/4459:  Train loss: 3.380844831466675  Test loss: 3.4157865047454834 \n",
      "Epoch: 1/10:  mini-batch 458/4459:  Train loss: 3.5392308235168457  Test loss: 3.4167654514312744 \n",
      "Epoch: 1/10:  mini-batch 459/4459:  Train loss: 3.1402807235717773  Test loss: 3.418384313583374 \n",
      "Epoch: 1/10:  mini-batch 460/4459:  Train loss: 3.38677978515625  Test loss: 3.4204208850860596 \n",
      "Epoch: 1/10:  mini-batch 461/4459:  Train loss: 3.373616933822632  Test loss: 3.4224886894226074 \n",
      "Epoch: 1/10:  mini-batch 462/4459:  Train loss: 3.3719723224639893  Test loss: 3.4241228103637695 \n",
      "Epoch: 1/10:  mini-batch 463/4459:  Train loss: 2.9353432655334473  Test loss: 3.426543951034546 \n",
      "Epoch: 1/10:  mini-batch 464/4459:  Train loss: 3.1157612800598145  Test loss: 3.429600715637207 \n",
      "Epoch: 1/10:  mini-batch 465/4459:  Train loss: 3.1869144439697266  Test loss: 3.4324471950531006 \n",
      "Epoch: 1/10:  mini-batch 466/4459:  Train loss: 3.465629816055298  Test loss: 3.434598207473755 \n",
      "Epoch: 1/10:  mini-batch 467/4459:  Train loss: 3.4618396759033203  Test loss: 3.4354262351989746 \n",
      "Epoch: 1/10:  mini-batch 468/4459:  Train loss: 3.345712661743164  Test loss: 3.434481620788574 \n",
      "Epoch: 1/10:  mini-batch 469/4459:  Train loss: 3.103058338165283  Test loss: 3.4340410232543945 \n",
      "Epoch: 1/10:  mini-batch 470/4459:  Train loss: 3.660153388977051  Test loss: 3.432011127471924 \n",
      "Epoch: 1/10:  mini-batch 471/4459:  Train loss: 3.339355230331421  Test loss: 3.430311679840088 \n",
      "Epoch: 1/10:  mini-batch 472/4459:  Train loss: 3.295640468597412  Test loss: 3.4282820224761963 \n",
      "Epoch: 1/10:  mini-batch 473/4459:  Train loss: 3.1685380935668945  Test loss: 3.4277617931365967 \n",
      "Epoch: 1/10:  mini-batch 474/4459:  Train loss: 2.799217700958252  Test loss: 3.4287374019622803 \n",
      "Epoch: 1/10:  mini-batch 475/4459:  Train loss: 2.927687168121338  Test loss: 3.4314677715301514 \n",
      "Epoch: 1/10:  mini-batch 476/4459:  Train loss: 3.2301852703094482  Test loss: 3.4341554641723633 \n",
      "Epoch: 1/10:  mini-batch 477/4459:  Train loss: 2.804802417755127  Test loss: 3.4385480880737305 \n",
      "Epoch: 1/10:  mini-batch 478/4459:  Train loss: 3.094529151916504  Test loss: 3.44419002532959 \n",
      "Epoch: 1/10:  mini-batch 479/4459:  Train loss: 3.0408618450164795  Test loss: 3.450873613357544 \n",
      "Epoch: 1/10:  mini-batch 480/4459:  Train loss: 3.478330135345459  Test loss: 3.456305503845215 \n",
      "Epoch: 1/10:  mini-batch 481/4459:  Train loss: 3.110337018966675  Test loss: 3.4625556468963623 \n",
      "Epoch: 1/10:  mini-batch 482/4459:  Train loss: 3.04034423828125  Test loss: 3.4710114002227783 \n",
      "Epoch: 1/10:  mini-batch 483/4459:  Train loss: 3.697603225708008  Test loss: 3.4737138748168945 \n",
      "Epoch: 1/10:  mini-batch 484/4459:  Train loss: 3.115391969680786  Test loss: 3.476222515106201 \n",
      "Epoch: 1/10:  mini-batch 485/4459:  Train loss: 3.8125224113464355  Test loss: 3.4715800285339355 \n",
      "Epoch: 1/10:  mini-batch 486/4459:  Train loss: 3.7343335151672363  Test loss: 3.4626176357269287 \n",
      "Epoch: 1/10:  mini-batch 487/4459:  Train loss: 3.0311079025268555  Test loss: 3.4564907550811768 \n",
      "Epoch: 1/10:  mini-batch 488/4459:  Train loss: 3.2514021396636963  Test loss: 3.4497742652893066 \n",
      "Epoch: 1/10:  mini-batch 489/4459:  Train loss: 3.3543176651000977  Test loss: 3.442939281463623 \n",
      "Epoch: 1/10:  mini-batch 490/4459:  Train loss: 3.494227647781372  Test loss: 3.4366464614868164 \n",
      "Epoch: 1/10:  mini-batch 491/4459:  Train loss: 3.4588940143585205  Test loss: 3.4295949935913086 \n",
      "Epoch: 1/10:  mini-batch 492/4459:  Train loss: 3.5769622325897217  Test loss: 3.4232864379882812 \n",
      "Epoch: 1/10:  mini-batch 493/4459:  Train loss: 3.2993385791778564  Test loss: 3.4176344871520996 \n",
      "Epoch: 1/10:  mini-batch 494/4459:  Train loss: 3.627464771270752  Test loss: 3.413179397583008 \n",
      "Epoch: 1/10:  mini-batch 495/4459:  Train loss: 3.385380983352661  Test loss: 3.409294843673706 \n",
      "Epoch: 1/10:  mini-batch 496/4459:  Train loss: 3.03857684135437  Test loss: 3.406893491744995 \n",
      "Epoch: 1/10:  mini-batch 497/4459:  Train loss: 3.8406991958618164  Test loss: 3.4047417640686035 \n",
      "Epoch: 1/10:  mini-batch 498/4459:  Train loss: 3.2294058799743652  Test loss: 3.403444766998291 \n",
      "Epoch: 1/10:  mini-batch 499/4459:  Train loss: 3.0428407192230225  Test loss: 3.403167724609375 \n",
      "Epoch: 1/10:  mini-batch 500/4459:  Train loss: 3.304849147796631  Test loss: 3.403303861618042 \n",
      "Epoch: 1/10:  mini-batch 501/4459:  Train loss: 3.6307854652404785  Test loss: 3.403223991394043 \n",
      "Epoch: 1/10:  mini-batch 502/4459:  Train loss: 3.1580100059509277  Test loss: 3.403113842010498 \n",
      "Epoch: 1/10:  mini-batch 503/4459:  Train loss: 3.3764286041259766  Test loss: 3.402704954147339 \n",
      "Epoch: 1/10:  mini-batch 504/4459:  Train loss: 3.548548698425293  Test loss: 3.403109550476074 \n",
      "Epoch: 1/10:  mini-batch 505/4459:  Train loss: 3.875688076019287  Test loss: 3.403306722640991 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 506/4459:  Train loss: 2.9020729064941406  Test loss: 3.403841972351074 \n",
      "Epoch: 1/10:  mini-batch 507/4459:  Train loss: 3.2396256923675537  Test loss: 3.4045324325561523 \n",
      "Epoch: 1/10:  mini-batch 508/4459:  Train loss: 3.4204611778259277  Test loss: 3.405032157897949 \n",
      "Epoch: 1/10:  mini-batch 509/4459:  Train loss: 3.3431200981140137  Test loss: 3.4052021503448486 \n",
      "Epoch: 1/10:  mini-batch 510/4459:  Train loss: 3.5196471214294434  Test loss: 3.4061214923858643 \n",
      "Epoch: 1/10:  mini-batch 511/4459:  Train loss: 3.262320041656494  Test loss: 3.4050326347351074 \n",
      "Epoch: 1/10:  mini-batch 512/4459:  Train loss: 3.1924595832824707  Test loss: 3.4048640727996826 \n",
      "Epoch: 1/10:  mini-batch 513/4459:  Train loss: 3.3602426052093506  Test loss: 3.4043166637420654 \n",
      "Epoch: 1/10:  mini-batch 514/4459:  Train loss: 3.252230644226074  Test loss: 3.4040322303771973 \n",
      "Epoch: 1/10:  mini-batch 515/4459:  Train loss: 2.9148383140563965  Test loss: 3.4036996364593506 \n",
      "Epoch: 1/10:  mini-batch 516/4459:  Train loss: 3.4910216331481934  Test loss: 3.403702974319458 \n",
      "Epoch: 1/10:  mini-batch 517/4459:  Train loss: 3.1852221488952637  Test loss: 3.4036223888397217 \n",
      "Epoch: 1/10:  mini-batch 518/4459:  Train loss: 3.3733081817626953  Test loss: 3.4039626121520996 \n",
      "Epoch: 1/10:  mini-batch 519/4459:  Train loss: 3.319486141204834  Test loss: 3.4050378799438477 \n",
      "Epoch: 1/10:  mini-batch 520/4459:  Train loss: 3.124750852584839  Test loss: 3.40594482421875 \n",
      "Epoch: 1/10:  mini-batch 521/4459:  Train loss: 3.211660861968994  Test loss: 3.407277822494507 \n",
      "Epoch: 1/10:  mini-batch 522/4459:  Train loss: 3.420741081237793  Test loss: 3.408881187438965 \n",
      "Epoch: 1/10:  mini-batch 523/4459:  Train loss: 3.269481897354126  Test loss: 3.410839080810547 \n",
      "Epoch: 1/10:  mini-batch 524/4459:  Train loss: 3.4401814937591553  Test loss: 3.4120781421661377 \n",
      "Epoch: 1/10:  mini-batch 525/4459:  Train loss: 3.4954402446746826  Test loss: 3.413003444671631 \n",
      "Epoch: 1/10:  mini-batch 526/4459:  Train loss: 3.1430060863494873  Test loss: 3.4143590927124023 \n",
      "Epoch: 1/10:  mini-batch 527/4459:  Train loss: 3.3233842849731445  Test loss: 3.415351390838623 \n",
      "Epoch: 1/10:  mini-batch 528/4459:  Train loss: 3.475738048553467  Test loss: 3.41591739654541 \n",
      "Epoch: 1/10:  mini-batch 529/4459:  Train loss: 3.283662796020508  Test loss: 3.4165608882904053 \n",
      "Epoch: 1/10:  mini-batch 530/4459:  Train loss: 3.2359490394592285  Test loss: 3.416917085647583 \n",
      "Epoch: 1/10:  mini-batch 531/4459:  Train loss: 3.0792574882507324  Test loss: 3.4173288345336914 \n",
      "Epoch: 1/10:  mini-batch 532/4459:  Train loss: 3.143615245819092  Test loss: 3.4171953201293945 \n",
      "Epoch: 1/10:  mini-batch 533/4459:  Train loss: 3.228018283843994  Test loss: 3.416889190673828 \n",
      "Epoch: 1/10:  mini-batch 534/4459:  Train loss: 2.968078851699829  Test loss: 3.417372703552246 \n",
      "Epoch: 1/10:  mini-batch 535/4459:  Train loss: 3.0607798099517822  Test loss: 3.418525218963623 \n",
      "Epoch: 1/10:  mini-batch 536/4459:  Train loss: 3.845754384994507  Test loss: 3.4181137084960938 \n",
      "Epoch: 1/10:  mini-batch 537/4459:  Train loss: 3.52675724029541  Test loss: 3.4168152809143066 \n",
      "Epoch: 1/10:  mini-batch 538/4459:  Train loss: 3.1103923320770264  Test loss: 3.4161510467529297 \n",
      "Epoch: 1/10:  mini-batch 539/4459:  Train loss: 3.056212902069092  Test loss: 3.4157304763793945 \n",
      "Epoch: 1/10:  mini-batch 540/4459:  Train loss: 3.332101821899414  Test loss: 3.4142351150512695 \n",
      "Epoch: 1/10:  mini-batch 541/4459:  Train loss: 3.2186717987060547  Test loss: 3.4130773544311523 \n",
      "Epoch: 1/10:  mini-batch 542/4459:  Train loss: 3.065101146697998  Test loss: 3.412959575653076 \n",
      "Epoch: 1/10:  mini-batch 543/4459:  Train loss: 3.32135009765625  Test loss: 3.412278890609741 \n",
      "Epoch: 1/10:  mini-batch 544/4459:  Train loss: 3.468360424041748  Test loss: 3.411569833755493 \n",
      "Epoch: 1/10:  mini-batch 545/4459:  Train loss: 2.722543478012085  Test loss: 3.4127695560455322 \n",
      "Epoch: 1/10:  mini-batch 546/4459:  Train loss: 3.1984682083129883  Test loss: 3.4144392013549805 \n",
      "Epoch: 1/10:  mini-batch 547/4459:  Train loss: 2.9741108417510986  Test loss: 3.416867971420288 \n",
      "Epoch: 1/10:  mini-batch 548/4459:  Train loss: 3.2652504444122314  Test loss: 3.418458938598633 \n",
      "Epoch: 1/10:  mini-batch 549/4459:  Train loss: 3.3580944538116455  Test loss: 3.4191200733184814 \n",
      "Epoch: 1/10:  mini-batch 550/4459:  Train loss: 3.893240451812744  Test loss: 3.4170610904693604 \n",
      "Epoch: 1/10:  mini-batch 551/4459:  Train loss: 2.947422504425049  Test loss: 3.4156405925750732 \n",
      "Epoch: 1/10:  mini-batch 552/4459:  Train loss: 3.834602117538452  Test loss: 3.412609815597534 \n",
      "Epoch: 1/10:  mini-batch 553/4459:  Train loss: 3.548461437225342  Test loss: 3.409636974334717 \n",
      "Epoch: 1/10:  mini-batch 554/4459:  Train loss: 3.442359447479248  Test loss: 3.406651020050049 \n",
      "Epoch: 1/10:  mini-batch 555/4459:  Train loss: 3.6547417640686035  Test loss: 3.4040205478668213 \n",
      "Epoch: 1/10:  mini-batch 556/4459:  Train loss: 3.249800682067871  Test loss: 3.402571201324463 \n",
      "Epoch: 1/10:  mini-batch 557/4459:  Train loss: 3.5229413509368896  Test loss: 3.401563882827759 \n",
      "Epoch: 1/10:  mini-batch 558/4459:  Train loss: 3.4066824913024902  Test loss: 3.400218963623047 \n",
      "Epoch: 1/10:  mini-batch 559/4459:  Train loss: 3.6563374996185303  Test loss: 3.399233818054199 \n",
      "Epoch: 1/10:  mini-batch 560/4459:  Train loss: 3.411269426345825  Test loss: 3.3989734649658203 \n",
      "Epoch: 1/10:  mini-batch 561/4459:  Train loss: 3.417865037918091  Test loss: 3.398625612258911 \n",
      "Epoch: 1/10:  mini-batch 562/4459:  Train loss: 3.8618102073669434  Test loss: 3.398083209991455 \n",
      "Epoch: 1/10:  mini-batch 563/4459:  Train loss: 3.3622076511383057  Test loss: 3.3982086181640625 \n",
      "Epoch: 1/10:  mini-batch 564/4459:  Train loss: 3.287144184112549  Test loss: 3.398568868637085 \n",
      "Epoch: 1/10:  mini-batch 565/4459:  Train loss: 3.3559460639953613  Test loss: 3.399014472961426 \n",
      "Epoch: 1/10:  mini-batch 566/4459:  Train loss: 3.001697540283203  Test loss: 3.3995790481567383 \n",
      "Epoch: 1/10:  mini-batch 567/4459:  Train loss: 3.0499024391174316  Test loss: 3.4001502990722656 \n",
      "Epoch: 1/10:  mini-batch 568/4459:  Train loss: 3.20548415184021  Test loss: 3.4000401496887207 \n",
      "Epoch: 1/10:  mini-batch 569/4459:  Train loss: 3.1610653400421143  Test loss: 3.400333881378174 \n",
      "Epoch: 1/10:  mini-batch 570/4459:  Train loss: 3.4907984733581543  Test loss: 3.400191307067871 \n",
      "Epoch: 1/10:  mini-batch 571/4459:  Train loss: 3.269087791442871  Test loss: 3.4007980823516846 \n",
      "Epoch: 1/10:  mini-batch 572/4459:  Train loss: 3.505394697189331  Test loss: 3.401834011077881 \n",
      "Epoch: 1/10:  mini-batch 573/4459:  Train loss: 3.492353916168213  Test loss: 3.4027674198150635 \n",
      "Epoch: 1/10:  mini-batch 574/4459:  Train loss: 3.456191062927246  Test loss: 3.4034881591796875 \n",
      "Epoch: 1/10:  mini-batch 575/4459:  Train loss: 3.1959500312805176  Test loss: 3.4038920402526855 \n",
      "Epoch: 1/10:  mini-batch 576/4459:  Train loss: 3.2560794353485107  Test loss: 3.404510498046875 \n",
      "Epoch: 1/10:  mini-batch 577/4459:  Train loss: 3.324136734008789  Test loss: 3.404660940170288 \n",
      "Epoch: 1/10:  mini-batch 578/4459:  Train loss: 3.476064443588257  Test loss: 3.4050633907318115 \n",
      "Epoch: 1/10:  mini-batch 579/4459:  Train loss: 3.284327507019043  Test loss: 3.4055590629577637 \n",
      "Epoch: 1/10:  mini-batch 580/4459:  Train loss: 3.2016892433166504  Test loss: 3.406580924987793 \n",
      "Epoch: 1/10:  mini-batch 581/4459:  Train loss: 3.23264479637146  Test loss: 3.407630205154419 \n",
      "Epoch: 1/10:  mini-batch 582/4459:  Train loss: 3.321434259414673  Test loss: 3.409090280532837 \n",
      "Epoch: 1/10:  mini-batch 583/4459:  Train loss: 3.5494894981384277  Test loss: 3.4106156826019287 \n",
      "Epoch: 1/10:  mini-batch 584/4459:  Train loss: 3.675389528274536  Test loss: 3.4121363162994385 \n",
      "Epoch: 1/10:  mini-batch 585/4459:  Train loss: 3.2304749488830566  Test loss: 3.4136853218078613 \n",
      "Epoch: 1/10:  mini-batch 586/4459:  Train loss: 3.263969659805298  Test loss: 3.4145402908325195 \n",
      "Epoch: 1/10:  mini-batch 587/4459:  Train loss: 3.2166383266448975  Test loss: 3.4152960777282715 \n",
      "Epoch: 1/10:  mini-batch 588/4459:  Train loss: 3.2485618591308594  Test loss: 3.4157559871673584 \n",
      "Epoch: 1/10:  mini-batch 589/4459:  Train loss: 3.3552117347717285  Test loss: 3.416306972503662 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 590/4459:  Train loss: 3.4899885654449463  Test loss: 3.416536569595337 \n",
      "Epoch: 1/10:  mini-batch 591/4459:  Train loss: 3.4374208450317383  Test loss: 3.416712522506714 \n",
      "Epoch: 1/10:  mini-batch 592/4459:  Train loss: 3.6769089698791504  Test loss: 3.4162487983703613 \n",
      "Epoch: 1/10:  mini-batch 593/4459:  Train loss: 2.847816228866577  Test loss: 3.4167118072509766 \n",
      "Epoch: 1/10:  mini-batch 594/4459:  Train loss: 3.383667469024658  Test loss: 3.416558027267456 \n",
      "Epoch: 1/10:  mini-batch 595/4459:  Train loss: 3.5423178672790527  Test loss: 3.416003704071045 \n",
      "Epoch: 1/10:  mini-batch 596/4459:  Train loss: 3.442394495010376  Test loss: 3.4154131412506104 \n",
      "Epoch: 1/10:  mini-batch 597/4459:  Train loss: 3.595111608505249  Test loss: 3.4144113063812256 \n",
      "Epoch: 1/10:  mini-batch 598/4459:  Train loss: 3.699545383453369  Test loss: 3.413332939147949 \n",
      "Epoch: 1/10:  mini-batch 599/4459:  Train loss: 3.3139374256134033  Test loss: 3.412667751312256 \n",
      "Epoch: 1/10:  mini-batch 600/4459:  Train loss: 2.9902002811431885  Test loss: 3.4128005504608154 \n",
      "Epoch: 1/10:  mini-batch 601/4459:  Train loss: 3.169262409210205  Test loss: 3.412504196166992 \n",
      "Epoch: 1/10:  mini-batch 602/4459:  Train loss: 3.612931966781616  Test loss: 3.4116148948669434 \n",
      "Epoch: 1/10:  mini-batch 603/4459:  Train loss: 3.284623384475708  Test loss: 3.4110612869262695 \n",
      "Epoch: 1/10:  mini-batch 604/4459:  Train loss: 3.3937973976135254  Test loss: 3.410860061645508 \n",
      "Epoch: 1/10:  mini-batch 605/4459:  Train loss: 3.0767593383789062  Test loss: 3.4109349250793457 \n",
      "Epoch: 1/10:  mini-batch 606/4459:  Train loss: 3.599144220352173  Test loss: 3.4109110832214355 \n",
      "Epoch: 1/10:  mini-batch 607/4459:  Train loss: 3.1070339679718018  Test loss: 3.4114577770233154 \n",
      "Epoch: 1/10:  mini-batch 608/4459:  Train loss: 2.977632761001587  Test loss: 3.412459135055542 \n",
      "Epoch: 1/10:  mini-batch 636/4459:  Train loss: 3.516808032989502  Test loss: 3.4023349285125732 \n",
      "Epoch: 1/10:  mini-batch 637/4459:  Train loss: 3.4646830558776855  Test loss: 3.4009857177734375 \n",
      "Epoch: 1/10:  mini-batch 638/4459:  Train loss: 3.4050002098083496  Test loss: 3.3997843265533447 \n",
      "Epoch: 1/10:  mini-batch 639/4459:  Train loss: 3.215055465698242  Test loss: 3.3989500999450684 \n",
      "Epoch: 1/10:  mini-batch 640/4459:  Train loss: 3.3200435638427734  Test loss: 3.398070812225342 \n",
      "Epoch: 1/10:  mini-batch 641/4459:  Train loss: 3.594216823577881  Test loss: 3.396747350692749 \n",
      "Epoch: 1/10:  mini-batch 642/4459:  Train loss: 3.006150484085083  Test loss: 3.3957314491271973 \n",
      "Epoch: 1/10:  mini-batch 643/4459:  Train loss: 3.2323532104492188  Test loss: 3.395817756652832 \n",
      "Epoch: 1/10:  mini-batch 644/4459:  Train loss: 3.3963236808776855  Test loss: 3.3966126441955566 \n",
      "Epoch: 1/10:  mini-batch 645/4459:  Train loss: 3.3714404106140137  Test loss: 3.397429943084717 \n",
      "Epoch: 1/10:  mini-batch 646/4459:  Train loss: 3.6071925163269043  Test loss: 3.3982555866241455 \n",
      "Epoch: 1/10:  mini-batch 647/4459:  Train loss: 3.2897188663482666  Test loss: 3.398836135864258 \n",
      "Epoch: 1/10:  mini-batch 648/4459:  Train loss: 3.4686505794525146  Test loss: 3.3982858657836914 \n",
      "Epoch: 1/10:  mini-batch 649/4459:  Train loss: 3.1306300163269043  Test loss: 3.39849853515625 \n",
      "Epoch: 1/10:  mini-batch 650/4459:  Train loss: 3.373744487762451  Test loss: 3.398566961288452 \n",
      "Epoch: 1/10:  mini-batch 651/4459:  Train loss: 3.5346951484680176  Test loss: 3.3993430137634277 \n",
      "Epoch: 1/10:  mini-batch 652/4459:  Train loss: 3.123213291168213  Test loss: 3.4005377292633057 \n",
      "Epoch: 1/10:  mini-batch 653/4459:  Train loss: 3.0395560264587402  Test loss: 3.401542901992798 \n",
      "Epoch: 1/10:  mini-batch 654/4459:  Train loss: 3.3365211486816406  Test loss: 3.4026846885681152 \n",
      "Epoch: 1/10:  mini-batch 655/4459:  Train loss: 3.3097734451293945  Test loss: 3.4035024642944336 \n",
      "Epoch: 1/10:  mini-batch 656/4459:  Train loss: 3.5203938484191895  Test loss: 3.4045116901397705 \n",
      "Epoch: 1/10:  mini-batch 657/4459:  Train loss: 3.138498306274414  Test loss: 3.4064464569091797 \n",
      "Epoch: 1/10:  mini-batch 658/4459:  Train loss: 3.0494325160980225  Test loss: 3.4081952571868896 \n",
      "Epoch: 1/10:  mini-batch 659/4459:  Train loss: 3.4303295612335205  Test loss: 3.4094085693359375 \n",
      "Epoch: 1/10:  mini-batch 660/4459:  Train loss: 3.181225538253784  Test loss: 3.410531520843506 \n",
      "Epoch: 1/10:  mini-batch 682/4459:  Train loss: 3.8508925437927246  Test loss: 3.404338836669922 \n",
      "Epoch: 1/10:  mini-batch 683/4459:  Train loss: 3.2267467975616455  Test loss: 3.4022457599639893 \n",
      "Epoch: 1/10:  mini-batch 684/4459:  Train loss: 3.067997932434082  Test loss: 3.4005188941955566 \n",
      "Epoch: 1/10:  mini-batch 685/4459:  Train loss: 3.489851951599121  Test loss: 3.398897647857666 \n",
      "Epoch: 1/10:  mini-batch 686/4459:  Train loss: 3.3541581630706787  Test loss: 3.3969571590423584 \n",
      "Epoch: 1/10:  mini-batch 687/4459:  Train loss: 3.1026790142059326  Test loss: 3.395310878753662 \n",
      "Epoch: 1/10:  mini-batch 688/4459:  Train loss: 3.6782751083374023  Test loss: 3.393679141998291 \n",
      "Epoch: 1/10:  mini-batch 689/4459:  Train loss: 3.0915298461914062  Test loss: 3.3918099403381348 \n",
      "Epoch: 1/10:  mini-batch 690/4459:  Train loss: 3.7537567615509033  Test loss: 3.3895883560180664 \n",
      "Epoch: 1/10:  mini-batch 691/4459:  Train loss: 3.6539368629455566  Test loss: 3.3870761394500732 \n",
      "Epoch: 1/10:  mini-batch 692/4459:  Train loss: 3.290888547897339  Test loss: 3.384612560272217 \n",
      "Epoch: 1/10:  mini-batch 693/4459:  Train loss: 2.930530071258545  Test loss: 3.3830738067626953 \n",
      "Epoch: 1/10:  mini-batch 694/4459:  Train loss: 3.5298104286193848  Test loss: 3.3818392753601074 \n",
      "Epoch: 1/10:  mini-batch 695/4459:  Train loss: 2.5132791996002197  Test loss: 3.3821046352386475 \n",
      "Epoch: 1/10:  mini-batch 696/4459:  Train loss: 3.297062397003174  Test loss: 3.382354736328125 \n",
      "Epoch: 1/10:  mini-batch 697/4459:  Train loss: 3.151373863220215  Test loss: 3.3833093643188477 \n",
      "Epoch: 1/10:  mini-batch 698/4459:  Train loss: 3.038414239883423  Test loss: 3.384760618209839 \n",
      "Epoch: 1/10:  mini-batch 699/4459:  Train loss: 2.824036121368408  Test loss: 3.3871936798095703 \n",
      "Epoch: 1/10:  mini-batch 700/4459:  Train loss: 2.9474973678588867  Test loss: 3.390223503112793 \n",
      "Epoch: 1/10:  mini-batch 701/4459:  Train loss: 3.025207996368408  Test loss: 3.393655300140381 \n",
      "Epoch: 1/10:  mini-batch 702/4459:  Train loss: 2.9247243404388428  Test loss: 3.39810848236084 \n",
      "Epoch: 1/10:  mini-batch 703/4459:  Train loss: 3.971080780029297  Test loss: 3.401266574859619 \n",
      "Epoch: 1/10:  mini-batch 704/4459:  Train loss: 3.5944557189941406  Test loss: 3.4033045768737793 \n",
      "Epoch: 1/10:  mini-batch 705/4459:  Train loss: 3.1553258895874023  Test loss: 3.4058637619018555 \n",
      "Epoch: 1/10:  mini-batch 706/4459:  Train loss: 3.4356637001037598  Test loss: 3.407343864440918 \n",
      "Epoch: 1/10:  mini-batch 707/4459:  Train loss: 2.7989501953125  Test loss: 3.4107296466827393 \n",
      "Epoch: 1/10:  mini-batch 708/4459:  Train loss: 3.1123383045196533  Test loss: 3.414654016494751 \n",
      "Epoch: 1/10:  mini-batch 709/4459:  Train loss: 3.4019174575805664  Test loss: 3.418220043182373 \n",
      "Epoch: 1/10:  mini-batch 710/4459:  Train loss: 3.2452139854431152  Test loss: 3.4210798740386963 \n",
      "Epoch: 1/10:  mini-batch 711/4459:  Train loss: 3.060404062271118  Test loss: 3.424464225769043 \n",
      "Epoch: 1/10:  mini-batch 712/4459:  Train loss: 3.1730875968933105  Test loss: 3.4272220134735107 \n",
      "Epoch: 1/10:  mini-batch 713/4459:  Train loss: 3.37454891204834  Test loss: 3.4287195205688477 \n",
      "Epoch: 1/10:  mini-batch 714/4459:  Train loss: 3.3191123008728027  Test loss: 3.427929639816284 \n",
      "Epoch: 1/10:  mini-batch 715/4459:  Train loss: 3.204881191253662  Test loss: 3.4255764484405518 \n",
      "Epoch: 1/10:  mini-batch 716/4459:  Train loss: 3.49562668800354  Test loss: 3.4234423637390137 \n",
      "Epoch: 1/10:  mini-batch 717/4459:  Train loss: 3.618694543838501  Test loss: 3.419735908508301 \n",
      "Epoch: 1/10:  mini-batch 718/4459:  Train loss: 2.981322765350342  Test loss: 3.416900634765625 \n",
      "Epoch: 1/10:  mini-batch 719/4459:  Train loss: 2.9741172790527344  Test loss: 3.4151296615600586 \n",
      "Epoch: 1/10:  mini-batch 720/4459:  Train loss: 3.284700870513916  Test loss: 3.4126973152160645 \n",
      "Epoch: 1/10:  mini-batch 721/4459:  Train loss: 3.050762176513672  Test loss: 3.410953998565674 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 722/4459:  Train loss: 3.1674346923828125  Test loss: 3.4085028171539307 \n",
      "Epoch: 1/10:  mini-batch 723/4459:  Train loss: 3.0940678119659424  Test loss: 3.4062085151672363 \n",
      "Epoch: 1/10:  mini-batch 724/4459:  Train loss: 3.1453118324279785  Test loss: 3.4040424823760986 \n",
      "Epoch: 1/10:  mini-batch 725/4459:  Train loss: 3.5306878089904785  Test loss: 3.400601387023926 \n",
      "Epoch: 1/10:  mini-batch 726/4459:  Train loss: 3.620347023010254  Test loss: 3.3965258598327637 \n",
      "Epoch: 1/10:  mini-batch 727/4459:  Train loss: 3.223822593688965  Test loss: 3.3929827213287354 \n",
      "Epoch: 1/10:  mini-batch 728/4459:  Train loss: 3.06598162651062  Test loss: 3.3900094032287598 \n",
      "Epoch: 1/10:  mini-batch 729/4459:  Train loss: 3.135211229324341  Test loss: 3.3879997730255127 \n",
      "Epoch: 1/10:  mini-batch 730/4459:  Train loss: 3.642826557159424  Test loss: 3.3849778175354004 \n",
      "Epoch: 1/10:  mini-batch 731/4459:  Train loss: 3.3665771484375  Test loss: 3.381680488586426 \n",
      "Epoch: 1/10:  mini-batch 732/4459:  Train loss: 3.0058822631835938  Test loss: 3.377810001373291 \n",
      "Epoch: 1/10:  mini-batch 733/4459:  Train loss: 3.2474923133850098  Test loss: 3.3739142417907715 \n",
      "Epoch: 1/10:  mini-batch 734/4459:  Train loss: 3.5298585891723633  Test loss: 3.3700149059295654 \n",
      "Epoch: 1/10:  mini-batch 735/4459:  Train loss: 3.1444976329803467  Test loss: 3.3668792247772217 \n",
      "Epoch: 1/10:  mini-batch 736/4459:  Train loss: 3.003286838531494  Test loss: 3.3639297485351562 \n",
      "Epoch: 1/10:  mini-batch 737/4459:  Train loss: 2.95040225982666  Test loss: 3.3615033626556396 \n",
      "Epoch: 1/10:  mini-batch 738/4459:  Train loss: 3.448286294937134  Test loss: 3.359637975692749 \n",
      "Epoch: 1/10:  mini-batch 739/4459:  Train loss: 3.2997121810913086  Test loss: 3.3569178581237793 \n",
      "Epoch: 1/10:  mini-batch 740/4459:  Train loss: 3.2081966400146484  Test loss: 3.3549046516418457 \n",
      "Epoch: 1/10:  mini-batch 741/4459:  Train loss: 3.2680258750915527  Test loss: 3.3529250621795654 \n",
      "Epoch: 1/10:  mini-batch 742/4459:  Train loss: 3.4112815856933594  Test loss: 3.350493907928467 \n",
      "Epoch: 1/10:  mini-batch 743/4459:  Train loss: 3.269946336746216  Test loss: 3.348897933959961 \n",
      "Epoch: 1/10:  mini-batch 744/4459:  Train loss: 3.355839967727661  Test loss: 3.347982883453369 \n",
      "Epoch: 1/10:  mini-batch 745/4459:  Train loss: 3.767302989959717  Test loss: 3.3467509746551514 \n",
      "Epoch: 1/10:  mini-batch 746/4459:  Train loss: 2.9816553592681885  Test loss: 3.3462934494018555 \n",
      "Epoch: 1/10:  mini-batch 747/4459:  Train loss: 2.702415943145752  Test loss: 3.346313714981079 \n",
      "Epoch: 1/10:  mini-batch 748/4459:  Train loss: 3.0118043422698975  Test loss: 3.3457541465759277 \n",
      "Epoch: 1/10:  mini-batch 749/4459:  Train loss: 3.4521689414978027  Test loss: 3.3454222679138184 \n",
      "Epoch: 1/10:  mini-batch 750/4459:  Train loss: 3.4164326190948486  Test loss: 3.3447399139404297 \n",
      "Epoch: 1/10:  mini-batch 751/4459:  Train loss: 3.707235336303711  Test loss: 3.3440377712249756 \n",
      "Epoch: 1/10:  mini-batch 752/4459:  Train loss: 3.454634666442871  Test loss: 3.343210220336914 \n",
      "Epoch: 1/10:  mini-batch 753/4459:  Train loss: 3.3918204307556152  Test loss: 3.342649459838867 \n",
      "Epoch: 1/10:  mini-batch 754/4459:  Train loss: 3.225383758544922  Test loss: 3.3422179222106934 \n",
      "Epoch: 1/10:  mini-batch 755/4459:  Train loss: 3.094879150390625  Test loss: 3.3422069549560547 \n",
      "Epoch: 1/10:  mini-batch 756/4459:  Train loss: 3.217484951019287  Test loss: 3.342231273651123 \n",
      "Epoch: 1/10:  mini-batch 757/4459:  Train loss: 2.8698694705963135  Test loss: 3.342290163040161 \n",
      "Epoch: 1/10:  mini-batch 758/4459:  Train loss: 2.9208881855010986  Test loss: 3.3425302505493164 \n",
      "Epoch: 1/10:  mini-batch 759/4459:  Train loss: 3.436713933944702  Test loss: 3.3423449993133545 \n",
      "Epoch: 1/10:  mini-batch 760/4459:  Train loss: 3.179224729537964  Test loss: 3.342214822769165 \n",
      "Epoch: 1/10:  mini-batch 761/4459:  Train loss: 3.152181625366211  Test loss: 3.342284917831421 \n",
      "Epoch: 1/10:  mini-batch 762/4459:  Train loss: 3.6756296157836914  Test loss: 3.3421430587768555 \n",
      "Epoch: 1/10:  mini-batch 763/4459:  Train loss: 3.455700635910034  Test loss: 3.342546224594116 \n",
      "Epoch: 1/10:  mini-batch 764/4459:  Train loss: 3.0515084266662598  Test loss: 3.3424906730651855 \n",
      "Epoch: 1/10:  mini-batch 765/4459:  Train loss: 3.5345425605773926  Test loss: 3.342198610305786 \n",
      "Epoch: 1/10:  mini-batch 766/4459:  Train loss: 3.81510329246521  Test loss: 3.341606616973877 \n",
      "Epoch: 1/10:  mini-batch 767/4459:  Train loss: 3.5448572635650635  Test loss: 3.341250419616699 \n",
      "Epoch: 1/10:  mini-batch 768/4459:  Train loss: 2.965545654296875  Test loss: 3.3409409523010254 \n",
      "Epoch: 1/10:  mini-batch 769/4459:  Train loss: 3.145087718963623  Test loss: 3.3402490615844727 \n",
      "Epoch: 1/10:  mini-batch 770/4459:  Train loss: 3.3258800506591797  Test loss: 3.339650869369507 \n",
      "Epoch: 1/10:  mini-batch 771/4459:  Train loss: 3.1196179389953613  Test loss: 3.339599847793579 \n",
      "Epoch: 1/10:  mini-batch 772/4459:  Train loss: 3.5150539875030518  Test loss: 3.339599609375 \n",
      "Epoch: 1/10:  mini-batch 773/4459:  Train loss: 3.173773765563965  Test loss: 3.3398375511169434 \n",
      "Epoch: 1/10:  mini-batch 774/4459:  Train loss: 3.2337965965270996  Test loss: 3.3401079177856445 \n",
      "Epoch: 1/10:  mini-batch 775/4459:  Train loss: 3.6131539344787598  Test loss: 3.3402750492095947 \n",
      "Epoch: 1/10:  mini-batch 776/4459:  Train loss: 2.920992374420166  Test loss: 3.3405203819274902 \n",
      "Epoch: 1/10:  mini-batch 777/4459:  Train loss: 3.0483667850494385  Test loss: 3.3411359786987305 \n",
      "Epoch: 1/10:  mini-batch 778/4459:  Train loss: 3.4408607482910156  Test loss: 3.341815710067749 \n",
      "Epoch: 1/10:  mini-batch 779/4459:  Train loss: 2.9438819885253906  Test loss: 3.3425073623657227 \n",
      "Epoch: 1/10:  mini-batch 780/4459:  Train loss: 3.045154094696045  Test loss: 3.343268871307373 \n",
      "Epoch: 1/10:  mini-batch 781/4459:  Train loss: 3.289841413497925  Test loss: 3.3443751335144043 \n",
      "Epoch: 1/10:  mini-batch 782/4459:  Train loss: 3.6382551193237305  Test loss: 3.345339775085449 \n",
      "Epoch: 1/10:  mini-batch 783/4459:  Train loss: 3.1357228755950928  Test loss: 3.345212459564209 \n",
      "Epoch: 1/10:  mini-batch 784/4459:  Train loss: 3.4726574420928955  Test loss: 3.345184564590454 \n",
      "Epoch: 1/10:  mini-batch 785/4459:  Train loss: 3.448256254196167  Test loss: 3.3452885150909424 \n",
      "Epoch: 1/10:  mini-batch 786/4459:  Train loss: 3.623164176940918  Test loss: 3.3449769020080566 \n",
      "Epoch: 1/10:  mini-batch 787/4459:  Train loss: 3.066094398498535  Test loss: 3.3450629711151123 \n",
      "Epoch: 1/10:  mini-batch 788/4459:  Train loss: 3.539503812789917  Test loss: 3.3455677032470703 \n",
      "Epoch: 1/10:  mini-batch 789/4459:  Train loss: 3.8590893745422363  Test loss: 3.3471171855926514 \n",
      "Epoch: 1/10:  mini-batch 790/4459:  Train loss: 3.5572476387023926  Test loss: 3.3491272926330566 \n",
      "Epoch: 1/10:  mini-batch 791/4459:  Train loss: 3.7664449214935303  Test loss: 3.351653575897217 \n",
      "Epoch: 1/10:  mini-batch 792/4459:  Train loss: 2.9199676513671875  Test loss: 3.3542637825012207 \n",
      "Epoch: 1/10:  mini-batch 793/4459:  Train loss: 3.4713079929351807  Test loss: 3.3567285537719727 \n",
      "Epoch: 1/10:  mini-batch 794/4459:  Train loss: 3.180870771408081  Test loss: 3.359743595123291 \n",
      "Epoch: 1/10:  mini-batch 795/4459:  Train loss: 3.060018539428711  Test loss: 3.361767530441284 \n",
      "Epoch: 1/10:  mini-batch 796/4459:  Train loss: 3.041372776031494  Test loss: 3.363267421722412 \n",
      "Epoch: 1/10:  mini-batch 797/4459:  Train loss: 3.3778162002563477  Test loss: 3.3644676208496094 \n",
      "Epoch: 1/10:  mini-batch 798/4459:  Train loss: 3.425266742706299  Test loss: 3.366595983505249 \n",
      "Epoch: 1/10:  mini-batch 799/4459:  Train loss: 3.0015439987182617  Test loss: 3.367861270904541 \n",
      "Epoch: 1/10:  mini-batch 800/4459:  Train loss: 3.034254550933838  Test loss: 3.3689818382263184 \n",
      "Epoch: 1/10:  mini-batch 801/4459:  Train loss: 3.441460371017456  Test loss: 3.369436025619507 \n",
      "Epoch: 1/10:  mini-batch 802/4459:  Train loss: 3.095607280731201  Test loss: 3.370518684387207 \n",
      "Epoch: 1/10:  mini-batch 803/4459:  Train loss: 3.3288896083831787  Test loss: 3.370420217514038 \n",
      "Epoch: 1/10:  mini-batch 804/4459:  Train loss: 3.2204883098602295  Test loss: 3.3697996139526367 \n",
      "Epoch: 1/10:  mini-batch 805/4459:  Train loss: 3.7340164184570312  Test loss: 3.36863112449646 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 806/4459:  Train loss: 3.3148066997528076  Test loss: 3.366790771484375 \n",
      "Epoch: 1/10:  mini-batch 807/4459:  Train loss: 3.4801602363586426  Test loss: 3.3647656440734863 \n",
      "Epoch: 1/10:  mini-batch 808/4459:  Train loss: 3.274165153503418  Test loss: 3.3623509407043457 \n",
      "Epoch: 1/10:  mini-batch 809/4459:  Train loss: 2.958995819091797  Test loss: 3.3602256774902344 \n",
      "Epoch: 1/10:  mini-batch 810/4459:  Train loss: 2.9303791522979736  Test loss: 3.358724594116211 \n",
      "Epoch: 1/10:  mini-batch 811/4459:  Train loss: 3.048635482788086  Test loss: 3.357447862625122 \n",
      "Epoch: 1/10:  mini-batch 812/4459:  Train loss: 2.9876716136932373  Test loss: 3.356110095977783 \n",
      "Epoch: 1/10:  mini-batch 813/4459:  Train loss: 3.0747642517089844  Test loss: 3.354755163192749 \n",
      "Epoch: 1/10:  mini-batch 814/4459:  Train loss: 3.1826388835906982  Test loss: 3.354063034057617 \n",
      "Epoch: 1/10:  mini-batch 815/4459:  Train loss: 2.9224977493286133  Test loss: 3.354935646057129 \n",
      "Epoch: 1/10:  mini-batch 816/4459:  Train loss: 4.012943267822266  Test loss: 3.3547961711883545 \n",
      "Epoch: 1/10:  mini-batch 817/4459:  Train loss: 3.2196340560913086  Test loss: 3.354729652404785 \n",
      "Epoch: 1/10:  mini-batch 818/4459:  Train loss: 3.5978851318359375  Test loss: 3.3547544479370117 \n",
      "Epoch: 1/10:  mini-batch 819/4459:  Train loss: 3.1409852504730225  Test loss: 3.3547255992889404 \n",
      "Epoch: 1/10:  mini-batch 820/4459:  Train loss: 3.5738019943237305  Test loss: 3.354384422302246 \n",
      "Epoch: 1/10:  mini-batch 821/4459:  Train loss: 2.9895825386047363  Test loss: 3.3540287017822266 \n",
      "Epoch: 1/10:  mini-batch 822/4459:  Train loss: 3.1985206604003906  Test loss: 3.3528385162353516 \n",
      "Epoch: 1/10:  mini-batch 823/4459:  Train loss: 3.2175662517547607  Test loss: 3.3519701957702637 \n",
      "Epoch: 1/10:  mini-batch 824/4459:  Train loss: 3.184995174407959  Test loss: 3.351901054382324 \n",
      "Epoch: 1/10:  mini-batch 825/4459:  Train loss: 3.3162665367126465  Test loss: 3.351706027984619 \n",
      "Epoch: 1/10:  mini-batch 826/4459:  Train loss: 3.5066299438476562  Test loss: 3.3508334159851074 \n",
      "Epoch: 1/10:  mini-batch 827/4459:  Train loss: 3.326105833053589  Test loss: 3.3484089374542236 \n",
      "Epoch: 1/10:  mini-batch 828/4459:  Train loss: 2.9578778743743896  Test loss: 3.3461227416992188 \n",
      "Epoch: 1/10:  mini-batch 829/4459:  Train loss: 3.173612594604492  Test loss: 3.344449758529663 \n",
      "Epoch: 1/10:  mini-batch 830/4459:  Train loss: 2.8154945373535156  Test loss: 3.343514919281006 \n",
      "Epoch: 1/10:  mini-batch 831/4459:  Train loss: 2.8834352493286133  Test loss: 3.3433570861816406 \n",
      "Epoch: 1/10:  mini-batch 832/4459:  Train loss: 3.579413414001465  Test loss: 3.342658519744873 \n",
      "Epoch: 1/10:  mini-batch 833/4459:  Train loss: 3.864715814590454  Test loss: 3.3406119346618652 \n",
      "Epoch: 1/10:  mini-batch 834/4459:  Train loss: 3.413182258605957  Test loss: 3.33889102935791 \n",
      "Epoch: 1/10:  mini-batch 835/4459:  Train loss: 3.4325952529907227  Test loss: 3.3374500274658203 \n",
      "Epoch: 1/10:  mini-batch 836/4459:  Train loss: 3.5838356018066406  Test loss: 3.3358616828918457 \n",
      "Epoch: 1/10:  mini-batch 837/4459:  Train loss: 3.080230951309204  Test loss: 3.3343870639801025 \n",
      "Epoch: 1/10:  mini-batch 838/4459:  Train loss: 2.7975423336029053  Test loss: 3.3339569568634033 \n",
      "Epoch: 1/10:  mini-batch 839/4459:  Train loss: 3.6474788188934326  Test loss: 3.3334388732910156 \n",
      "Epoch: 1/10:  mini-batch 840/4459:  Train loss: 3.652434825897217  Test loss: 3.331983804702759 \n",
      "Epoch: 1/10:  mini-batch 841/4459:  Train loss: 3.787116527557373  Test loss: 3.33152437210083 \n",
      "Epoch: 1/10:  mini-batch 842/4459:  Train loss: 3.2150821685791016  Test loss: 3.3314545154571533 \n",
      "Epoch: 1/10:  mini-batch 843/4459:  Train loss: 3.5565831661224365  Test loss: 3.3313212394714355 \n",
      "Epoch: 1/10:  mini-batch 844/4459:  Train loss: 3.075348138809204  Test loss: 3.3317782878875732 \n",
      "Epoch: 1/10:  mini-batch 845/4459:  Train loss: 3.283517360687256  Test loss: 3.332402229309082 \n",
      "Epoch: 1/10:  mini-batch 846/4459:  Train loss: 3.12015438079834  Test loss: 3.332834005355835 \n",
      "Epoch: 1/10:  mini-batch 847/4459:  Train loss: 3.2462947368621826  Test loss: 3.333307981491089 \n",
      "Epoch: 1/10:  mini-batch 848/4459:  Train loss: 3.5141243934631348  Test loss: 3.333652973175049 \n",
      "Epoch: 1/10:  mini-batch 849/4459:  Train loss: 3.6012117862701416  Test loss: 3.3343303203582764 \n",
      "Epoch: 1/10:  mini-batch 850/4459:  Train loss: 3.5160179138183594  Test loss: 3.335564613342285 \n",
      "Epoch: 1/10:  mini-batch 851/4459:  Train loss: 3.429202079772949  Test loss: 3.337024450302124 \n",
      "Epoch: 1/10:  mini-batch 852/4459:  Train loss: 2.9502830505371094  Test loss: 3.338169574737549 \n",
      "Epoch: 1/10:  mini-batch 853/4459:  Train loss: 3.6847009658813477  Test loss: 3.3393032550811768 \n",
      "Epoch: 1/10:  mini-batch 854/4459:  Train loss: 3.4296679496765137  Test loss: 3.340125322341919 \n",
      "Epoch: 1/10:  mini-batch 855/4459:  Train loss: 3.681656837463379  Test loss: 3.341376781463623 \n",
      "Epoch: 1/10:  mini-batch 856/4459:  Train loss: 2.858659029006958  Test loss: 3.342315912246704 \n",
      "Epoch: 1/10:  mini-batch 857/4459:  Train loss: 3.271772861480713  Test loss: 3.3429489135742188 \n",
      "Epoch: 1/10:  mini-batch 858/4459:  Train loss: 3.1764864921569824  Test loss: 3.343463182449341 \n",
      "Epoch: 1/10:  mini-batch 859/4459:  Train loss: 3.1573081016540527  Test loss: 3.3437142372131348 \n",
      "Epoch: 1/10:  mini-batch 860/4459:  Train loss: 3.4971065521240234  Test loss: 3.3442156314849854 \n",
      "Epoch: 1/10:  mini-batch 861/4459:  Train loss: 3.1861019134521484  Test loss: 3.3447508811950684 \n",
      "Epoch: 1/10:  mini-batch 862/4459:  Train loss: 3.543774127960205  Test loss: 3.3453245162963867 \n",
      "Epoch: 1/10:  mini-batch 863/4459:  Train loss: 3.3784055709838867  Test loss: 3.345946788787842 \n",
      "Epoch: 1/10:  mini-batch 864/4459:  Train loss: 3.4246954917907715  Test loss: 3.3464195728302 \n",
      "Epoch: 1/10:  mini-batch 865/4459:  Train loss: 3.525034189224243  Test loss: 3.3469650745391846 \n",
      "Epoch: 1/10:  mini-batch 866/4459:  Train loss: 3.6734933853149414  Test loss: 3.347931385040283 \n",
      "Epoch: 1/10:  mini-batch 867/4459:  Train loss: 3.152336359024048  Test loss: 3.348712205886841 \n",
      "Epoch: 1/10:  mini-batch 868/4459:  Train loss: 3.4660933017730713  Test loss: 3.34989595413208 \n",
      "Epoch: 1/10:  mini-batch 869/4459:  Train loss: 3.2624356746673584  Test loss: 3.3511128425598145 \n",
      "Epoch: 1/10:  mini-batch 870/4459:  Train loss: 3.2388017177581787  Test loss: 3.352165937423706 \n",
      "Epoch: 1/10:  mini-batch 871/4459:  Train loss: 3.683940887451172  Test loss: 3.353372097015381 \n",
      "Epoch: 1/10:  mini-batch 872/4459:  Train loss: 3.0869076251983643  Test loss: 3.3545405864715576 \n",
      "Epoch: 1/10:  mini-batch 873/4459:  Train loss: 3.32798433303833  Test loss: 3.3556504249572754 \n",
      "Epoch: 1/10:  mini-batch 874/4459:  Train loss: 3.1462254524230957  Test loss: 3.356452465057373 \n",
      "Epoch: 1/10:  mini-batch 875/4459:  Train loss: 2.991983413696289  Test loss: 3.3572049140930176 \n",
      "Epoch: 1/10:  mini-batch 876/4459:  Train loss: 3.0867176055908203  Test loss: 3.357743740081787 \n",
      "Epoch: 1/10:  mini-batch 877/4459:  Train loss: 3.107868194580078  Test loss: 3.3582751750946045 \n",
      "Epoch: 1/10:  mini-batch 878/4459:  Train loss: 3.206803321838379  Test loss: 3.359304904937744 \n",
      "Epoch: 1/10:  mini-batch 879/4459:  Train loss: 3.14687442779541  Test loss: 3.360466480255127 \n",
      "Epoch: 1/10:  mini-batch 880/4459:  Train loss: 3.7756495475769043  Test loss: 3.3619275093078613 \n",
      "Epoch: 1/10:  mini-batch 881/4459:  Train loss: 3.6403656005859375  Test loss: 3.3630568981170654 \n",
      "Epoch: 1/10:  mini-batch 882/4459:  Train loss: 3.171741247177124  Test loss: 3.364028215408325 \n",
      "Epoch: 1/10:  mini-batch 883/4459:  Train loss: 2.96012544631958  Test loss: 3.365004539489746 \n",
      "Epoch: 1/10:  mini-batch 884/4459:  Train loss: 3.028768301010132  Test loss: 3.365846872329712 \n",
      "Epoch: 1/10:  mini-batch 885/4459:  Train loss: 3.2395951747894287  Test loss: 3.3665218353271484 \n",
      "Epoch: 1/10:  mini-batch 886/4459:  Train loss: 2.9766077995300293  Test loss: 3.3679909706115723 \n",
      "Epoch: 1/10:  mini-batch 887/4459:  Train loss: 3.2254974842071533  Test loss: 3.370345115661621 \n",
      "Epoch: 1/10:  mini-batch 888/4459:  Train loss: 3.2373948097229004  Test loss: 3.3724875450134277 \n",
      "Epoch: 1/10:  mini-batch 889/4459:  Train loss: 3.462120771408081  Test loss: 3.3738908767700195 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 890/4459:  Train loss: 2.902163505554199  Test loss: 3.3761444091796875 \n",
      "Epoch: 1/10:  mini-batch 891/4459:  Train loss: 3.0881052017211914  Test loss: 3.3785316944122314 \n",
      "Epoch: 1/10:  mini-batch 892/4459:  Train loss: 2.788543224334717  Test loss: 3.38193941116333 \n",
      "Epoch: 1/10:  mini-batch 893/4459:  Train loss: 3.7579715251922607  Test loss: 3.3845295906066895 \n",
      "Epoch: 1/10:  mini-batch 894/4459:  Train loss: 3.1393344402313232  Test loss: 3.3873085975646973 \n",
      "Epoch: 1/10:  mini-batch 895/4459:  Train loss: 3.580848217010498  Test loss: 3.3884315490722656 \n",
      "Epoch: 1/10:  mini-batch 896/4459:  Train loss: 3.040226459503174  Test loss: 3.38958740234375 \n",
      "Epoch: 1/10:  mini-batch 897/4459:  Train loss: 3.2975919246673584  Test loss: 3.3902668952941895 \n",
      "Epoch: 1/10:  mini-batch 898/4459:  Train loss: 3.5818560123443604  Test loss: 3.389302968978882 \n",
      "Epoch: 1/10:  mini-batch 899/4459:  Train loss: 3.4582386016845703  Test loss: 3.388024091720581 \n",
      "Epoch: 1/10:  mini-batch 900/4459:  Train loss: 2.6711013317108154  Test loss: 3.3888556957244873 \n",
      "Epoch: 1/10:  mini-batch 901/4459:  Train loss: 3.267988681793213  Test loss: 3.3886661529541016 \n",
      "Epoch: 1/10:  mini-batch 902/4459:  Train loss: 3.467156410217285  Test loss: 3.387885570526123 \n",
      "Epoch: 1/10:  mini-batch 903/4459:  Train loss: 3.1864471435546875  Test loss: 3.386756181716919 \n",
      "Epoch: 1/10:  mini-batch 904/4459:  Train loss: 2.91072940826416  Test loss: 3.3859786987304688 \n",
      "Epoch: 1/10:  mini-batch 905/4459:  Train loss: 3.077316999435425  Test loss: 3.3854258060455322 \n",
      "Epoch: 1/10:  mini-batch 906/4459:  Train loss: 3.426419258117676  Test loss: 3.3845622539520264 \n",
      "Epoch: 1/10:  mini-batch 907/4459:  Train loss: 2.9624578952789307  Test loss: 3.38407564163208 \n",
      "Epoch: 1/10:  mini-batch 908/4459:  Train loss: 3.8526463508605957  Test loss: 3.3816583156585693 \n",
      "Epoch: 1/10:  mini-batch 909/4459:  Train loss: 3.714028835296631  Test loss: 3.3783469200134277 \n",
      "Epoch: 1/10:  mini-batch 910/4459:  Train loss: 3.1518850326538086  Test loss: 3.3757481575012207 \n",
      "Epoch: 1/10:  mini-batch 911/4459:  Train loss: 3.6184535026550293  Test loss: 3.3719441890716553 \n",
      "Epoch: 1/10:  mini-batch 912/4459:  Train loss: 3.681971549987793  Test loss: 3.368234872817993 \n",
      "Epoch: 1/10:  mini-batch 913/4459:  Train loss: 3.5322532653808594  Test loss: 3.36480450630188 \n",
      "Epoch: 1/10:  mini-batch 914/4459:  Train loss: 3.4893875122070312  Test loss: 3.3615200519561768 \n",
      "Epoch: 1/10:  mini-batch 915/4459:  Train loss: 3.7266244888305664  Test loss: 3.3589720726013184 \n",
      "Epoch: 1/10:  mini-batch 916/4459:  Train loss: 3.4918103218078613  Test loss: 3.357369899749756 \n",
      "Epoch: 1/10:  mini-batch 917/4459:  Train loss: 3.190723180770874  Test loss: 3.3563950061798096 \n",
      "Epoch: 1/10:  mini-batch 918/4459:  Train loss: 3.4657764434814453  Test loss: 3.355821371078491 \n",
      "Epoch: 1/10:  mini-batch 919/4459:  Train loss: 3.647359848022461  Test loss: 3.3557395935058594 \n",
      "Epoch: 1/10:  mini-batch 920/4459:  Train loss: 3.115396738052368  Test loss: 3.3561301231384277 \n",
      "Epoch: 1/10:  mini-batch 921/4459:  Train loss: 2.983623504638672  Test loss: 3.3568038940429688 \n",
      "Epoch: 1/10:  mini-batch 922/4459:  Train loss: 3.0161640644073486  Test loss: 3.3571605682373047 \n",
      "Epoch: 1/10:  mini-batch 923/4459:  Train loss: 2.8740217685699463  Test loss: 3.357639789581299 \n",
      "Epoch: 1/10:  mini-batch 924/4459:  Train loss: 3.3453562259674072  Test loss: 3.357966184616089 \n",
      "Epoch: 1/10:  mini-batch 925/4459:  Train loss: 3.1968674659729004  Test loss: 3.3581929206848145 \n",
      "Epoch: 1/10:  mini-batch 926/4459:  Train loss: 3.2241334915161133  Test loss: 3.3583054542541504 \n",
      "Epoch: 1/10:  mini-batch 927/4459:  Train loss: 3.775386095046997  Test loss: 3.3586583137512207 \n",
      "Epoch: 1/10:  mini-batch 928/4459:  Train loss: 3.2229719161987305  Test loss: 3.3589882850646973 \n",
      "Epoch: 1/10:  mini-batch 929/4459:  Train loss: 3.426182270050049  Test loss: 3.3591010570526123 \n",
      "Epoch: 1/10:  mini-batch 930/4459:  Train loss: 3.1755011081695557  Test loss: 3.359055995941162 \n",
      "Epoch: 1/10:  mini-batch 931/4459:  Train loss: 3.315030574798584  Test loss: 3.358786106109619 \n",
      "Epoch: 1/10:  mini-batch 932/4459:  Train loss: 3.3178956508636475  Test loss: 3.3583765029907227 \n",
      "Epoch: 1/10:  mini-batch 933/4459:  Train loss: 3.2714078426361084  Test loss: 3.3580713272094727 \n",
      "Epoch: 1/10:  mini-batch 934/4459:  Train loss: 3.610358953475952  Test loss: 3.357966899871826 \n",
      "Epoch: 1/10:  mini-batch 935/4459:  Train loss: 2.9843339920043945  Test loss: 3.358095645904541 \n",
      "Epoch: 1/10:  mini-batch 936/4459:  Train loss: 3.1830434799194336  Test loss: 3.358330726623535 \n",
      "Epoch: 1/10:  mini-batch 937/4459:  Train loss: 3.0437519550323486  Test loss: 3.3583061695098877 \n",
      "Epoch: 1/10:  mini-batch 938/4459:  Train loss: 3.2587482929229736  Test loss: 3.3587560653686523 \n",
      "Epoch: 1/10:  mini-batch 939/4459:  Train loss: 3.391000747680664  Test loss: 3.358888864517212 \n",
      "Epoch: 1/10:  mini-batch 940/4459:  Train loss: 2.9697699546813965  Test loss: 3.3594837188720703 \n",
      "Epoch: 1/10:  mini-batch 941/4459:  Train loss: 3.329206943511963  Test loss: 3.359877586364746 \n",
      "Epoch: 1/10:  mini-batch 942/4459:  Train loss: 2.9239959716796875  Test loss: 3.360841751098633 \n",
      "Epoch: 1/10:  mini-batch 943/4459:  Train loss: 3.3181190490722656  Test loss: 3.3618927001953125 \n",
      "Epoch: 1/10:  mini-batch 944/4459:  Train loss: 2.99039363861084  Test loss: 3.363337993621826 \n",
      "Epoch: 1/10:  mini-batch 945/4459:  Train loss: 2.8472208976745605  Test loss: 3.3656105995178223 \n",
      "Epoch: 1/10:  mini-batch 946/4459:  Train loss: 3.30285906791687  Test loss: 3.3681159019470215 \n",
      "Epoch: 1/10:  mini-batch 947/4459:  Train loss: 3.3936715126037598  Test loss: 3.3706095218658447 \n",
      "Epoch: 1/10:  mini-batch 948/4459:  Train loss: 2.8763813972473145  Test loss: 3.3739054203033447 \n",
      "Epoch: 1/10:  mini-batch 949/4459:  Train loss: 3.152482032775879  Test loss: 3.3768134117126465 \n",
      "Epoch: 1/10:  mini-batch 950/4459:  Train loss: 3.1973628997802734  Test loss: 3.3794898986816406 \n",
      "Epoch: 1/10:  mini-batch 951/4459:  Train loss: 3.3765077590942383  Test loss: 3.3813369274139404 \n",
      "Epoch: 1/10:  mini-batch 952/4459:  Train loss: 3.098641872406006  Test loss: 3.384049654006958 \n",
      "Epoch: 1/10:  mini-batch 953/4459:  Train loss: 2.91414213180542  Test loss: 3.3877172470092773 \n",
      "Epoch: 1/10:  mini-batch 954/4459:  Train loss: 3.473177433013916  Test loss: 3.390246868133545 \n",
      "Epoch: 1/10:  mini-batch 955/4459:  Train loss: 3.3253962993621826  Test loss: 3.3916149139404297 \n",
      "Epoch: 1/10:  mini-batch 956/4459:  Train loss: 3.3688862323760986  Test loss: 3.392345666885376 \n",
      "Epoch: 1/10:  mini-batch 957/4459:  Train loss: 3.2186410427093506  Test loss: 3.3927879333496094 \n",
      "Epoch: 1/10:  mini-batch 958/4459:  Train loss: 3.211787700653076  Test loss: 3.393369197845459 \n",
      "Epoch: 1/10:  mini-batch 959/4459:  Train loss: 3.1331448554992676  Test loss: 3.3938534259796143 \n",
      "Epoch: 1/10:  mini-batch 960/4459:  Train loss: 3.1698758602142334  Test loss: 3.394512414932251 \n",
      "Epoch: 1/10:  mini-batch 961/4459:  Train loss: 3.21252179145813  Test loss: 3.3945178985595703 \n",
      "Epoch: 1/10:  mini-batch 962/4459:  Train loss: 3.7769229412078857  Test loss: 3.3922314643859863 \n",
      "Epoch: 1/10:  mini-batch 963/4459:  Train loss: 3.407806396484375  Test loss: 3.389918804168701 \n",
      "Epoch: 1/10:  mini-batch 964/4459:  Train loss: 3.4113173484802246  Test loss: 3.3861351013183594 \n",
      "Epoch: 1/10:  mini-batch 965/4459:  Train loss: 3.499932050704956  Test loss: 3.381887197494507 \n",
      "Epoch: 1/10:  mini-batch 966/4459:  Train loss: 3.012235164642334  Test loss: 3.3791580200195312 \n",
      "Epoch: 1/10:  mini-batch 967/4459:  Train loss: 3.5006070137023926  Test loss: 3.376174211502075 \n",
      "Epoch: 1/10:  mini-batch 968/4459:  Train loss: 3.453885555267334  Test loss: 3.3731143474578857 \n",
      "Epoch: 1/10:  mini-batch 969/4459:  Train loss: 3.7577407360076904  Test loss: 3.3691320419311523 \n",
      "Epoch: 1/10:  mini-batch 970/4459:  Train loss: 3.9901814460754395  Test loss: 3.3649721145629883 \n",
      "Epoch: 1/10:  mini-batch 971/4459:  Train loss: 3.517263889312744  Test loss: 3.3623878955841064 \n",
      "Epoch: 1/10:  mini-batch 972/4459:  Train loss: 3.3440349102020264  Test loss: 3.3604893684387207 \n",
      "Epoch: 1/10:  mini-batch 973/4459:  Train loss: 2.9068992137908936  Test loss: 3.3599536418914795 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 974/4459:  Train loss: 3.6483349800109863  Test loss: 3.3593783378601074 \n",
      "Epoch: 1/10:  mini-batch 975/4459:  Train loss: 3.287419319152832  Test loss: 3.3592731952667236 \n",
      "Epoch: 1/10:  mini-batch 976/4459:  Train loss: 3.129469394683838  Test loss: 3.358430862426758 \n",
      "Epoch: 1/10:  mini-batch 977/4459:  Train loss: 2.9494705200195312  Test loss: 3.35758113861084 \n",
      "Epoch: 1/10:  mini-batch 978/4459:  Train loss: 2.982625961303711  Test loss: 3.3569769859313965 \n",
      "Epoch: 1/10:  mini-batch 979/4459:  Train loss: 3.4395599365234375  Test loss: 3.3567304611206055 \n",
      "Epoch: 1/10:  mini-batch 980/4459:  Train loss: 3.235821485519409  Test loss: 3.356706142425537 \n",
      "Epoch: 1/10:  mini-batch 981/4459:  Train loss: 2.957396984100342  Test loss: 3.3566715717315674 \n",
      "Epoch: 1/10:  mini-batch 982/4459:  Train loss: 3.144284248352051  Test loss: 3.3564066886901855 \n",
      "Epoch: 1/10:  mini-batch 983/4459:  Train loss: 3.38806414604187  Test loss: 3.355886936187744 \n",
      "Epoch: 1/10:  mini-batch 984/4459:  Train loss: 3.3653202056884766  Test loss: 3.3556127548217773 \n",
      "Epoch: 1/10:  mini-batch 985/4459:  Train loss: 3.430370807647705  Test loss: 3.3551864624023438 \n",
      "Epoch: 1/10:  mini-batch 986/4459:  Train loss: 3.38045334815979  Test loss: 3.3546195030212402 \n",
      "Epoch: 1/10:  mini-batch 987/4459:  Train loss: 3.2882723808288574  Test loss: 3.3539295196533203 \n",
      "Epoch: 1/10:  mini-batch 988/4459:  Train loss: 3.6166810989379883  Test loss: 3.3533244132995605 \n",
      "Epoch: 1/10:  mini-batch 989/4459:  Train loss: 3.3745903968811035  Test loss: 3.352442979812622 \n",
      "Epoch: 1/10:  mini-batch 990/4459:  Train loss: 3.102846622467041  Test loss: 3.3513128757476807 \n",
      "Epoch: 1/10:  mini-batch 991/4459:  Train loss: 3.4010229110717773  Test loss: 3.3503170013427734 \n",
      "Epoch: 1/10:  mini-batch 992/4459:  Train loss: 3.2087554931640625  Test loss: 3.349886417388916 \n",
      "Epoch: 1/10:  mini-batch 993/4459:  Train loss: 3.104093551635742  Test loss: 3.3491415977478027 \n",
      "Epoch: 1/10:  mini-batch 994/4459:  Train loss: 3.278032064437866  Test loss: 3.348301887512207 \n",
      "Epoch: 1/10:  mini-batch 995/4459:  Train loss: 3.4736461639404297  Test loss: 3.347158908843994 \n",
      "Epoch: 1/10:  mini-batch 996/4459:  Train loss: 3.619201898574829  Test loss: 3.3464159965515137 \n",
      "Epoch: 1/10:  mini-batch 997/4459:  Train loss: 3.637402057647705  Test loss: 3.345829486846924 \n",
      "Epoch: 1/10:  mini-batch 998/4459:  Train loss: 3.278167486190796  Test loss: 3.3451812267303467 \n",
      "Epoch: 1/10:  mini-batch 999/4459:  Train loss: 3.2824690341949463  Test loss: 3.3447558879852295 \n",
      "Epoch: 1/10:  mini-batch 1000/4459:  Train loss: 3.186267375946045  Test loss: 3.344561815261841 \n",
      "Epoch: 1/10:  mini-batch 1001/4459:  Train loss: 3.200948715209961  Test loss: 3.344151496887207 \n",
      "Epoch: 1/10:  mini-batch 1002/4459:  Train loss: 3.3523902893066406  Test loss: 3.343451499938965 \n",
      "Epoch: 1/10:  mini-batch 1003/4459:  Train loss: 2.95417857170105  Test loss: 3.3430089950561523 \n",
      "Epoch: 1/10:  mini-batch 1004/4459:  Train loss: 3.139369487762451  Test loss: 3.3430960178375244 \n",
      "Epoch: 1/10:  mini-batch 1005/4459:  Train loss: 3.36860728263855  Test loss: 3.342595100402832 \n",
      "Epoch: 1/10:  mini-batch 1006/4459:  Train loss: 3.488698720932007  Test loss: 3.3422818183898926 \n",
      "Epoch: 1/10:  mini-batch 1007/4459:  Train loss: 3.0350046157836914  Test loss: 3.3421924114227295 \n",
      "Epoch: 1/10:  mini-batch 1008/4459:  Train loss: 3.136246919631958  Test loss: 3.34149432182312 \n",
      "Epoch: 1/10:  mini-batch 1009/4459:  Train loss: 2.965245246887207  Test loss: 3.341047763824463 \n",
      "Epoch: 1/10:  mini-batch 1010/4459:  Train loss: 3.180616617202759  Test loss: 3.34084153175354 \n",
      "Epoch: 1/10:  mini-batch 1011/4459:  Train loss: 3.3418965339660645  Test loss: 3.3406851291656494 \n",
      "Epoch: 1/10:  mini-batch 1012/4459:  Train loss: 3.2926671504974365  Test loss: 3.3407301902770996 \n",
      "Epoch: 1/10:  mini-batch 1013/4459:  Train loss: 2.8505325317382812  Test loss: 3.3416876792907715 \n",
      "Epoch: 1/10:  mini-batch 1014/4459:  Train loss: 3.2789459228515625  Test loss: 3.343029260635376 \n",
      "Epoch: 1/10:  mini-batch 1015/4459:  Train loss: 2.9485044479370117  Test loss: 3.3450169563293457 \n",
      "Epoch: 1/10:  mini-batch 1016/4459:  Train loss: 3.225343704223633  Test loss: 3.346672534942627 \n",
      "Epoch: 1/10:  mini-batch 1017/4459:  Train loss: 3.098043203353882  Test loss: 3.348973512649536 \n",
      "Epoch: 1/10:  mini-batch 1018/4459:  Train loss: 2.7354683876037598  Test loss: 3.352756977081299 \n",
      "Epoch: 1/10:  mini-batch 1019/4459:  Train loss: 3.116664409637451  Test loss: 3.3563973903656006 \n",
      "Epoch: 1/10:  mini-batch 1020/4459:  Train loss: 3.5282175540924072  Test loss: 3.358947515487671 \n",
      "Epoch: 1/10:  mini-batch 1021/4459:  Train loss: 3.611690044403076  Test loss: 3.3604800701141357 \n",
      "Epoch: 1/10:  mini-batch 1022/4459:  Train loss: 3.4596104621887207  Test loss: 3.3618502616882324 \n",
      "Epoch: 1/10:  mini-batch 1023/4459:  Train loss: 3.289381504058838  Test loss: 3.3622779846191406 \n",
      "Epoch: 1/10:  mini-batch 1024/4459:  Train loss: 3.2228264808654785  Test loss: 3.3625168800354004 \n",
      "Epoch: 1/10:  mini-batch 1025/4459:  Train loss: 3.4953320026397705  Test loss: 3.3622775077819824 \n",
      "Epoch: 1/10:  mini-batch 1026/4459:  Train loss: 3.2707998752593994  Test loss: 3.361525774002075 \n",
      "Epoch: 1/10:  mini-batch 1027/4459:  Train loss: 3.1873066425323486  Test loss: 3.360016345977783 \n",
      "Epoch: 1/10:  mini-batch 1028/4459:  Train loss: 3.039325714111328  Test loss: 3.3582377433776855 \n",
      "Epoch: 1/10:  mini-batch 1029/4459:  Train loss: 3.7903361320495605  Test loss: 3.355363130569458 \n",
      "Epoch: 1/10:  mini-batch 1030/4459:  Train loss: 3.7233757972717285  Test loss: 3.351821184158325 \n",
      "Epoch: 1/10:  mini-batch 1031/4459:  Train loss: 3.8753838539123535  Test loss: 3.348086357116699 \n",
      "Epoch: 1/10:  mini-batch 1032/4459:  Train loss: 3.346719741821289  Test loss: 3.3451528549194336 \n",
      "Epoch: 1/10:  mini-batch 1033/4459:  Train loss: 3.1416449546813965  Test loss: 3.3434135913848877 \n",
      "Epoch: 1/10:  mini-batch 1034/4459:  Train loss: 3.345040798187256  Test loss: 3.3418009281158447 \n",
      "Epoch: 1/10:  mini-batch 1035/4459:  Train loss: 3.3561501502990723  Test loss: 3.340245246887207 \n",
      "Epoch: 1/10:  mini-batch 1036/4459:  Train loss: 2.986412286758423  Test loss: 3.339202880859375 \n",
      "Epoch: 1/10:  mini-batch 1037/4459:  Train loss: 3.017199993133545  Test loss: 3.3384344577789307 \n",
      "Epoch: 1/10:  mini-batch 1038/4459:  Train loss: 3.2427237033843994  Test loss: 3.3378827571868896 \n",
      "Epoch: 1/10:  mini-batch 1039/4459:  Train loss: 3.2201709747314453  Test loss: 3.336866855621338 \n",
      "Epoch: 1/10:  mini-batch 1040/4459:  Train loss: 3.562051773071289  Test loss: 3.3358497619628906 \n",
      "Epoch: 1/10:  mini-batch 1041/4459:  Train loss: 3.0916926860809326  Test loss: 3.334986925125122 \n",
      "Epoch: 1/10:  mini-batch 1042/4459:  Train loss: 3.753377676010132  Test loss: 3.3345718383789062 \n",
      "Epoch: 1/10:  mini-batch 1043/4459:  Train loss: 3.516033172607422  Test loss: 3.33408784866333 \n",
      "Epoch: 1/10:  mini-batch 1044/4459:  Train loss: 3.880625009536743  Test loss: 3.3341660499572754 \n",
      "Epoch: 1/10:  mini-batch 1045/4459:  Train loss: 3.7378745079040527  Test loss: 3.334249973297119 \n",
      "Epoch: 1/10:  mini-batch 1046/4459:  Train loss: 3.131842851638794  Test loss: 3.334306001663208 \n",
      "Epoch: 1/10:  mini-batch 1047/4459:  Train loss: 3.4019594192504883  Test loss: 3.334369421005249 \n",
      "Epoch: 1/10:  mini-batch 1048/4459:  Train loss: 3.128708839416504  Test loss: 3.3344433307647705 \n",
      "Epoch: 1/10:  mini-batch 1049/4459:  Train loss: 3.1681575775146484  Test loss: 3.334524631500244 \n",
      "Epoch: 1/10:  mini-batch 1050/4459:  Train loss: 3.6908340454101562  Test loss: 3.334587574005127 \n",
      "Epoch: 1/10:  mini-batch 1051/4459:  Train loss: 3.6166579723358154  Test loss: 3.33552885055542 \n",
      "Epoch: 1/10:  mini-batch 1052/4459:  Train loss: 2.834237575531006  Test loss: 3.3362984657287598 \n",
      "Epoch: 1/10:  mini-batch 1053/4459:  Train loss: 2.9895272254943848  Test loss: 3.3367507457733154 \n",
      "Epoch: 1/10:  mini-batch 1054/4459:  Train loss: 3.1569838523864746  Test loss: 3.336702585220337 \n",
      "Epoch: 1/10:  mini-batch 1055/4459:  Train loss: 3.4373323917388916  Test loss: 3.3371100425720215 \n",
      "Epoch: 1/10:  mini-batch 1056/4459:  Train loss: 3.566253662109375  Test loss: 3.337453842163086 \n",
      "Epoch: 1/10:  mini-batch 1057/4459:  Train loss: 3.251269817352295  Test loss: 3.3374083042144775 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1058/4459:  Train loss: 3.365046501159668  Test loss: 3.33766770362854 \n",
      "Epoch: 1/10:  mini-batch 1059/4459:  Train loss: 3.2791714668273926  Test loss: 3.3376235961914062 \n",
      "Epoch: 1/10:  mini-batch 1060/4459:  Train loss: 3.2178525924682617  Test loss: 3.3377301692962646 \n",
      "Epoch: 1/10:  mini-batch 1061/4459:  Train loss: 3.5852277278900146  Test loss: 3.3380346298217773 \n",
      "Epoch: 1/10:  mini-batch 1062/4459:  Train loss: 3.252347707748413  Test loss: 3.337672710418701 \n",
      "Epoch: 1/10:  mini-batch 1063/4459:  Train loss: 3.6742753982543945  Test loss: 3.3377532958984375 \n",
      "Epoch: 1/10:  mini-batch 1064/4459:  Train loss: 3.571978807449341  Test loss: 3.338052749633789 \n",
      "Epoch: 1/10:  mini-batch 1065/4459:  Train loss: 2.946425437927246  Test loss: 3.338165521621704 \n",
      "Epoch: 1/10:  mini-batch 1066/4459:  Train loss: 3.598846197128296  Test loss: 3.338731050491333 \n",
      "Epoch: 1/10:  mini-batch 1067/4459:  Train loss: 3.286426305770874  Test loss: 3.3391621112823486 \n",
      "Epoch: 1/10:  mini-batch 1068/4459:  Train loss: 3.269369602203369  Test loss: 3.339303970336914 \n",
      "Epoch: 1/10:  mini-batch 1069/4459:  Train loss: 3.3298325538635254  Test loss: 3.339482307434082 \n",
      "Epoch: 1/10:  mini-batch 1070/4459:  Train loss: 2.807957410812378  Test loss: 3.3392562866210938 \n",
      "Epoch: 1/10:  mini-batch 1071/4459:  Train loss: 3.25822377204895  Test loss: 3.339050769805908 \n",
      "Epoch: 1/10:  mini-batch 1072/4459:  Train loss: 3.5503005981445312  Test loss: 3.3387818336486816 \n",
      "Epoch: 1/10:  mini-batch 1073/4459:  Train loss: 3.064936637878418  Test loss: 3.337939739227295 \n",
      "Epoch: 1/10:  mini-batch 1074/4459:  Train loss: 3.2641544342041016  Test loss: 3.3367981910705566 \n",
      "Epoch: 1/10:  mini-batch 1075/4459:  Train loss: 3.507662773132324  Test loss: 3.335083484649658 \n",
      "Epoch: 1/10:  mini-batch 1076/4459:  Train loss: 3.2568628787994385  Test loss: 3.3333401679992676 \n",
      "Epoch: 1/10:  mini-batch 1077/4459:  Train loss: 2.979888916015625  Test loss: 3.3320298194885254 \n",
      "Epoch: 1/10:  mini-batch 1078/4459:  Train loss: 3.814115047454834  Test loss: 3.3312740325927734 \n",
      "Epoch: 1/10:  mini-batch 1079/4459:  Train loss: 3.5731005668640137  Test loss: 3.3308956623077393 \n",
      "Epoch: 1/10:  mini-batch 1080/4459:  Train loss: 3.1700122356414795  Test loss: 3.330827236175537 \n",
      "Epoch: 1/10:  mini-batch 1081/4459:  Train loss: 3.1972997188568115  Test loss: 3.3310165405273438 \n",
      "Epoch: 1/10:  mini-batch 1082/4459:  Train loss: 3.5517466068267822  Test loss: 3.3314943313598633 \n",
      "Epoch: 1/10:  mini-batch 1083/4459:  Train loss: 3.459886074066162  Test loss: 3.3315393924713135 \n",
      "Epoch: 1/10:  mini-batch 1084/4459:  Train loss: 3.1568472385406494  Test loss: 3.3314080238342285 \n",
      "Epoch: 1/10:  mini-batch 1085/4459:  Train loss: 3.4521408081054688  Test loss: 3.3310506343841553 \n",
      "Epoch: 1/10:  mini-batch 1086/4459:  Train loss: 2.7629928588867188  Test loss: 3.3308305740356445 \n",
      "Epoch: 1/10:  mini-batch 1087/4459:  Train loss: 3.545072555541992  Test loss: 3.331242084503174 \n",
      "Epoch: 1/10:  mini-batch 1088/4459:  Train loss: 3.7186694145202637  Test loss: 3.331432580947876 \n",
      "Epoch: 1/10:  mini-batch 1089/4459:  Train loss: 3.6596076488494873  Test loss: 3.331697463989258 \n",
      "Epoch: 1/10:  mini-batch 1090/4459:  Train loss: 3.7960143089294434  Test loss: 3.332482099533081 \n",
      "Epoch: 1/10:  mini-batch 1091/4459:  Train loss: 3.680872917175293  Test loss: 3.3339052200317383 \n",
      "Epoch: 1/10:  mini-batch 1092/4459:  Train loss: 3.5729236602783203  Test loss: 3.3356096744537354 \n",
      "Epoch: 1/10:  mini-batch 1093/4459:  Train loss: 3.470789909362793  Test loss: 3.336862325668335 \n",
      "Epoch: 1/10:  mini-batch 1094/4459:  Train loss: 3.1936092376708984  Test loss: 3.3377366065979004 \n",
      "Epoch: 1/10:  mini-batch 1095/4459:  Train loss: 3.372413158416748  Test loss: 3.338517189025879 \n",
      "Epoch: 1/10:  mini-batch 1096/4459:  Train loss: 3.1420059204101562  Test loss: 3.3386709690093994 \n",
      "Epoch: 1/10:  mini-batch 1097/4459:  Train loss: 3.238060474395752  Test loss: 3.338315486907959 \n",
      "Epoch: 1/10:  mini-batch 1098/4459:  Train loss: 3.7628188133239746  Test loss: 3.3385074138641357 \n",
      "Epoch: 1/10:  mini-batch 1099/4459:  Train loss: 3.000303268432617  Test loss: 3.3386027812957764 \n",
      "Epoch: 1/10:  mini-batch 1100/4459:  Train loss: 3.598919153213501  Test loss: 3.339250087738037 \n",
      "Epoch: 1/10:  mini-batch 1101/4459:  Train loss: 3.1866040229797363  Test loss: 3.3397746086120605 \n",
      "Epoch: 1/10:  mini-batch 1102/4459:  Train loss: 3.356752872467041  Test loss: 3.3396477699279785 \n",
      "Epoch: 1/10:  mini-batch 1103/4459:  Train loss: 3.3930251598358154  Test loss: 3.3394434452056885 \n",
      "Epoch: 1/10:  mini-batch 1104/4459:  Train loss: 3.235988140106201  Test loss: 3.3392810821533203 \n",
      "Epoch: 1/10:  mini-batch 1105/4459:  Train loss: 3.3671817779541016  Test loss: 3.3394126892089844 \n",
      "Epoch: 1/10:  mini-batch 1106/4459:  Train loss: 3.5980238914489746  Test loss: 3.3401308059692383 \n",
      "Epoch: 1/10:  mini-batch 1107/4459:  Train loss: 3.390641212463379  Test loss: 3.340951919555664 \n",
      "Epoch: 1/10:  mini-batch 1108/4459:  Train loss: 3.4315996170043945  Test loss: 3.341872453689575 \n",
      "Epoch: 1/10:  mini-batch 1109/4459:  Train loss: 3.619396686553955  Test loss: 3.3428263664245605 \n",
      "Epoch: 1/10:  mini-batch 1110/4459:  Train loss: 3.595123767852783  Test loss: 3.344099760055542 \n",
      "Epoch: 1/10:  mini-batch 1111/4459:  Train loss: 3.312448501586914  Test loss: 3.3452987670898438 \n",
      "Epoch: 1/10:  mini-batch 1112/4459:  Train loss: 3.844287633895874  Test loss: 3.3472819328308105 \n",
      "Epoch: 1/10:  mini-batch 1113/4459:  Train loss: 3.40250563621521  Test loss: 3.3491334915161133 \n",
      "Epoch: 1/10:  mini-batch 1114/4459:  Train loss: 3.525298595428467  Test loss: 3.3507261276245117 \n",
      "Epoch: 1/10:  mini-batch 1115/4459:  Train loss: 3.418498992919922  Test loss: 3.3519082069396973 \n",
      "Epoch: 1/10:  mini-batch 1116/4459:  Train loss: 3.3769145011901855  Test loss: 3.3531107902526855 \n",
      "Epoch: 1/10:  mini-batch 1117/4459:  Train loss: 3.2377841472625732  Test loss: 3.353761672973633 \n",
      "Epoch: 1/10:  mini-batch 1118/4459:  Train loss: 3.6714258193969727  Test loss: 3.3544952869415283 \n",
      "Epoch: 1/10:  mini-batch 1119/4459:  Train loss: 3.5268983840942383  Test loss: 3.3553361892700195 \n",
      "Epoch: 1/10:  mini-batch 1120/4459:  Train loss: 3.2956159114837646  Test loss: 3.3558666706085205 \n",
      "Epoch: 1/10:  mini-batch 1121/4459:  Train loss: 3.3745741844177246  Test loss: 3.3563179969787598 \n",
      "Epoch: 1/10:  mini-batch 1122/4459:  Train loss: 3.4681553840637207  Test loss: 3.356710910797119 \n",
      "Epoch: 1/10:  mini-batch 1123/4459:  Train loss: 3.4915170669555664  Test loss: 3.357114791870117 \n",
      "Epoch: 1/10:  mini-batch 1124/4459:  Train loss: 3.4536547660827637  Test loss: 3.357271671295166 \n",
      "Epoch: 1/10:  mini-batch 1125/4459:  Train loss: 3.5074238777160645  Test loss: 3.3570504188537598 \n",
      "Epoch: 1/10:  mini-batch 1126/4459:  Train loss: 3.3392515182495117  Test loss: 3.356905460357666 \n",
      "Epoch: 1/10:  mini-batch 1127/4459:  Train loss: 3.5799202919006348  Test loss: 3.356904983520508 \n",
      "Epoch: 1/10:  mini-batch 1128/4459:  Train loss: 3.417487621307373  Test loss: 3.356633186340332 \n",
      "Epoch: 1/10:  mini-batch 1129/4459:  Train loss: 3.45863676071167  Test loss: 3.3564043045043945 \n",
      "Epoch: 1/10:  mini-batch 1130/4459:  Train loss: 3.2563788890838623  Test loss: 3.3560383319854736 \n",
      "Epoch: 1/10:  mini-batch 1131/4459:  Train loss: 3.3060665130615234  Test loss: 3.355520725250244 \n",
      "Epoch: 1/10:  mini-batch 1132/4459:  Train loss: 3.3055312633514404  Test loss: 3.354799509048462 \n",
      "Epoch: 1/10:  mini-batch 1133/4459:  Train loss: 3.358072519302368  Test loss: 3.3537888526916504 \n",
      "Epoch: 1/10:  mini-batch 1134/4459:  Train loss: 3.518085479736328  Test loss: 3.353008270263672 \n",
      "Epoch: 1/10:  mini-batch 1135/4459:  Train loss: 3.602975368499756  Test loss: 3.3531134128570557 \n",
      "Epoch: 1/10:  mini-batch 1136/4459:  Train loss: 3.4935450553894043  Test loss: 3.3531503677368164 \n",
      "Epoch: 1/10:  mini-batch 1137/4459:  Train loss: 3.5727789402008057  Test loss: 3.3536558151245117 \n",
      "Epoch: 1/10:  mini-batch 1138/4459:  Train loss: 3.442606210708618  Test loss: 3.3540468215942383 \n",
      "Epoch: 1/10:  mini-batch 1139/4459:  Train loss: 3.5144076347351074  Test loss: 3.3546576499938965 \n",
      "Epoch: 1/10:  mini-batch 1140/4459:  Train loss: 3.531801700592041  Test loss: 3.3555057048797607 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1141/4459:  Train loss: 3.3847384452819824  Test loss: 3.356297016143799 \n",
      "Epoch: 1/10:  mini-batch 1142/4459:  Train loss: 3.3812761306762695  Test loss: 3.3565633296966553 \n",
      "Epoch: 1/10:  mini-batch 1143/4459:  Train loss: 3.26706862449646  Test loss: 3.356264591217041 \n",
      "Epoch: 1/10:  mini-batch 1144/4459:  Train loss: 3.6236400604248047  Test loss: 3.3567276000976562 \n",
      "Epoch: 1/10:  mini-batch 1145/4459:  Train loss: 3.40933895111084  Test loss: 3.3572793006896973 \n",
      "Epoch: 1/10:  mini-batch 1146/4459:  Train loss: 3.2882609367370605  Test loss: 3.357591152191162 \n",
      "Epoch: 1/10:  mini-batch 1147/4459:  Train loss: 3.3167829513549805  Test loss: 3.357584238052368 \n",
      "Epoch: 1/10:  mini-batch 1148/4459:  Train loss: 3.415762424468994  Test loss: 3.357530117034912 \n",
      "Epoch: 1/10:  mini-batch 1149/4459:  Train loss: 3.5615174770355225  Test loss: 3.3573694229125977 \n",
      "Epoch: 1/10:  mini-batch 1150/4459:  Train loss: 3.503181219100952  Test loss: 3.3571016788482666 \n",
      "Epoch: 1/10:  mini-batch 1151/4459:  Train loss: 3.2743141651153564  Test loss: 3.3565969467163086 \n",
      "Epoch: 1/10:  mini-batch 1152/4459:  Train loss: 3.3347744941711426  Test loss: 3.3558669090270996 \n",
      "Epoch: 1/10:  mini-batch 1153/4459:  Train loss: 3.399833917617798  Test loss: 3.355076313018799 \n",
      "Epoch: 1/10:  mini-batch 1154/4459:  Train loss: 3.4833908081054688  Test loss: 3.3544206619262695 \n",
      "Epoch: 1/10:  mini-batch 1155/4459:  Train loss: 3.3222060203552246  Test loss: 3.3536388874053955 \n",
      "Epoch: 1/10:  mini-batch 1156/4459:  Train loss: 3.4603428840637207  Test loss: 3.3531243801116943 \n",
      "Epoch: 1/10:  mini-batch 1157/4459:  Train loss: 3.456510543823242  Test loss: 3.3527209758758545 \n",
      "Epoch: 1/10:  mini-batch 1158/4459:  Train loss: 3.3357911109924316  Test loss: 3.351536512374878 \n",
      "Epoch: 1/10:  mini-batch 1159/4459:  Train loss: 3.5358452796936035  Test loss: 3.3508176803588867 \n",
      "Epoch: 1/10:  mini-batch 1160/4459:  Train loss: 3.377220392227173  Test loss: 3.350029468536377 \n",
      "Epoch: 1/10:  mini-batch 1161/4459:  Train loss: 3.473224639892578  Test loss: 3.3496627807617188 \n",
      "Epoch: 1/10:  mini-batch 1162/4459:  Train loss: 3.407641887664795  Test loss: 3.349567413330078 \n",
      "Epoch: 1/10:  mini-batch 1163/4459:  Train loss: 3.3612823486328125  Test loss: 3.3493218421936035 \n",
      "Epoch: 1/10:  mini-batch 1164/4459:  Train loss: 3.3107433319091797  Test loss: 3.3490257263183594 \n",
      "Epoch: 1/10:  mini-batch 1165/4459:  Train loss: 3.432229995727539  Test loss: 3.3487119674682617 \n",
      "Epoch: 1/10:  mini-batch 1166/4459:  Train loss: 3.173107624053955  Test loss: 3.348459243774414 \n",
      "Epoch: 1/10:  mini-batch 1167/4459:  Train loss: 3.4946486949920654  Test loss: 3.3474936485290527 \n",
      "Epoch: 1/10:  mini-batch 1168/4459:  Train loss: 3.4247238636016846  Test loss: 3.346505641937256 \n",
      "Epoch: 1/10:  mini-batch 1169/4459:  Train loss: 3.519108772277832  Test loss: 3.3460776805877686 \n",
      "Epoch: 1/10:  mini-batch 1170/4459:  Train loss: 3.5199356079101562  Test loss: 3.3460350036621094 \n",
      "Epoch: 1/10:  mini-batch 1171/4459:  Train loss: 3.524589776992798  Test loss: 3.3462343215942383 \n",
      "Epoch: 1/10:  mini-batch 1172/4459:  Train loss: 3.4535584449768066  Test loss: 3.346652030944824 \n",
      "Epoch: 1/10:  mini-batch 1173/4459:  Train loss: 3.275186061859131  Test loss: 3.3470237255096436 \n",
      "Epoch: 1/10:  mini-batch 1174/4459:  Train loss: 3.5579757690429688  Test loss: 3.347668170928955 \n",
      "Epoch: 1/10:  mini-batch 1175/4459:  Train loss: 3.132222890853882  Test loss: 3.3479857444763184 \n",
      "Epoch: 1/10:  mini-batch 1176/4459:  Train loss: 3.366762638092041  Test loss: 3.347782850265503 \n",
      "Epoch: 1/10:  mini-batch 1177/4459:  Train loss: 3.6574878692626953  Test loss: 3.3480288982391357 \n",
      "Epoch: 1/10:  mini-batch 1178/4459:  Train loss: 3.2114481925964355  Test loss: 3.3478193283081055 \n",
      "Epoch: 1/10:  mini-batch 1179/4459:  Train loss: 3.272940158843994  Test loss: 3.347377061843872 \n",
      "Epoch: 1/10:  mini-batch 1180/4459:  Train loss: 3.2341291904449463  Test loss: 3.346648693084717 \n",
      "Epoch: 1/10:  mini-batch 1181/4459:  Train loss: 3.2478291988372803  Test loss: 3.3462038040161133 \n",
      "Epoch: 1/10:  mini-batch 1182/4459:  Train loss: 3.477595329284668  Test loss: 3.3458304405212402 \n",
      "Epoch: 1/10:  mini-batch 1183/4459:  Train loss: 3.4067039489746094  Test loss: 3.3455820083618164 \n",
      "Epoch: 1/10:  mini-batch 1184/4459:  Train loss: 3.3561439514160156  Test loss: 3.3453316688537598 \n",
      "Epoch: 1/10:  mini-batch 1185/4459:  Train loss: 3.4796876907348633  Test loss: 3.345163106918335 \n",
      "Epoch: 1/10:  mini-batch 1186/4459:  Train loss: 3.584993362426758  Test loss: 3.3450496196746826 \n",
      "Epoch: 1/10:  mini-batch 1187/4459:  Train loss: 3.3597373962402344  Test loss: 3.344844341278076 \n",
      "Epoch: 1/10:  mini-batch 1188/4459:  Train loss: 3.313951253890991  Test loss: 3.3441641330718994 \n",
      "Epoch: 1/10:  mini-batch 1189/4459:  Train loss: 3.325408458709717  Test loss: 3.3435583114624023 \n",
      "Epoch: 1/10:  mini-batch 1190/4459:  Train loss: 3.3984861373901367  Test loss: 3.34283447265625 \n",
      "Epoch: 1/10:  mini-batch 1191/4459:  Train loss: 3.4256327152252197  Test loss: 3.342238187789917 \n",
      "Epoch: 1/10:  mini-batch 1192/4459:  Train loss: 3.2090249061584473  Test loss: 3.341463088989258 \n",
      "Epoch: 1/10:  mini-batch 1193/4459:  Train loss: 3.4127533435821533  Test loss: 3.3406643867492676 \n",
      "Epoch: 1/10:  mini-batch 1194/4459:  Train loss: 3.407003879547119  Test loss: 3.3397839069366455 \n",
      "Epoch: 1/10:  mini-batch 1195/4459:  Train loss: 3.4699513912200928  Test loss: 3.338693141937256 \n",
      "Epoch: 1/10:  mini-batch 1196/4459:  Train loss: 3.1826391220092773  Test loss: 3.337573766708374 \n",
      "Epoch: 1/10:  mini-batch 1197/4459:  Train loss: 3.509310245513916  Test loss: 3.336759090423584 \n",
      "Epoch: 1/10:  mini-batch 1198/4459:  Train loss: 3.559304714202881  Test loss: 3.3362374305725098 \n",
      "Epoch: 1/10:  mini-batch 1199/4459:  Train loss: 3.311638355255127  Test loss: 3.3358263969421387 \n",
      "Epoch: 1/10:  mini-batch 1200/4459:  Train loss: 3.481204032897949  Test loss: 3.3353099822998047 \n",
      "Epoch: 1/10:  mini-batch 1201/4459:  Train loss: 3.0735092163085938  Test loss: 3.3346123695373535 \n",
      "Epoch: 1/10:  mini-batch 1202/4459:  Train loss: 3.516460657119751  Test loss: 3.334176540374756 \n",
      "Epoch: 1/10:  mini-batch 1203/4459:  Train loss: 3.2616219520568848  Test loss: 3.3337759971618652 \n",
      "Epoch: 1/10:  mini-batch 1204/4459:  Train loss: 3.35771107673645  Test loss: 3.333528757095337 \n",
      "Epoch: 1/10:  mini-batch 1205/4459:  Train loss: 3.196955919265747  Test loss: 3.3333780765533447 \n",
      "Epoch: 1/10:  mini-batch 1206/4459:  Train loss: 3.2415359020233154  Test loss: 3.3331027030944824 \n",
      "Epoch: 1/10:  mini-batch 1207/4459:  Train loss: 3.564804792404175  Test loss: 3.333132743835449 \n",
      "Epoch: 1/10:  mini-batch 1208/4459:  Train loss: 3.3707756996154785  Test loss: 3.333493709564209 \n",
      "Epoch: 1/10:  mini-batch 1209/4459:  Train loss: 3.3552422523498535  Test loss: 3.334254741668701 \n",
      "Epoch: 1/10:  mini-batch 1210/4459:  Train loss: 3.6792707443237305  Test loss: 3.335388660430908 \n",
      "Epoch: 1/10:  mini-batch 1211/4459:  Train loss: 3.6406965255737305  Test loss: 3.3370611667633057 \n",
      "Epoch: 1/10:  mini-batch 1212/4459:  Train loss: 3.3635339736938477  Test loss: 3.3384175300598145 \n",
      "Epoch: 1/10:  mini-batch 1213/4459:  Train loss: 3.387091875076294  Test loss: 3.3400633335113525 \n",
      "Epoch: 1/10:  mini-batch 1214/4459:  Train loss: 3.7287685871124268  Test loss: 3.341789722442627 \n",
      "Epoch: 1/10:  mini-batch 1215/4459:  Train loss: 3.949747323989868  Test loss: 3.3438806533813477 \n",
      "Epoch: 1/10:  mini-batch 1216/4459:  Train loss: 3.4078099727630615  Test loss: 3.34555721282959 \n",
      "Epoch: 1/10:  mini-batch 1217/4459:  Train loss: 3.281139612197876  Test loss: 3.3468728065490723 \n",
      "Epoch: 1/10:  mini-batch 1218/4459:  Train loss: 3.323927879333496  Test loss: 3.347607135772705 \n",
      "Epoch: 1/10:  mini-batch 1219/4459:  Train loss: 3.096454620361328  Test loss: 3.3475072383880615 \n",
      "Epoch: 1/10:  mini-batch 1220/4459:  Train loss: 3.269644021987915  Test loss: 3.3469104766845703 \n",
      "Epoch: 1/10:  mini-batch 1221/4459:  Train loss: 3.085501194000244  Test loss: 3.346036434173584 \n",
      "Epoch: 1/10:  mini-batch 1222/4459:  Train loss: 3.405851125717163  Test loss: 3.345883846282959 \n",
      "Epoch: 1/10:  mini-batch 1223/4459:  Train loss: 3.4633278846740723  Test loss: 3.3456311225891113 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1224/4459:  Train loss: 3.2478432655334473  Test loss: 3.3451857566833496 \n",
      "Epoch: 1/10:  mini-batch 1225/4459:  Train loss: 3.258197784423828  Test loss: 3.3444252014160156 \n",
      "Epoch: 1/10:  mini-batch 1226/4459:  Train loss: 3.4212121963500977  Test loss: 3.3436222076416016 \n",
      "Epoch: 1/10:  mini-batch 1227/4459:  Train loss: 3.309138536453247  Test loss: 3.3430075645446777 \n",
      "Epoch: 1/10:  mini-batch 1228/4459:  Train loss: 3.413285732269287  Test loss: 3.342820644378662 \n",
      "Epoch: 1/10:  mini-batch 1229/4459:  Train loss: 3.67202091217041  Test loss: 3.343174457550049 \n",
      "Epoch: 1/10:  mini-batch 1230/4459:  Train loss: 3.395524501800537  Test loss: 3.3436853885650635 \n",
      "Epoch: 1/10:  mini-batch 1231/4459:  Train loss: 3.47820782661438  Test loss: 3.344127893447876 \n",
      "Epoch: 1/10:  mini-batch 1232/4459:  Train loss: 3.2134273052215576  Test loss: 3.3443186283111572 \n",
      "Epoch: 1/10:  mini-batch 1233/4459:  Train loss: 3.5214898586273193  Test loss: 3.3448264598846436 \n",
      "Epoch: 1/10:  mini-batch 1234/4459:  Train loss: 3.3924174308776855  Test loss: 3.3458876609802246 \n",
      "Epoch: 1/10:  mini-batch 1235/4459:  Train loss: 3.3058009147644043  Test loss: 3.3464162349700928 \n",
      "Epoch: 1/10:  mini-batch 1236/4459:  Train loss: 3.3122682571411133  Test loss: 3.347074270248413 \n",
      "Epoch: 1/10:  mini-batch 1237/4459:  Train loss: 3.336851119995117  Test loss: 3.3476035594940186 \n",
      "Epoch: 1/10:  mini-batch 1238/4459:  Train loss: 3.018139123916626  Test loss: 3.3476815223693848 \n",
      "Epoch: 1/10:  mini-batch 1239/4459:  Train loss: 3.38637113571167  Test loss: 3.3471274375915527 \n",
      "Epoch: 1/10:  mini-batch 1240/4459:  Train loss: 3.365506649017334  Test loss: 3.346330165863037 \n",
      "Epoch: 1/10:  mini-batch 1241/4459:  Train loss: 3.6047229766845703  Test loss: 3.3458540439605713 \n",
      "Epoch: 1/10:  mini-batch 1242/4459:  Train loss: 3.427590847015381  Test loss: 3.345524787902832 \n",
      "Epoch: 1/10:  mini-batch 1243/4459:  Train loss: 3.3695778846740723  Test loss: 3.3451290130615234 \n",
      "Epoch: 1/10:  mini-batch 1244/4459:  Train loss: 3.588742733001709  Test loss: 3.344188690185547 \n",
      "Epoch: 1/10:  mini-batch 1245/4459:  Train loss: 3.6066949367523193  Test loss: 3.344163656234741 \n",
      "Epoch: 1/10:  mini-batch 1246/4459:  Train loss: 3.375553607940674  Test loss: 3.3445940017700195 \n",
      "Epoch: 1/10:  mini-batch 1247/4459:  Train loss: 3.253235340118408  Test loss: 3.3450987339019775 \n",
      "Epoch: 1/10:  mini-batch 1248/4459:  Train loss: 3.2473204135894775  Test loss: 3.345062732696533 \n",
      "Epoch: 1/10:  mini-batch 1249/4459:  Train loss: 3.205883026123047  Test loss: 3.345325469970703 \n",
      "Epoch: 1/10:  mini-batch 1250/4459:  Train loss: 3.377772331237793  Test loss: 3.345327377319336 \n",
      "Epoch: 1/10:  mini-batch 1251/4459:  Train loss: 3.2826616764068604  Test loss: 3.34486985206604 \n",
      "Epoch: 1/10:  mini-batch 1252/4459:  Train loss: 3.3444321155548096  Test loss: 3.3441174030303955 \n",
      "Epoch: 1/10:  mini-batch 1253/4459:  Train loss: 3.210003137588501  Test loss: 3.3431901931762695 \n",
      "Epoch: 1/10:  mini-batch 1254/4459:  Train loss: 3.4278132915496826  Test loss: 3.3428797721862793 \n",
      "Epoch: 1/10:  mini-batch 1255/4459:  Train loss: 3.553475856781006  Test loss: 3.342538833618164 \n",
      "Epoch: 1/10:  mini-batch 1256/4459:  Train loss: 3.337299346923828  Test loss: 3.3423242568969727 \n",
      "Epoch: 1/10:  mini-batch 1257/4459:  Train loss: 3.342693328857422  Test loss: 3.3419456481933594 \n",
      "Epoch: 1/10:  mini-batch 1258/4459:  Train loss: 3.221930503845215  Test loss: 3.341770648956299 \n",
      "Epoch: 1/10:  mini-batch 1259/4459:  Train loss: 3.3639018535614014  Test loss: 3.3416061401367188 \n",
      "Epoch: 1/10:  mini-batch 1260/4459:  Train loss: 3.24434232711792  Test loss: 3.3413374423980713 \n",
      "Epoch: 1/10:  mini-batch 1261/4459:  Train loss: 3.5138776302337646  Test loss: 3.340710401535034 \n",
      "Epoch: 1/10:  mini-batch 1262/4459:  Train loss: 3.128727912902832  Test loss: 3.339918613433838 \n",
      "Epoch: 1/10:  mini-batch 1263/4459:  Train loss: 3.382488250732422  Test loss: 3.338463306427002 \n",
      "Epoch: 1/10:  mini-batch 1264/4459:  Train loss: 3.5028953552246094  Test loss: 3.3373026847839355 \n",
      "Epoch: 1/10:  mini-batch 1265/4459:  Train loss: 3.4192888736724854  Test loss: 3.336500406265259 \n",
      "Epoch: 1/10:  mini-batch 1266/4459:  Train loss: 3.3280632495880127  Test loss: 3.336099624633789 \n",
      "Epoch: 1/10:  mini-batch 1267/4459:  Train loss: 3.4381415843963623  Test loss: 3.335453748703003 \n",
      "Epoch: 1/10:  mini-batch 1268/4459:  Train loss: 3.5863895416259766  Test loss: 3.334355115890503 \n",
      "Epoch: 1/10:  mini-batch 1269/4459:  Train loss: 3.5096018314361572  Test loss: 3.3334217071533203 \n",
      "Epoch: 1/10:  mini-batch 1270/4459:  Train loss: 3.6033575534820557  Test loss: 3.332655191421509 \n",
      "Epoch: 1/10:  mini-batch 1271/4459:  Train loss: 3.416616916656494  Test loss: 3.332036018371582 \n",
      "Epoch: 1/10:  mini-batch 1272/4459:  Train loss: 3.328223466873169  Test loss: 3.331516742706299 \n",
      "Epoch: 1/10:  mini-batch 1273/4459:  Train loss: 3.4674794673919678  Test loss: 3.331360340118408 \n",
      "Epoch: 1/10:  mini-batch 1274/4459:  Train loss: 3.2547178268432617  Test loss: 3.331249237060547 \n",
      "Epoch: 1/10:  mini-batch 1275/4459:  Train loss: 3.3345367908477783  Test loss: 3.3311662673950195 \n",
      "Epoch: 1/10:  mini-batch 1276/4459:  Train loss: 3.3581247329711914  Test loss: 3.3311312198638916 \n",
      "Epoch: 1/10:  mini-batch 1277/4459:  Train loss: 3.531848669052124  Test loss: 3.331418037414551 \n",
      "Epoch: 1/10:  mini-batch 1278/4459:  Train loss: 3.4521398544311523  Test loss: 3.3316452503204346 \n",
      "Epoch: 1/10:  mini-batch 1279/4459:  Train loss: 3.1773200035095215  Test loss: 3.331317186355591 \n",
      "Epoch: 1/10:  mini-batch 1280/4459:  Train loss: 3.4409592151641846  Test loss: 3.3311431407928467 \n",
      "Epoch: 1/10:  mini-batch 1281/4459:  Train loss: 3.37459397315979  Test loss: 3.330812931060791 \n",
      "Epoch: 1/10:  mini-batch 1282/4459:  Train loss: 3.5356571674346924  Test loss: 3.331235885620117 \n",
      "Epoch: 1/10:  mini-batch 1283/4459:  Train loss: 3.1380438804626465  Test loss: 3.331233024597168 \n",
      "Epoch: 1/10:  mini-batch 1284/4459:  Train loss: 3.180091381072998  Test loss: 3.3310160636901855 \n",
      "Epoch: 1/10:  mini-batch 1285/4459:  Train loss: 3.114382266998291  Test loss: 3.3303565979003906 \n",
      "Epoch: 1/10:  mini-batch 1286/4459:  Train loss: 3.3182711601257324  Test loss: 3.3296918869018555 \n",
      "Epoch: 1/10:  mini-batch 1287/4459:  Train loss: 3.6619036197662354  Test loss: 3.329573154449463 \n",
      "Epoch: 1/10:  mini-batch 1288/4459:  Train loss: 3.643366575241089  Test loss: 3.3300726413726807 \n",
      "Epoch: 1/10:  mini-batch 1289/4459:  Train loss: 3.2800488471984863  Test loss: 3.3305468559265137 \n",
      "Epoch: 1/10:  mini-batch 1290/4459:  Train loss: 3.066802501678467  Test loss: 3.330522060394287 \n",
      "Epoch: 1/10:  mini-batch 1291/4459:  Train loss: 3.2352118492126465  Test loss: 3.330118179321289 \n",
      "Epoch: 1/10:  mini-batch 1292/4459:  Train loss: 3.0941855907440186  Test loss: 3.32975697517395 \n",
      "Epoch: 1/10:  mini-batch 1293/4459:  Train loss: 3.191530466079712  Test loss: 3.3297271728515625 \n",
      "Epoch: 1/10:  mini-batch 1294/4459:  Train loss: 3.2780613899230957  Test loss: 3.3298401832580566 \n",
      "Epoch: 1/10:  mini-batch 1295/4459:  Train loss: 3.606421947479248  Test loss: 3.329498291015625 \n",
      "Epoch: 1/10:  mini-batch 1296/4459:  Train loss: 3.4562571048736572  Test loss: 3.329040765762329 \n",
      "Epoch: 1/10:  mini-batch 1297/4459:  Train loss: 3.4358654022216797  Test loss: 3.3283872604370117 \n",
      "Epoch: 1/10:  mini-batch 1298/4459:  Train loss: 3.3829851150512695  Test loss: 3.3270795345306396 \n",
      "Epoch: 1/10:  mini-batch 1299/4459:  Train loss: 3.2172276973724365  Test loss: 3.325876474380493 \n",
      "Epoch: 1/10:  mini-batch 1300/4459:  Train loss: 3.4700751304626465  Test loss: 3.32515811920166 \n",
      "Epoch: 1/10:  mini-batch 1301/4459:  Train loss: 3.24705171585083  Test loss: 3.324374198913574 \n",
      "Epoch: 1/10:  mini-batch 1302/4459:  Train loss: 3.104750394821167  Test loss: 3.323786735534668 \n",
      "Epoch: 1/10:  mini-batch 1303/4459:  Train loss: 3.244793176651001  Test loss: 3.3236007690429688 \n",
      "Epoch: 1/10:  mini-batch 1304/4459:  Train loss: 3.2406492233276367  Test loss: 3.3231260776519775 \n",
      "Epoch: 1/10:  mini-batch 1305/4459:  Train loss: 3.6216280460357666  Test loss: 3.3222033977508545 \n",
      "Epoch: 1/10:  mini-batch 1306/4459:  Train loss: 3.2983365058898926  Test loss: 3.3212761878967285 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1307/4459:  Train loss: 3.2925498485565186  Test loss: 3.3193275928497314 \n",
      "Epoch: 1/10:  mini-batch 1308/4459:  Train loss: 3.3911614418029785  Test loss: 3.3176345825195312 \n",
      "Epoch: 1/10:  mini-batch 1309/4459:  Train loss: 3.4223225116729736  Test loss: 3.316439628601074 \n",
      "Epoch: 1/10:  mini-batch 1310/4459:  Train loss: 3.615304470062256  Test loss: 3.3155248165130615 \n",
      "Epoch: 1/10:  mini-batch 1311/4459:  Train loss: 3.555050849914551  Test loss: 3.315357208251953 \n",
      "Epoch: 1/10:  mini-batch 1312/4459:  Train loss: 3.227843761444092  Test loss: 3.3153035640716553 \n",
      "Epoch: 1/10:  mini-batch 1313/4459:  Train loss: 3.3542532920837402  Test loss: 3.3152427673339844 \n",
      "Epoch: 1/10:  mini-batch 1314/4459:  Train loss: 3.6006624698638916  Test loss: 3.3162364959716797 \n",
      "Epoch: 1/10:  mini-batch 1315/4459:  Train loss: 3.5681140422821045  Test loss: 3.317145824432373 \n",
      "Epoch: 1/10:  mini-batch 1316/4459:  Train loss: 3.637847900390625  Test loss: 3.318244457244873 \n",
      "Epoch: 1/10:  mini-batch 1317/4459:  Train loss: 3.5587987899780273  Test loss: 3.3194448947906494 \n",
      "Epoch: 1/10:  mini-batch 1318/4459:  Train loss: 3.2154741287231445  Test loss: 3.320307731628418 \n",
      "Epoch: 1/10:  mini-batch 1319/4459:  Train loss: 3.378946304321289  Test loss: 3.321000099182129 \n",
      "Epoch: 1/10:  mini-batch 1320/4459:  Train loss: 3.4430947303771973  Test loss: 3.3215298652648926 \n",
      "Epoch: 1/10:  mini-batch 1321/4459:  Train loss: 3.3496251106262207  Test loss: 3.3225533962249756 \n",
      "Epoch: 1/10:  mini-batch 1322/4459:  Train loss: 3.285311460494995  Test loss: 3.323549509048462 \n",
      "Epoch: 1/10:  mini-batch 1323/4459:  Train loss: 3.3270339965820312  Test loss: 3.324434280395508 \n",
      "Epoch: 1/10:  mini-batch 1324/4459:  Train loss: 3.293201446533203  Test loss: 3.325169324874878 \n",
      "Epoch: 1/10:  mini-batch 1325/4459:  Train loss: 3.4843997955322266  Test loss: 3.326542377471924 \n",
      "Epoch: 1/10:  mini-batch 1326/4459:  Train loss: 3.401871919631958  Test loss: 3.327683687210083 \n",
      "Epoch: 1/10:  mini-batch 1327/4459:  Train loss: 3.2811317443847656  Test loss: 3.328585386276245 \n",
      "Epoch: 1/10:  mini-batch 1328/4459:  Train loss: 3.3095524311065674  Test loss: 3.328791379928589 \n",
      "Epoch: 1/10:  mini-batch 1329/4459:  Train loss: 3.364729881286621  Test loss: 3.3288278579711914 \n",
      "Epoch: 1/10:  mini-batch 1330/4459:  Train loss: 3.3884942531585693  Test loss: 3.327983856201172 \n",
      "Epoch: 1/10:  mini-batch 1331/4459:  Train loss: 3.6920080184936523  Test loss: 3.3268771171569824 \n",
      "Epoch: 1/10:  mini-batch 1332/4459:  Train loss: 3.2695207595825195  Test loss: 3.3258156776428223 \n",
      "Epoch: 1/10:  mini-batch 1333/4459:  Train loss: 3.267951488494873  Test loss: 3.324437141418457 \n",
      "Epoch: 1/10:  mini-batch 1334/4459:  Train loss: 3.7016031742095947  Test loss: 3.3237316608428955 \n",
      "Epoch: 1/10:  mini-batch 1335/4459:  Train loss: 3.5558881759643555  Test loss: 3.3233158588409424 \n",
      "Epoch: 1/10:  mini-batch 1336/4459:  Train loss: 3.760564088821411  Test loss: 3.3238792419433594 \n",
      "Epoch: 1/10:  mini-batch 1337/4459:  Train loss: 3.357764720916748  Test loss: 3.3242831230163574 \n",
      "Epoch: 1/10:  mini-batch 1338/4459:  Train loss: 3.4280993938446045  Test loss: 3.3244223594665527 \n",
      "Epoch: 1/10:  mini-batch 1339/4459:  Train loss: 3.240382194519043  Test loss: 3.3243627548217773 \n",
      "Epoch: 1/10:  mini-batch 1340/4459:  Train loss: 3.5661065578460693  Test loss: 3.3248202800750732 \n",
      "Epoch: 1/10:  mini-batch 1341/4459:  Train loss: 3.251316785812378  Test loss: 3.325019598007202 \n",
      "Epoch: 1/10:  mini-batch 1342/4459:  Train loss: 3.343878984451294  Test loss: 3.325122833251953 \n",
      "Epoch: 1/10:  mini-batch 1343/4459:  Train loss: 3.4812920093536377  Test loss: 3.3255372047424316 \n",
      "Epoch: 1/10:  mini-batch 1344/4459:  Train loss: 3.3402955532073975  Test loss: 3.3257012367248535 \n",
      "Epoch: 1/10:  mini-batch 1345/4459:  Train loss: 3.3367276191711426  Test loss: 3.3255767822265625 \n",
      "Epoch: 1/10:  mini-batch 1346/4459:  Train loss: 3.425229072570801  Test loss: 3.3252415657043457 \n",
      "Epoch: 1/10:  mini-batch 1347/4459:  Train loss: 3.3857390880584717  Test loss: 3.3250765800476074 \n",
      "Epoch: 1/10:  mini-batch 1348/4459:  Train loss: 3.488079786300659  Test loss: 3.32501482963562 \n",
      "Epoch: 1/10:  mini-batch 1349/4459:  Train loss: 3.2846031188964844  Test loss: 3.324866533279419 \n",
      "Epoch: 1/10:  mini-batch 1350/4459:  Train loss: 3.2052741050720215  Test loss: 3.324141025543213 \n",
      "Epoch: 1/10:  mini-batch 1351/4459:  Train loss: 3.301239252090454  Test loss: 3.3234939575195312 \n",
      "Epoch: 1/10:  mini-batch 1352/4459:  Train loss: 3.3321619033813477  Test loss: 3.3224024772644043 \n",
      "Epoch: 1/10:  mini-batch 1353/4459:  Train loss: 3.409653902053833  Test loss: 3.321394205093384 \n",
      "Epoch: 1/10:  mini-batch 1354/4459:  Train loss: 3.6230545043945312  Test loss: 3.320627212524414 \n",
      "Epoch: 1/10:  mini-batch 1355/4459:  Train loss: 3.394904851913452  Test loss: 3.3198866844177246 \n",
      "Epoch: 1/10:  mini-batch 1356/4459:  Train loss: 3.267193555831909  Test loss: 3.318904399871826 \n",
      "Epoch: 1/10:  mini-batch 1357/4459:  Train loss: 3.167050361633301  Test loss: 3.31744122505188 \n",
      "Epoch: 1/10:  mini-batch 1358/4459:  Train loss: 3.430717945098877  Test loss: 3.3161325454711914 \n",
      "Epoch: 1/10:  mini-batch 1359/4459:  Train loss: 3.280519962310791  Test loss: 3.314439535140991 \n",
      "Epoch: 1/10:  mini-batch 1360/4459:  Train loss: 3.2596664428710938  Test loss: 3.312559127807617 \n",
      "Epoch: 1/10:  mini-batch 1361/4459:  Train loss: 3.110940933227539  Test loss: 3.310222864151001 \n",
      "Epoch: 1/10:  mini-batch 1362/4459:  Train loss: 3.3093628883361816  Test loss: 3.308091402053833 \n",
      "Epoch: 1/10:  mini-batch 1363/4459:  Train loss: 3.2807505130767822  Test loss: 3.3058786392211914 \n",
      "Epoch: 1/10:  mini-batch 1364/4459:  Train loss: 3.143224000930786  Test loss: 3.303835153579712 \n",
      "Epoch: 1/10:  mini-batch 1365/4459:  Train loss: 3.0438599586486816  Test loss: 3.3017029762268066 \n",
      "Epoch: 1/10:  mini-batch 1366/4459:  Train loss: 3.2736241817474365  Test loss: 3.2992992401123047 \n",
      "Epoch: 1/10:  mini-batch 1367/4459:  Train loss: 3.5394694805145264  Test loss: 3.297375202178955 \n",
      "Epoch: 1/10:  mini-batch 1368/4459:  Train loss: 3.2924516201019287  Test loss: 3.2957873344421387 \n",
      "Epoch: 1/10:  mini-batch 1369/4459:  Train loss: 3.4926373958587646  Test loss: 3.294498920440674 \n",
      "Epoch: 1/10:  mini-batch 1370/4459:  Train loss: 3.208974838256836  Test loss: 3.2926530838012695 \n",
      "Epoch: 1/10:  mini-batch 1371/4459:  Train loss: 3.371957778930664  Test loss: 3.2904019355773926 \n",
      "Epoch: 1/10:  mini-batch 1372/4459:  Train loss: 3.0732100009918213  Test loss: 3.2880780696868896 \n",
      "Epoch: 1/10:  mini-batch 1373/4459:  Train loss: 3.1257987022399902  Test loss: 3.285928964614868 \n",
      "Epoch: 1/10:  mini-batch 1374/4459:  Train loss: 3.1431775093078613  Test loss: 3.2836902141571045 \n",
      "Epoch: 1/10:  mini-batch 1375/4459:  Train loss: 3.1395859718322754  Test loss: 3.2815771102905273 \n",
      "Epoch: 1/10:  mini-batch 1376/4459:  Train loss: 3.641118049621582  Test loss: 3.2795400619506836 \n",
      "Epoch: 1/10:  mini-batch 1377/4459:  Train loss: 3.400186777114868  Test loss: 3.27783203125 \n",
      "Epoch: 1/10:  mini-batch 1378/4459:  Train loss: 3.2901878356933594  Test loss: 3.276492118835449 \n",
      "Epoch: 1/10:  mini-batch 1379/4459:  Train loss: 3.3451967239379883  Test loss: 3.275641918182373 \n",
      "Epoch: 1/10:  mini-batch 1380/4459:  Train loss: 3.545285224914551  Test loss: 3.275272846221924 \n",
      "Epoch: 1/10:  mini-batch 1381/4459:  Train loss: 3.9007339477539062  Test loss: 3.276045322418213 \n",
      "Epoch: 1/10:  mini-batch 1382/4459:  Train loss: 3.895583391189575  Test loss: 3.2771551609039307 \n",
      "Epoch: 1/10:  mini-batch 1383/4459:  Train loss: 3.501274585723877  Test loss: 3.278266191482544 \n",
      "Epoch: 1/10:  mini-batch 1384/4459:  Train loss: 3.352376937866211  Test loss: 3.279130458831787 \n",
      "Epoch: 1/10:  mini-batch 1385/4459:  Train loss: 3.16040301322937  Test loss: 3.2805211544036865 \n",
      "Epoch: 1/10:  mini-batch 1386/4459:  Train loss: 3.0691850185394287  Test loss: 3.2820887565612793 \n",
      "Epoch: 1/10:  mini-batch 1387/4459:  Train loss: 3.3176066875457764  Test loss: 3.283181667327881 \n",
      "Epoch: 1/10:  mini-batch 1388/4459:  Train loss: 2.9462337493896484  Test loss: 3.284123659133911 \n",
      "Epoch: 1/10:  mini-batch 1389/4459:  Train loss: 3.1655309200286865  Test loss: 3.2846765518188477 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1390/4459:  Train loss: 3.1727938652038574  Test loss: 3.2850077152252197 \n",
      "Epoch: 1/10:  mini-batch 1391/4459:  Train loss: 3.549220561981201  Test loss: 3.2853431701660156 \n",
      "Epoch: 1/10:  mini-batch 1392/4459:  Train loss: 3.555098533630371  Test loss: 3.2861058712005615 \n",
      "Epoch: 1/10:  mini-batch 1393/4459:  Train loss: 3.2640602588653564  Test loss: 3.286625385284424 \n",
      "Epoch: 1/10:  mini-batch 1394/4459:  Train loss: 3.4940195083618164  Test loss: 3.2875614166259766 \n",
      "Epoch: 1/10:  mini-batch 1395/4459:  Train loss: 3.365964412689209  Test loss: 3.288390636444092 \n",
      "Epoch: 1/10:  mini-batch 1396/4459:  Train loss: 2.7777938842773438  Test loss: 3.2889392375946045 \n",
      "Epoch: 1/10:  mini-batch 1397/4459:  Train loss: 3.1629974842071533  Test loss: 3.2894515991210938 \n",
      "Epoch: 1/10:  mini-batch 1398/4459:  Train loss: 3.563237190246582  Test loss: 3.290314197540283 \n",
      "Epoch: 1/10:  mini-batch 1399/4459:  Train loss: 3.590039014816284  Test loss: 3.291367769241333 \n",
      "Epoch: 1/10:  mini-batch 1400/4459:  Train loss: 3.476764678955078  Test loss: 3.2924773693084717 \n",
      "Epoch: 1/10:  mini-batch 1401/4459:  Train loss: 3.2498321533203125  Test loss: 3.2934961318969727 \n",
      "Epoch: 1/10:  mini-batch 1402/4459:  Train loss: 3.2164134979248047  Test loss: 3.2943196296691895 \n",
      "Epoch: 1/10:  mini-batch 1403/4459:  Train loss: 3.3150017261505127  Test loss: 3.2942562103271484 \n",
      "Epoch: 1/10:  mini-batch 1404/4459:  Train loss: 3.2375752925872803  Test loss: 3.2934370040893555 \n",
      "Epoch: 1/10:  mini-batch 1405/4459:  Train loss: 3.412806987762451  Test loss: 3.293010711669922 \n",
      "Epoch: 1/10:  mini-batch 1406/4459:  Train loss: 3.080517530441284  Test loss: 3.2924978733062744 \n",
      "Epoch: 1/10:  mini-batch 1407/4459:  Train loss: 3.3243908882141113  Test loss: 3.292609214782715 \n",
      "Epoch: 1/10:  mini-batch 1408/4459:  Train loss: 3.4619905948638916  Test loss: 3.2927138805389404 \n",
      "Epoch: 1/10:  mini-batch 1409/4459:  Train loss: 3.15794038772583  Test loss: 3.2929718494415283 \n",
      "Epoch: 1/10:  mini-batch 1410/4459:  Train loss: 3.2592458724975586  Test loss: 3.29288387298584 \n",
      "Epoch: 1/10:  mini-batch 1411/4459:  Train loss: 3.5126702785491943  Test loss: 3.2932982444763184 \n",
      "Epoch: 1/10:  mini-batch 1412/4459:  Train loss: 3.0149455070495605  Test loss: 3.2927536964416504 \n",
      "Epoch: 1/10:  mini-batch 1413/4459:  Train loss: 3.183436393737793  Test loss: 3.292454719543457 \n",
      "Epoch: 1/10:  mini-batch 1414/4459:  Train loss: 3.2243621349334717  Test loss: 3.292149782180786 \n",
      "Epoch: 1/10:  mini-batch 1415/4459:  Train loss: 2.8598599433898926  Test loss: 3.2920095920562744 \n",
      "Epoch: 1/10:  mini-batch 1416/4459:  Train loss: 2.8768117427825928  Test loss: 3.292206048965454 \n",
      "Epoch: 1/10:  mini-batch 1417/4459:  Train loss: 3.3941030502319336  Test loss: 3.2923264503479004 \n",
      "Epoch: 1/10:  mini-batch 1418/4459:  Train loss: 3.2920122146606445  Test loss: 3.292581081390381 \n",
      "Epoch: 1/10:  mini-batch 1419/4459:  Train loss: 3.0558223724365234  Test loss: 3.293097496032715 \n",
      "Epoch: 1/10:  mini-batch 1420/4459:  Train loss: 3.0684521198272705  Test loss: 3.2930099964141846 \n",
      "Epoch: 1/10:  mini-batch 1421/4459:  Train loss: 3.0405874252319336  Test loss: 3.2929868698120117 \n",
      "Epoch: 1/10:  mini-batch 1422/4459:  Train loss: 3.4263834953308105  Test loss: 3.293041229248047 \n",
      "Epoch: 1/10:  mini-batch 1423/4459:  Train loss: 3.451030731201172  Test loss: 3.2928664684295654 \n",
      "Epoch: 1/10:  mini-batch 1424/4459:  Train loss: 2.832130193710327  Test loss: 3.293407440185547 \n",
      "Epoch: 1/10:  mini-batch 1425/4459:  Train loss: 3.3441014289855957  Test loss: 3.293708324432373 \n",
      "Epoch: 1/10:  mini-batch 1426/4459:  Train loss: 3.3962318897247314  Test loss: 3.2938084602355957 \n",
      "Epoch: 1/10:  mini-batch 1427/4459:  Train loss: 2.9923975467681885  Test loss: 3.2938685417175293 \n",
      "Epoch: 1/10:  mini-batch 1428/4459:  Train loss: 2.9224276542663574  Test loss: 3.2937188148498535 \n",
      "Epoch: 1/10:  mini-batch 1429/4459:  Train loss: 3.1191511154174805  Test loss: 3.2935373783111572 \n",
      "Epoch: 1/10:  mini-batch 1430/4459:  Train loss: 3.4432413578033447  Test loss: 3.294178009033203 \n",
      "Epoch: 1/10:  mini-batch 1431/4459:  Train loss: 3.4639744758605957  Test loss: 3.2950081825256348 \n",
      "Epoch: 1/10:  mini-batch 1432/4459:  Train loss: 3.2423462867736816  Test loss: 3.2950611114501953 \n",
      "Epoch: 1/10:  mini-batch 1433/4459:  Train loss: 2.8173069953918457  Test loss: 3.2949767112731934 \n",
      "Epoch: 1/10:  mini-batch 1434/4459:  Train loss: 3.08486270904541  Test loss: 3.293858766555786 \n",
      "Epoch: 1/10:  mini-batch 1435/4459:  Train loss: 3.5741753578186035  Test loss: 3.2930519580841064 \n",
      "Epoch: 1/10:  mini-batch 1436/4459:  Train loss: 3.0286409854888916  Test loss: 3.29244327545166 \n",
      "Epoch: 1/10:  mini-batch 1460/4459:  Train loss: 3.442434310913086  Test loss: 3.2698473930358887 \n",
      "Epoch: 1/10:  mini-batch 1461/4459:  Train loss: 3.3904552459716797  Test loss: 3.2707297801971436 \n",
      "Epoch: 1/10:  mini-batch 1462/4459:  Train loss: 3.2105984687805176  Test loss: 3.2706146240234375 \n",
      "Epoch: 1/10:  mini-batch 1463/4459:  Train loss: 3.230835437774658  Test loss: 3.270561695098877 \n",
      "Epoch: 1/10:  mini-batch 1464/4459:  Train loss: 3.540003776550293  Test loss: 3.27095890045166 \n",
      "Epoch: 1/10:  mini-batch 1465/4459:  Train loss: 3.1474769115448  Test loss: 3.270864486694336 \n",
      "Epoch: 1/10:  mini-batch 1466/4459:  Train loss: 3.210178852081299  Test loss: 3.2707056999206543 \n",
      "Epoch: 1/10:  mini-batch 1467/4459:  Train loss: 3.186596393585205  Test loss: 3.270301103591919 \n",
      "Epoch: 1/10:  mini-batch 1468/4459:  Train loss: 3.4387130737304688  Test loss: 3.269843101501465 \n",
      "Epoch: 1/10:  mini-batch 1469/4459:  Train loss: 3.4025444984436035  Test loss: 3.2693119049072266 \n",
      "Epoch: 1/10:  mini-batch 1470/4459:  Train loss: 2.969733238220215  Test loss: 3.267758846282959 \n",
      "Epoch: 1/10:  mini-batch 1471/4459:  Train loss: 3.2243423461914062  Test loss: 3.266035556793213 \n",
      "Epoch: 1/10:  mini-batch 1472/4459:  Train loss: 3.385563850402832  Test loss: 3.2639431953430176 \n",
      "Epoch: 1/10:  mini-batch 1473/4459:  Train loss: 3.406458854675293  Test loss: 3.262622594833374 \n",
      "Epoch: 1/10:  mini-batch 1474/4459:  Train loss: 3.3037991523742676  Test loss: 3.261164665222168 \n",
      "Epoch: 1/10:  mini-batch 1475/4459:  Train loss: 3.22440242767334  Test loss: 3.2601194381713867 \n",
      "Epoch: 1/10:  mini-batch 1476/4459:  Train loss: 3.222632884979248  Test loss: 3.2589473724365234 \n",
      "Epoch: 1/10:  mini-batch 1477/4459:  Train loss: 3.2104287147521973  Test loss: 3.257204055786133 \n",
      "Epoch: 1/10:  mini-batch 1478/4459:  Train loss: 3.2708911895751953  Test loss: 3.2557661533355713 \n",
      "Epoch: 1/10:  mini-batch 1479/4459:  Train loss: 3.205167293548584  Test loss: 3.2541589736938477 \n",
      "Epoch: 1/10:  mini-batch 1480/4459:  Train loss: 3.318082332611084  Test loss: 3.2523281574249268 \n",
      "Epoch: 1/10:  mini-batch 1481/4459:  Train loss: 3.17767333984375  Test loss: 3.250141143798828 \n",
      "Epoch: 1/10:  mini-batch 1482/4459:  Train loss: 3.2486681938171387  Test loss: 3.248690605163574 \n",
      "Epoch: 1/10:  mini-batch 1483/4459:  Train loss: 3.2760159969329834  Test loss: 3.2472124099731445 \n",
      "Epoch: 1/10:  mini-batch 1484/4459:  Train loss: 3.2753517627716064  Test loss: 3.2457332611083984 \n",
      "Epoch: 1/10:  mini-batch 1485/4459:  Train loss: 3.146821975708008  Test loss: 3.244170904159546 \n",
      "Epoch: 1/10:  mini-batch 1486/4459:  Train loss: 3.623012065887451  Test loss: 3.2431507110595703 \n",
      "Epoch: 1/10:  mini-batch 1487/4459:  Train loss: 3.5550365447998047  Test loss: 3.2427008152008057 \n",
      "Epoch: 1/10:  mini-batch 1488/4459:  Train loss: 3.5901050567626953  Test loss: 3.2422826290130615 \n",
      "Epoch: 1/10:  mini-batch 1489/4459:  Train loss: 3.125821828842163  Test loss: 3.241055965423584 \n",
      "Epoch: 1/10:  mini-batch 1490/4459:  Train loss: 3.457510471343994  Test loss: 3.240327835083008 \n",
      "Epoch: 1/10:  mini-batch 1491/4459:  Train loss: 3.376540184020996  Test loss: 3.2397475242614746 \n",
      "Epoch: 1/10:  mini-batch 1492/4459:  Train loss: 3.4698004722595215  Test loss: 3.2391104698181152 \n",
      "Epoch: 1/10:  mini-batch 1493/4459:  Train loss: 3.4011380672454834  Test loss: 3.2388992309570312 \n",
      "Epoch: 1/10:  mini-batch 1494/4459:  Train loss: 3.3288278579711914  Test loss: 3.2386012077331543 \n",
      "Epoch: 1/10:  mini-batch 1495/4459:  Train loss: 3.150360584259033  Test loss: 3.238182544708252 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1496/4459:  Train loss: 3.6528666019439697  Test loss: 3.2387986183166504 \n",
      "Epoch: 1/10:  mini-batch 1497/4459:  Train loss: 3.340826988220215  Test loss: 3.239663600921631 \n",
      "Epoch: 1/10:  mini-batch 1498/4459:  Train loss: 3.2297422885894775  Test loss: 3.2406513690948486 \n",
      "Epoch: 1/10:  mini-batch 1499/4459:  Train loss: 3.2316737174987793  Test loss: 3.240983486175537 \n",
      "Epoch: 1/10:  mini-batch 1500/4459:  Train loss: 3.2380475997924805  Test loss: 3.2410128116607666 \n",
      "Epoch: 1/10:  mini-batch 1501/4459:  Train loss: 3.5283358097076416  Test loss: 3.2412333488464355 \n",
      "Epoch: 1/10:  mini-batch 1502/4459:  Train loss: 3.1490819454193115  Test loss: 3.2406249046325684 \n",
      "Epoch: 1/10:  mini-batch 1503/4459:  Train loss: 3.275444507598877  Test loss: 3.2397773265838623 \n",
      "Epoch: 1/10:  mini-batch 1504/4459:  Train loss: 3.176320791244507  Test loss: 3.2385616302490234 \n",
      "Epoch: 1/10:  mini-batch 1505/4459:  Train loss: 3.2036139965057373  Test loss: 3.2372727394104004 \n",
      "Epoch: 1/10:  mini-batch 1506/4459:  Train loss: 3.6744039058685303  Test loss: 3.236941337585449 \n",
      "Epoch: 1/10:  mini-batch 1507/4459:  Train loss: 3.015340566635132  Test loss: 3.2360987663269043 \n",
      "Epoch: 1/10:  mini-batch 1508/4459:  Train loss: 3.3396525382995605  Test loss: 3.235717296600342 \n",
      "Epoch: 1/10:  mini-batch 1509/4459:  Train loss: 3.2895164489746094  Test loss: 3.2352590560913086 \n",
      "Epoch: 1/10:  mini-batch 1510/4459:  Train loss: 3.3839545249938965  Test loss: 3.235077142715454 \n",
      "Epoch: 1/10:  mini-batch 1511/4459:  Train loss: 3.6738593578338623  Test loss: 3.2357218265533447 \n",
      "Epoch: 1/10:  mini-batch 1512/4459:  Train loss: 3.2236642837524414  Test loss: 3.236330509185791 \n",
      "Epoch: 1/10:  mini-batch 1513/4459:  Train loss: 3.2955141067504883  Test loss: 3.2369587421417236 \n",
      "Epoch: 1/10:  mini-batch 1514/4459:  Train loss: 3.1580371856689453  Test loss: 3.2374396324157715 \n",
      "Epoch: 1/10:  mini-batch 1515/4459:  Train loss: 3.866086483001709  Test loss: 3.2389674186706543 \n",
      "Epoch: 1/10:  mini-batch 1516/4459:  Train loss: 3.369333505630493  Test loss: 3.2403571605682373 \n",
      "Epoch: 1/10:  mini-batch 1517/4459:  Train loss: 3.5229034423828125  Test loss: 3.2420341968536377 \n",
      "Epoch: 1/10:  mini-batch 1518/4459:  Train loss: 3.384579658508301  Test loss: 3.2439844608306885 \n",
      "Epoch: 1/10:  mini-batch 1519/4459:  Train loss: 3.203218460083008  Test loss: 3.245859384536743 \n",
      "Epoch: 1/10:  mini-batch 1520/4459:  Train loss: 3.1406257152557373  Test loss: 3.2471747398376465 \n",
      "Epoch: 1/10:  mini-batch 1521/4459:  Train loss: 3.374716281890869  Test loss: 3.248570442199707 \n",
      "Epoch: 1/10:  mini-batch 1522/4459:  Train loss: 3.3372507095336914  Test loss: 3.249859094619751 \n",
      "Epoch: 1/10:  mini-batch 1523/4459:  Train loss: 3.5460619926452637  Test loss: 3.251512050628662 \n",
      "Epoch: 1/10:  mini-batch 1524/4459:  Train loss: 3.407810688018799  Test loss: 3.2532315254211426 \n",
      "Epoch: 1/10:  mini-batch 1525/4459:  Train loss: 3.4168450832366943  Test loss: 3.254530906677246 \n",
      "Epoch: 1/10:  mini-batch 1526/4459:  Train loss: 3.19868803024292  Test loss: 3.255384922027588 \n",
      "Epoch: 1/10:  mini-batch 1527/4459:  Train loss: 3.2808501720428467  Test loss: 3.255910634994507 \n",
      "Epoch: 1/10:  mini-batch 1528/4459:  Train loss: 3.571536064147949  Test loss: 3.2569425106048584 \n",
      "Epoch: 1/10:  mini-batch 1529/4459:  Train loss: 3.214351177215576  Test loss: 3.257664918899536 \n",
      "Epoch: 1/10:  mini-batch 1530/4459:  Train loss: 3.1881918907165527  Test loss: 3.2578604221343994 \n",
      "Epoch: 1/10:  mini-batch 1531/4459:  Train loss: 3.4221041202545166  Test loss: 3.258514404296875 \n",
      "Epoch: 1/10:  mini-batch 1532/4459:  Train loss: 3.3995344638824463  Test loss: 3.2593212127685547 \n",
      "Epoch: 1/10:  mini-batch 1533/4459:  Train loss: 2.8339011669158936  Test loss: 3.259326696395874 \n",
      "Epoch: 1/10:  mini-batch 1534/4459:  Train loss: 3.4712820053100586  Test loss: 3.259725570678711 \n",
      "Epoch: 1/10:  mini-batch 1535/4459:  Train loss: 3.597445249557495  Test loss: 3.260652780532837 \n",
      "Epoch: 1/10:  mini-batch 1536/4459:  Train loss: 3.476809501647949  Test loss: 3.262148857116699 \n",
      "Epoch: 1/10:  mini-batch 1537/4459:  Train loss: 3.646946430206299  Test loss: 3.2643239498138428 \n",
      "Epoch: 1/10:  mini-batch 1538/4459:  Train loss: 3.3208718299865723  Test loss: 3.2664358615875244 \n",
      "Epoch: 1/10:  mini-batch 1539/4459:  Train loss: 3.271951198577881  Test loss: 3.2681851387023926 \n",
      "Epoch: 1/10:  mini-batch 1540/4459:  Train loss: 3.8127129077911377  Test loss: 3.27085542678833 \n",
      "Epoch: 1/10:  mini-batch 1541/4459:  Train loss: 3.5102193355560303  Test loss: 3.2737083435058594 \n",
      "Epoch: 1/10:  mini-batch 1542/4459:  Train loss: 3.208909034729004  Test loss: 3.275923728942871 \n",
      "Epoch: 1/10:  mini-batch 1543/4459:  Train loss: 3.4895434379577637  Test loss: 3.278552532196045 \n",
      "Epoch: 1/10:  mini-batch 1544/4459:  Train loss: 3.1936275959014893  Test loss: 3.281057834625244 \n",
      "Epoch: 1/10:  mini-batch 1545/4459:  Train loss: 3.474470615386963  Test loss: 3.283642292022705 \n",
      "Epoch: 1/10:  mini-batch 1546/4459:  Train loss: 3.0415687561035156  Test loss: 3.2851922512054443 \n",
      "Epoch: 1/10:  mini-batch 1547/4459:  Train loss: 3.43376088142395  Test loss: 3.286980390548706 \n",
      "Epoch: 1/10:  mini-batch 1548/4459:  Train loss: 3.489360809326172  Test loss: 3.2890310287475586 \n",
      "Epoch: 1/10:  mini-batch 1549/4459:  Train loss: 3.4560189247131348  Test loss: 3.2910428047180176 \n",
      "Epoch: 1/10:  mini-batch 1550/4459:  Train loss: 3.3274550437927246  Test loss: 3.2929253578186035 \n",
      "Epoch: 1/10:  mini-batch 1551/4459:  Train loss: 3.323976516723633  Test loss: 3.2944746017456055 \n",
      "Epoch: 1/10:  mini-batch 1552/4459:  Train loss: 3.480290651321411  Test loss: 3.2960853576660156 \n",
      "Epoch: 1/10:  mini-batch 1553/4459:  Train loss: 3.5773472785949707  Test loss: 3.297546625137329 \n",
      "Epoch: 1/10:  mini-batch 1554/4459:  Train loss: 3.509891986846924  Test loss: 3.299182415008545 \n",
      "Epoch: 1/10:  mini-batch 1555/4459:  Train loss: 3.3456227779388428  Test loss: 3.300845146179199 \n",
      "Epoch: 1/10:  mini-batch 1556/4459:  Train loss: 3.55633544921875  Test loss: 3.3026976585388184 \n",
      "Epoch: 1/10:  mini-batch 1557/4459:  Train loss: 3.570988655090332  Test loss: 3.304861068725586 \n",
      "Epoch: 1/10:  mini-batch 1558/4459:  Train loss: 3.343648910522461  Test loss: 3.3069095611572266 \n",
      "Epoch: 1/10:  mini-batch 1559/4459:  Train loss: 3.098489284515381  Test loss: 3.3074519634246826 \n",
      "Epoch: 1/10:  mini-batch 1560/4459:  Train loss: 3.4148449897766113  Test loss: 3.3078322410583496 \n",
      "Epoch: 1/10:  mini-batch 1561/4459:  Train loss: 3.2800967693328857  Test loss: 3.307920455932617 \n",
      "Epoch: 1/10:  mini-batch 1562/4459:  Train loss: 3.2145333290100098  Test loss: 3.307739019393921 \n",
      "Epoch: 1/10:  mini-batch 1563/4459:  Train loss: 3.448054790496826  Test loss: 3.3078055381774902 \n",
      "Epoch: 1/10:  mini-batch 1564/4459:  Train loss: 3.659538984298706  Test loss: 3.3084371089935303 \n",
      "Epoch: 1/10:  mini-batch 1565/4459:  Train loss: 3.5094990730285645  Test loss: 3.309274435043335 \n",
      "Epoch: 1/10:  mini-batch 1566/4459:  Train loss: 3.530468225479126  Test loss: 3.310549259185791 \n",
      "Epoch: 1/10:  mini-batch 1567/4459:  Train loss: 3.2210893630981445  Test loss: 3.31135892868042 \n",
      "Epoch: 1/10:  mini-batch 1568/4459:  Train loss: 3.3189773559570312  Test loss: 3.311944007873535 \n",
      "Epoch: 1/10:  mini-batch 1569/4459:  Train loss: 3.131187915802002  Test loss: 3.3121488094329834 \n",
      "Epoch: 1/10:  mini-batch 1570/4459:  Train loss: 3.0767157077789307  Test loss: 3.311330795288086 \n",
      "Epoch: 1/10:  mini-batch 1571/4459:  Train loss: 3.1998469829559326  Test loss: 3.310011625289917 \n",
      "Epoch: 1/10:  mini-batch 1572/4459:  Train loss: 3.2853035926818848  Test loss: 3.308250904083252 \n",
      "Epoch: 1/10:  mini-batch 1573/4459:  Train loss: 3.36025071144104  Test loss: 3.306602716445923 \n",
      "Epoch: 1/10:  mini-batch 1574/4459:  Train loss: 3.1849923133850098  Test loss: 3.304868698120117 \n",
      "Epoch: 1/10:  mini-batch 1575/4459:  Train loss: 3.1704001426696777  Test loss: 3.3028342723846436 \n",
      "Epoch: 1/10:  mini-batch 1576/4459:  Train loss: 3.287778615951538  Test loss: 3.301163911819458 \n",
      "Epoch: 1/10:  mini-batch 1577/4459:  Train loss: 3.377866268157959  Test loss: 3.2996532917022705 \n",
      "Epoch: 1/10:  mini-batch 1578/4459:  Train loss: 2.796586036682129  Test loss: 3.2975029945373535 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1579/4459:  Train loss: 3.0255274772644043  Test loss: 3.2952728271484375 \n",
      "Epoch: 1/10:  mini-batch 1580/4459:  Train loss: 3.1613922119140625  Test loss: 3.2932801246643066 \n",
      "Epoch: 1/10:  mini-batch 1581/4459:  Train loss: 3.20428729057312  Test loss: 3.2917003631591797 \n",
      "Epoch: 1/10:  mini-batch 1582/4459:  Train loss: 3.2378151416778564  Test loss: 3.2910284996032715 \n",
      "Epoch: 1/10:  mini-batch 1583/4459:  Train loss: 3.2063004970550537  Test loss: 3.2898612022399902 \n",
      "Epoch: 1/10:  mini-batch 1584/4459:  Train loss: 3.121392011642456  Test loss: 3.2896928787231445 \n",
      "Epoch: 1/10:  mini-batch 1585/4459:  Train loss: 3.1129589080810547  Test loss: 3.290858745574951 \n",
      "Epoch: 1/10:  mini-batch 1586/4459:  Train loss: 2.939209222793579  Test loss: 3.295004367828369 \n",
      "Epoch: 1/10:  mini-batch 1587/4459:  Train loss: 3.254298210144043  Test loss: 3.302263021469116 \n",
      "Epoch: 1/10:  mini-batch 1588/4459:  Train loss: 3.600402593612671  Test loss: 3.3034677505493164 \n",
      "Epoch: 1/10:  mini-batch 1589/4459:  Train loss: 3.2836546897888184  Test loss: 3.3038222789764404 \n",
      "Epoch: 1/10:  mini-batch 1590/4459:  Train loss: 3.1639962196350098  Test loss: 3.3032636642456055 \n",
      "Epoch: 1/10:  mini-batch 1591/4459:  Train loss: 3.3428969383239746  Test loss: 3.30135178565979 \n",
      "Epoch: 1/10:  mini-batch 1592/4459:  Train loss: 3.261782646179199  Test loss: 3.2994556427001953 \n",
      "Epoch: 1/10:  mini-batch 1593/4459:  Train loss: 3.653764247894287  Test loss: 3.296358108520508 \n",
      "Epoch: 1/10:  mini-batch 1594/4459:  Train loss: 3.11637544631958  Test loss: 3.294715404510498 \n",
      "Epoch: 1/10:  mini-batch 1595/4459:  Train loss: 3.0099682807922363  Test loss: 3.29409122467041 \n",
      "Epoch: 1/10:  mini-batch 1596/4459:  Train loss: 3.035935878753662  Test loss: 3.295001983642578 \n",
      "Epoch: 1/10:  mini-batch 1597/4459:  Train loss: 3.7012863159179688  Test loss: 3.2953734397888184 \n",
      "Epoch: 1/10:  mini-batch 1598/4459:  Train loss: 2.8615975379943848  Test loss: 3.2964820861816406 \n",
      "Epoch: 1/10:  mini-batch 1599/4459:  Train loss: 2.912846803665161  Test loss: 3.2977733612060547 \n",
      "Epoch: 1/10:  mini-batch 1600/4459:  Train loss: 3.3080620765686035  Test loss: 3.2992286682128906 \n",
      "Epoch: 1/10:  mini-batch 1601/4459:  Train loss: 3.453596830368042  Test loss: 3.301347017288208 \n",
      "Epoch: 1/10:  mini-batch 1602/4459:  Train loss: 2.794558525085449  Test loss: 3.3040289878845215 \n",
      "Epoch: 1/10:  mini-batch 1603/4459:  Train loss: 2.9889965057373047  Test loss: 3.307276725769043 \n",
      "Epoch: 1/10:  mini-batch 1604/4459:  Train loss: 3.8987393379211426  Test loss: 3.3096141815185547 \n",
      "Epoch: 1/10:  mini-batch 1605/4459:  Train loss: 3.6642167568206787  Test loss: 3.3109803199768066 \n",
      "Epoch: 1/10:  mini-batch 1606/4459:  Train loss: 3.026270866394043  Test loss: 3.3125686645507812 \n",
      "Epoch: 1/10:  mini-batch 1607/4459:  Train loss: 2.8036983013153076  Test loss: 3.3147945404052734 \n",
      "Epoch: 1/10:  mini-batch 1608/4459:  Train loss: 3.3612661361694336  Test loss: 3.3159923553466797 \n",
      "Epoch: 1/10:  mini-batch 1609/4459:  Train loss: 3.138676881790161  Test loss: 3.3168835639953613 \n",
      "Epoch: 1/10:  mini-batch 1610/4459:  Train loss: 3.116522789001465  Test loss: 3.317429542541504 \n",
      "Epoch: 1/10:  mini-batch 1611/4459:  Train loss: 3.2821147441864014  Test loss: 3.31784725189209 \n",
      "Epoch: 1/10:  mini-batch 1612/4459:  Train loss: 3.164419412612915  Test loss: 3.31833815574646 \n",
      "Epoch: 1/10:  mini-batch 1613/4459:  Train loss: 3.114297389984131  Test loss: 3.3181300163269043 \n",
      "Epoch: 1/10:  mini-batch 1614/4459:  Train loss: 3.278186082839966  Test loss: 3.318218231201172 \n",
      "Epoch: 1/10:  mini-batch 1615/4459:  Train loss: 3.544106960296631  Test loss: 3.3183224201202393 \n",
      "Epoch: 1/10:  mini-batch 1616/4459:  Train loss: 2.972534656524658  Test loss: 3.3196041584014893 \n",
      "Epoch: 1/10:  mini-batch 1617/4459:  Train loss: 3.3147964477539062  Test loss: 3.3206753730773926 \n",
      "Epoch: 1/10:  mini-batch 1618/4459:  Train loss: 3.193075656890869  Test loss: 3.3221497535705566 \n",
      "Epoch: 1/10:  mini-batch 1619/4459:  Train loss: 3.624648094177246  Test loss: 3.3222408294677734 \n",
      "Epoch: 1/10:  mini-batch 1620/4459:  Train loss: 2.9953718185424805  Test loss: 3.3227367401123047 \n",
      "Epoch: 1/10:  mini-batch 1621/4459:  Train loss: 3.0152602195739746  Test loss: 3.323040723800659 \n",
      "Epoch: 1/10:  mini-batch 1622/4459:  Train loss: 2.877939224243164  Test loss: 3.324561834335327 \n",
      "Epoch: 1/10:  mini-batch 1623/4459:  Train loss: 3.536953926086426  Test loss: 3.326155185699463 \n",
      "Epoch: 1/10:  mini-batch 1624/4459:  Train loss: 3.122814655303955  Test loss: 3.3280553817749023 \n",
      "Epoch: 1/10:  mini-batch 1625/4459:  Train loss: 3.190652847290039  Test loss: 3.329658031463623 \n",
      "Epoch: 1/10:  mini-batch 1626/4459:  Train loss: 3.994457483291626  Test loss: 3.329941749572754 \n",
      "Epoch: 1/10:  mini-batch 1627/4459:  Train loss: 3.846045970916748  Test loss: 3.3291385173797607 \n",
      "Epoch: 1/10:  mini-batch 1628/4459:  Train loss: 3.2784290313720703  Test loss: 3.32774019241333 \n",
      "Epoch: 1/10:  mini-batch 1629/4459:  Train loss: 3.2689812183380127  Test loss: 3.327042579650879 \n",
      "Epoch: 1/10:  mini-batch 1630/4459:  Train loss: 3.6899895668029785  Test loss: 3.326859712600708 \n",
      "Epoch: 1/10:  mini-batch 1631/4459:  Train loss: 2.858454704284668  Test loss: 3.326944351196289 \n",
      "Epoch: 1/10:  mini-batch 1632/4459:  Train loss: 3.2781410217285156  Test loss: 3.326150894165039 \n",
      "Epoch: 1/10:  mini-batch 1633/4459:  Train loss: 3.7267026901245117  Test loss: 3.324984312057495 \n",
      "Epoch: 1/10:  mini-batch 1634/4459:  Train loss: 2.8453166484832764  Test loss: 3.324658155441284 \n",
      "Epoch: 1/10:  mini-batch 1635/4459:  Train loss: 3.3018012046813965  Test loss: 3.324275255203247 \n",
      "Epoch: 1/10:  mini-batch 1636/4459:  Train loss: 3.0841240882873535  Test loss: 3.324157238006592 \n",
      "Epoch: 1/10:  mini-batch 1637/4459:  Train loss: 3.156282663345337  Test loss: 3.324329376220703 \n",
      "Epoch: 1/10:  mini-batch 1638/4459:  Train loss: 3.1491684913635254  Test loss: 3.3246774673461914 \n",
      "Epoch: 1/10:  mini-batch 1639/4459:  Train loss: 3.2325077056884766  Test loss: 3.3247487545013428 \n",
      "Epoch: 1/10:  mini-batch 1640/4459:  Train loss: 3.072294235229492  Test loss: 3.3249025344848633 \n",
      "Epoch: 1/10:  mini-batch 1641/4459:  Train loss: 3.1227822303771973  Test loss: 3.325864315032959 \n",
      "Epoch: 1/10:  mini-batch 1642/4459:  Train loss: 3.388643980026245  Test loss: 3.3269577026367188 \n",
      "Epoch: 1/10:  mini-batch 1643/4459:  Train loss: 3.2610673904418945  Test loss: 3.327042579650879 \n",
      "Epoch: 1/10:  mini-batch 1644/4459:  Train loss: 3.3067240715026855  Test loss: 3.3270270824432373 \n",
      "Epoch: 1/10:  mini-batch 1645/4459:  Train loss: 3.5487771034240723  Test loss: 3.32735538482666 \n",
      "Epoch: 1/10:  mini-batch 1646/4459:  Train loss: 3.5314013957977295  Test loss: 3.3276164531707764 \n",
      "Epoch: 1/10:  mini-batch 1647/4459:  Train loss: 3.4674060344696045  Test loss: 3.3286476135253906 \n",
      "Epoch: 1/10:  mini-batch 1648/4459:  Train loss: 3.372617721557617  Test loss: 3.3293464183807373 \n",
      "Epoch: 1/10:  mini-batch 1649/4459:  Train loss: 3.506279468536377  Test loss: 3.329711437225342 \n",
      "Epoch: 1/10:  mini-batch 1650/4459:  Train loss: 3.4343228340148926  Test loss: 3.330528736114502 \n",
      "Epoch: 1/10:  mini-batch 1651/4459:  Train loss: 3.006103515625  Test loss: 3.3311195373535156 \n",
      "Epoch: 1/10:  mini-batch 1652/4459:  Train loss: 3.184086799621582  Test loss: 3.3313701152801514 \n",
      "Epoch: 1/10:  mini-batch 1653/4459:  Train loss: 3.5648484230041504  Test loss: 3.3318612575531006 \n",
      "Epoch: 1/10:  mini-batch 1654/4459:  Train loss: 2.9721691608428955  Test loss: 3.332561492919922 \n",
      "Epoch: 1/10:  mini-batch 1655/4459:  Train loss: 2.862199306488037  Test loss: 3.332838773727417 \n",
      "Epoch: 1/10:  mini-batch 1656/4459:  Train loss: 3.3098347187042236  Test loss: 3.333097457885742 \n",
      "Epoch: 1/10:  mini-batch 1657/4459:  Train loss: 3.5202102661132812  Test loss: 3.3337554931640625 \n",
      "Epoch: 1/10:  mini-batch 1658/4459:  Train loss: 3.4466779232025146  Test loss: 3.334423303604126 \n",
      "Epoch: 1/10:  mini-batch 1659/4459:  Train loss: 2.908937454223633  Test loss: 3.3352999687194824 \n",
      "Epoch: 1/10:  mini-batch 1660/4459:  Train loss: 3.081267833709717  Test loss: 3.3360443115234375 \n",
      "Epoch: 1/10:  mini-batch 1661/4459:  Train loss: 3.245863676071167  Test loss: 3.336833953857422 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1662/4459:  Train loss: 3.192267417907715  Test loss: 3.3372037410736084 \n",
      "Epoch: 1/10:  mini-batch 1663/4459:  Train loss: 3.2212045192718506  Test loss: 3.3371896743774414 \n",
      "Epoch: 1/10:  mini-batch 1664/4459:  Train loss: 3.3118574619293213  Test loss: 3.3372626304626465 \n",
      "Epoch: 1/10:  mini-batch 1665/4459:  Train loss: 2.9827260971069336  Test loss: 3.338172435760498 \n",
      "Epoch: 1/10:  mini-batch 1666/4459:  Train loss: 3.0965349674224854  Test loss: 3.3391706943511963 \n",
      "Epoch: 1/10:  mini-batch 1667/4459:  Train loss: 3.6440834999084473  Test loss: 3.3393898010253906 \n",
      "Epoch: 1/10:  mini-batch 1668/4459:  Train loss: 3.193143844604492  Test loss: 3.3393986225128174 \n",
      "Epoch: 1/10:  mini-batch 1669/4459:  Train loss: 3.268812894821167  Test loss: 3.3390870094299316 \n",
      "Epoch: 1/10:  mini-batch 1670/4459:  Train loss: 3.449885129928589  Test loss: 3.3388900756835938 \n",
      "Epoch: 1/10:  mini-batch 1671/4459:  Train loss: 3.1371021270751953  Test loss: 3.338994026184082 \n",
      "Epoch: 1/10:  mini-batch 1672/4459:  Train loss: 3.2366158962249756  Test loss: 3.3389148712158203 \n",
      "Epoch: 1/10:  mini-batch 1673/4459:  Train loss: 2.9042842388153076  Test loss: 3.338968276977539 \n",
      "Epoch: 1/10:  mini-batch 1674/4459:  Train loss: 3.0996253490448  Test loss: 3.338926315307617 \n",
      "Epoch: 1/10:  mini-batch 1675/4459:  Train loss: 3.5145692825317383  Test loss: 3.337846279144287 \n",
      "Epoch: 1/10:  mini-batch 1676/4459:  Train loss: 2.968388319015503  Test loss: 3.3375794887542725 \n",
      "Epoch: 1/10:  mini-batch 1677/4459:  Train loss: 3.1552982330322266  Test loss: 3.336923599243164 \n",
      "Epoch: 1/10:  mini-batch 1678/4459:  Train loss: 2.8979687690734863  Test loss: 3.3369717597961426 \n",
      "Epoch: 1/10:  mini-batch 1679/4459:  Train loss: 3.360908031463623  Test loss: 3.3365230560302734 \n",
      "Epoch: 1/10:  mini-batch 1680/4459:  Train loss: 3.4049453735351562  Test loss: 3.3355391025543213 \n",
      "Epoch: 1/10:  mini-batch 1681/4459:  Train loss: 3.031548500061035  Test loss: 3.334836483001709 \n",
      "Epoch: 1/10:  mini-batch 1682/4459:  Train loss: 3.139765739440918  Test loss: 3.334744930267334 \n",
      "Epoch: 1/10:  mini-batch 1683/4459:  Train loss: 3.1387057304382324  Test loss: 3.33480167388916 \n",
      "Epoch: 1/10:  mini-batch 1684/4459:  Train loss: 3.0736746788024902  Test loss: 3.335055351257324 \n",
      "Epoch: 1/10:  mini-batch 1685/4459:  Train loss: 3.0784449577331543  Test loss: 3.3357558250427246 \n",
      "Epoch: 1/10:  mini-batch 1686/4459:  Train loss: 3.402914524078369  Test loss: 3.3357977867126465 \n",
      "Epoch: 1/10:  mini-batch 1687/4459:  Train loss: 3.3629982471466064  Test loss: 3.334970474243164 \n",
      "Epoch: 1/10:  mini-batch 1688/4459:  Train loss: 3.2739620208740234  Test loss: 3.3346147537231445 \n",
      "Epoch: 1/10:  mini-batch 1689/4459:  Train loss: 3.5195722579956055  Test loss: 3.333446979522705 \n",
      "Epoch: 1/10:  mini-batch 1690/4459:  Train loss: 3.7083897590637207  Test loss: 3.3315212726593018 \n",
      "Epoch: 1/10:  mini-batch 1691/4459:  Train loss: 3.1372270584106445  Test loss: 3.3301351070404053 \n",
      "Epoch: 1/10:  mini-batch 1692/4459:  Train loss: 3.2428550720214844  Test loss: 3.3282103538513184 \n",
      "Epoch: 1/10:  mini-batch 1693/4459:  Train loss: 3.6561145782470703  Test loss: 3.3256235122680664 \n",
      "Epoch: 1/10:  mini-batch 1694/4459:  Train loss: 3.076000213623047  Test loss: 3.324319839477539 \n",
      "Epoch: 1/10:  mini-batch 1695/4459:  Train loss: 3.2077090740203857  Test loss: 3.3230879306793213 \n",
      "Epoch: 1/10:  mini-batch 1696/4459:  Train loss: 3.1220953464508057  Test loss: 3.322478771209717 \n",
      "Epoch: 1/10:  mini-batch 1697/4459:  Train loss: 2.8484280109405518  Test loss: 3.322284698486328 \n",
      "Epoch: 1/10:  mini-batch 1698/4459:  Train loss: 3.1121912002563477  Test loss: 3.3222789764404297 \n",
      "Epoch: 1/10:  mini-batch 1699/4459:  Train loss: 3.3310298919677734  Test loss: 3.3219051361083984 \n",
      "Epoch: 1/10:  mini-batch 1700/4459:  Train loss: 3.649026870727539  Test loss: 3.3213918209075928 \n",
      "Epoch: 1/10:  mini-batch 1701/4459:  Train loss: 2.704101085662842  Test loss: 3.3222110271453857 \n",
      "Epoch: 1/10:  mini-batch 1702/4459:  Train loss: 3.162196159362793  Test loss: 3.3227901458740234 \n",
      "Epoch: 1/10:  mini-batch 1703/4459:  Train loss: 3.350796937942505  Test loss: 3.323395252227783 \n",
      "Epoch: 1/10:  mini-batch 1704/4459:  Train loss: 3.9163436889648438  Test loss: 3.3236701488494873 \n",
      "Epoch: 1/10:  mini-batch 1705/4459:  Train loss: 3.026719331741333  Test loss: 3.323901414871216 \n",
      "Epoch: 1/10:  mini-batch 1706/4459:  Train loss: 2.911311388015747  Test loss: 3.324833869934082 \n",
      "Epoch: 1/10:  mini-batch 1707/4459:  Train loss: 3.3979759216308594  Test loss: 3.325483798980713 \n",
      "Epoch: 1/10:  mini-batch 1708/4459:  Train loss: 2.9595494270324707  Test loss: 3.3267362117767334 \n",
      "Epoch: 1/10:  mini-batch 1709/4459:  Train loss: 3.1602623462677  Test loss: 3.32827091217041 \n",
      "Epoch: 1/10:  mini-batch 1710/4459:  Train loss: 3.836000919342041  Test loss: 3.329400062561035 \n",
      "Epoch: 1/10:  mini-batch 1711/4459:  Train loss: 3.323639392852783  Test loss: 3.330284595489502 \n",
      "Epoch: 1/10:  mini-batch 1712/4459:  Train loss: 2.8711602687835693  Test loss: 3.3314504623413086 \n",
      "Epoch: 1/10:  mini-batch 1713/4459:  Train loss: 2.7736165523529053  Test loss: 3.3334832191467285 \n",
      "Epoch: 1/10:  mini-batch 1714/4459:  Train loss: 2.948005199432373  Test loss: 3.3358561992645264 \n",
      "Epoch: 1/10:  mini-batch 1715/4459:  Train loss: 3.305264472961426  Test loss: 3.337728500366211 \n",
      "Epoch: 1/10:  mini-batch 1716/4459:  Train loss: 3.1341171264648438  Test loss: 3.3393807411193848 \n",
      "Epoch: 1/10:  mini-batch 1717/4459:  Train loss: 3.460456609725952  Test loss: 3.3410160541534424 \n",
      "Epoch: 1/10:  mini-batch 1718/4459:  Train loss: 3.6528329849243164  Test loss: 3.342085838317871 \n",
      "Epoch: 1/10:  mini-batch 1719/4459:  Train loss: 3.9136548042297363  Test loss: 3.3418612480163574 \n",
      "Epoch: 1/10:  mini-batch 1720/4459:  Train loss: 3.649827480316162  Test loss: 3.3409178256988525 \n",
      "Epoch: 1/10:  mini-batch 1721/4459:  Train loss: 3.2568247318267822  Test loss: 3.339820384979248 \n",
      "Epoch: 1/10:  mini-batch 1722/4459:  Train loss: 2.9618773460388184  Test loss: 3.339492082595825 \n",
      "Epoch: 1/10:  mini-batch 1723/4459:  Train loss: 3.150449752807617  Test loss: 3.3385722637176514 \n",
      "Epoch: 1/10:  mini-batch 1724/4459:  Train loss: 3.3869380950927734  Test loss: 3.337662696838379 \n",
      "Epoch: 1/10:  mini-batch 1725/4459:  Train loss: 3.411011219024658  Test loss: 3.336534023284912 \n",
      "Epoch: 1/10:  mini-batch 1726/4459:  Train loss: 3.211357593536377  Test loss: 3.3356504440307617 \n",
      "Epoch: 1/10:  mini-batch 1727/4459:  Train loss: 3.1309621334075928  Test loss: 3.3349876403808594 \n",
      "Epoch: 1/10:  mini-batch 1728/4459:  Train loss: 3.249481201171875  Test loss: 3.334359645843506 \n",
      "Epoch: 1/10:  mini-batch 1729/4459:  Train loss: 3.4654347896575928  Test loss: 3.3337249755859375 \n",
      "Epoch: 1/10:  mini-batch 1730/4459:  Train loss: 3.3468899726867676  Test loss: 3.332984209060669 \n",
      "Epoch: 1/10:  mini-batch 1731/4459:  Train loss: 3.5427541732788086  Test loss: 3.3320367336273193 \n",
      "Epoch: 1/10:  mini-batch 1732/4459:  Train loss: 3.201004981994629  Test loss: 3.331362247467041 \n",
      "Epoch: 1/10:  mini-batch 1733/4459:  Train loss: 3.4011874198913574  Test loss: 3.3309125900268555 \n",
      "Epoch: 1/10:  mini-batch 1734/4459:  Train loss: 3.3504199981689453  Test loss: 3.3311972618103027 \n",
      "Epoch: 1/10:  mini-batch 1735/4459:  Train loss: 3.0640268325805664  Test loss: 3.331480026245117 \n",
      "Epoch: 1/10:  mini-batch 1736/4459:  Train loss: 3.305023431777954  Test loss: 3.3315844535827637 \n",
      "Epoch: 1/10:  mini-batch 1737/4459:  Train loss: 3.2346105575561523  Test loss: 3.331660270690918 \n",
      "Epoch: 1/10:  mini-batch 1738/4459:  Train loss: 3.1856460571289062  Test loss: 3.332345485687256 \n",
      "Epoch: 1/10:  mini-batch 1739/4459:  Train loss: 3.3350439071655273  Test loss: 3.332690477371216 \n",
      "Epoch: 1/10:  mini-batch 1740/4459:  Train loss: 3.414944648742676  Test loss: 3.332705020904541 \n",
      "Epoch: 1/10:  mini-batch 1741/4459:  Train loss: 3.0891170501708984  Test loss: 3.3328757286071777 \n",
      "Epoch: 1/10:  mini-batch 1742/4459:  Train loss: 3.4095122814178467  Test loss: 3.3334269523620605 \n",
      "Epoch: 1/10:  mini-batch 1743/4459:  Train loss: 3.6934099197387695  Test loss: 3.334259033203125 \n",
      "Epoch: 1/10:  mini-batch 1744/4459:  Train loss: 3.6023263931274414  Test loss: 3.335268497467041 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1745/4459:  Train loss: 3.6670196056365967  Test loss: 3.3356516361236572 \n",
      "Epoch: 1/10:  mini-batch 1746/4459:  Train loss: 3.339406967163086  Test loss: 3.3362436294555664 \n",
      "Epoch: 1/10:  mini-batch 1747/4459:  Train loss: 3.0509493350982666  Test loss: 3.3365087509155273 \n",
      "Epoch: 1/10:  mini-batch 1748/4459:  Train loss: 3.2068381309509277  Test loss: 3.3369250297546387 \n",
      "Epoch: 1/10:  mini-batch 1749/4459:  Train loss: 3.423964262008667  Test loss: 3.3374290466308594 \n",
      "Epoch: 1/10:  mini-batch 1750/4459:  Train loss: 3.445295572280884  Test loss: 3.3378562927246094 \n",
      "Epoch: 1/10:  mini-batch 1751/4459:  Train loss: 2.9276835918426514  Test loss: 3.3383774757385254 \n",
      "Epoch: 1/10:  mini-batch 1752/4459:  Train loss: 3.5510025024414062  Test loss: 3.3389453887939453 \n",
      "Epoch: 1/10:  mini-batch 1753/4459:  Train loss: 3.8938117027282715  Test loss: 3.339757204055786 \n",
      "Epoch: 1/10:  mini-batch 1754/4459:  Train loss: 3.651550531387329  Test loss: 3.3407115936279297 \n",
      "Epoch: 1/10:  mini-batch 1755/4459:  Train loss: 3.199270248413086  Test loss: 3.342029571533203 \n",
      "Epoch: 1/10:  mini-batch 1756/4459:  Train loss: 3.609053611755371  Test loss: 3.3434672355651855 \n",
      "Epoch: 1/10:  mini-batch 1757/4459:  Train loss: 3.5188536643981934  Test loss: 3.344900131225586 \n",
      "Epoch: 1/10:  mini-batch 1758/4459:  Train loss: 3.1655547618865967  Test loss: 3.34624981880188 \n",
      "Epoch: 1/10:  mini-batch 1759/4459:  Train loss: 2.880206346511841  Test loss: 3.3472588062286377 \n",
      "Epoch: 1/10:  mini-batch 1760/4459:  Train loss: 3.3300700187683105  Test loss: 3.348435163497925 \n",
      "Epoch: 1/10:  mini-batch 1761/4459:  Train loss: 2.99514102935791  Test loss: 3.349184036254883 \n",
      "Epoch: 1/10:  mini-batch 1762/4459:  Train loss: 3.6161487102508545  Test loss: 3.3502368927001953 \n",
      "Epoch: 1/10:  mini-batch 1763/4459:  Train loss: 3.598653554916382  Test loss: 3.3514583110809326 \n",
      "Epoch: 1/10:  mini-batch 1764/4459:  Train loss: 3.324335813522339  Test loss: 3.352198362350464 \n",
      "Epoch: 1/10:  mini-batch 1765/4459:  Train loss: 3.4630627632141113  Test loss: 3.3524296283721924 \n",
      "Epoch: 1/10:  mini-batch 1766/4459:  Train loss: 3.282606363296509  Test loss: 3.3529491424560547 \n",
      "Epoch: 1/10:  mini-batch 1767/4459:  Train loss: 2.9402809143066406  Test loss: 3.353062391281128 \n",
      "Epoch: 1/10:  mini-batch 1768/4459:  Train loss: 3.4593377113342285  Test loss: 3.3530259132385254 \n",
      "Epoch: 1/10:  mini-batch 1769/4459:  Train loss: 3.4974758625030518  Test loss: 3.3531060218811035 \n",
      "Epoch: 1/10:  mini-batch 1770/4459:  Train loss: 3.5489346981048584  Test loss: 3.3533852100372314 \n",
      "Epoch: 1/10:  mini-batch 1771/4459:  Train loss: 3.345627546310425  Test loss: 3.353696584701538 \n",
      "Epoch: 1/10:  mini-batch 1772/4459:  Train loss: 3.770469903945923  Test loss: 3.35441255569458 \n",
      "Epoch: 1/10:  mini-batch 1773/4459:  Train loss: 3.2307121753692627  Test loss: 3.354668617248535 \n",
      "Epoch: 1/10:  mini-batch 1774/4459:  Train loss: 3.1584362983703613  Test loss: 3.354938507080078 \n",
      "Epoch: 1/10:  mini-batch 1775/4459:  Train loss: 3.1518843173980713  Test loss: 3.354854106903076 \n",
      "Epoch: 1/10:  mini-batch 1776/4459:  Train loss: 3.3450160026550293  Test loss: 3.355138063430786 \n",
      "Epoch: 1/10:  mini-batch 1777/4459:  Train loss: 3.228161096572876  Test loss: 3.3551571369171143 \n",
      "Epoch: 1/10:  mini-batch 1778/4459:  Train loss: 2.9285199642181396  Test loss: 3.3551549911499023 \n",
      "Epoch: 1/10:  mini-batch 1779/4459:  Train loss: 2.9263410568237305  Test loss: 3.3555235862731934 \n",
      "Epoch: 1/10:  mini-batch 1780/4459:  Train loss: 3.002303123474121  Test loss: 3.3560357093811035 \n",
      "Epoch: 1/10:  mini-batch 1781/4459:  Train loss: 3.6200356483459473  Test loss: 3.356795310974121 \n",
      "Epoch: 1/10:  mini-batch 1782/4459:  Train loss: 3.357675075531006  Test loss: 3.3574376106262207 \n",
      "Epoch: 1/10:  mini-batch 1783/4459:  Train loss: 3.4301698207855225  Test loss: 3.3586597442626953 \n",
      "Epoch: 1/10:  mini-batch 1784/4459:  Train loss: 3.6445188522338867  Test loss: 3.360219717025757 \n",
      "Epoch: 1/10:  mini-batch 1785/4459:  Train loss: 3.6571812629699707  Test loss: 3.3620643615722656 \n",
      "Epoch: 1/10:  mini-batch 1786/4459:  Train loss: 3.3607242107391357  Test loss: 3.3638017177581787 \n",
      "Epoch: 1/10:  mini-batch 1787/4459:  Train loss: 3.234720230102539  Test loss: 3.3651673793792725 \n",
      "Epoch: 1/10:  mini-batch 1788/4459:  Train loss: 3.5294554233551025  Test loss: 3.365980863571167 \n",
      "Epoch: 1/10:  mini-batch 1789/4459:  Train loss: 3.226072311401367  Test loss: 3.366274356842041 \n",
      "Epoch: 1/10:  mini-batch 1790/4459:  Train loss: 3.302680492401123  Test loss: 3.366189479827881 \n",
      "Epoch: 1/10:  mini-batch 1791/4459:  Train loss: 3.642629384994507  Test loss: 3.3663864135742188 \n",
      "Epoch: 1/10:  mini-batch 1792/4459:  Train loss: 3.4470443725585938  Test loss: 3.366074562072754 \n",
      "Epoch: 1/10:  mini-batch 1793/4459:  Train loss: 3.562936782836914  Test loss: 3.3659346103668213 \n",
      "Epoch: 1/10:  mini-batch 1794/4459:  Train loss: 3.5087406635284424  Test loss: 3.3659157752990723 \n",
      "Epoch: 1/10:  mini-batch 1795/4459:  Train loss: 3.3365747928619385  Test loss: 3.3657777309417725 \n",
      "Epoch: 1/10:  mini-batch 1796/4459:  Train loss: 3.0557379722595215  Test loss: 3.3659119606018066 \n",
      "Epoch: 1/10:  mini-batch 1797/4459:  Train loss: 3.4615063667297363  Test loss: 3.3661301136016846 \n",
      "Epoch: 1/10:  mini-batch 1798/4459:  Train loss: 3.6221508979797363  Test loss: 3.3668503761291504 \n",
      "Epoch: 1/10:  mini-batch 1799/4459:  Train loss: 3.4810218811035156  Test loss: 3.3675119876861572 \n",
      "Epoch: 1/10:  mini-batch 1800/4459:  Train loss: 3.8085622787475586  Test loss: 3.368239402770996 \n",
      "Epoch: 1/10:  mini-batch 1801/4459:  Train loss: 3.8857975006103516  Test loss: 3.369412899017334 \n",
      "Epoch: 1/10:  mini-batch 1802/4459:  Train loss: 3.2980453968048096  Test loss: 3.3708362579345703 \n",
      "Epoch: 1/10:  mini-batch 1803/4459:  Train loss: 3.419283866882324  Test loss: 3.3722097873687744 \n",
      "Epoch: 1/10:  mini-batch 1804/4459:  Train loss: 3.191868305206299  Test loss: 3.373551607131958 \n",
      "Epoch: 1/10:  mini-batch 1805/4459:  Train loss: 3.4054808616638184  Test loss: 3.3754405975341797 \n",
      "Epoch: 1/10:  mini-batch 1806/4459:  Train loss: 3.498757839202881  Test loss: 3.3776116371154785 \n",
      "Epoch: 1/10:  mini-batch 1807/4459:  Train loss: 3.118143081665039  Test loss: 3.379180908203125 \n",
      "Epoch: 1/10:  mini-batch 1808/4459:  Train loss: 3.278285264968872  Test loss: 3.380521774291992 \n",
      "Epoch: 1/10:  mini-batch 1809/4459:  Train loss: 3.2635724544525146  Test loss: 3.381897449493408 \n",
      "Epoch: 1/10:  mini-batch 1810/4459:  Train loss: 3.0750155448913574  Test loss: 3.3831069469451904 \n",
      "Epoch: 1/10:  mini-batch 1811/4459:  Train loss: 3.3247623443603516  Test loss: 3.384340286254883 \n",
      "Epoch: 1/10:  mini-batch 1812/4459:  Train loss: 3.187540054321289  Test loss: 3.3851637840270996 \n",
      "Epoch: 1/10:  mini-batch 1813/4459:  Train loss: 3.329749584197998  Test loss: 3.386058807373047 \n",
      "Epoch: 1/10:  mini-batch 1814/4459:  Train loss: 3.1366233825683594  Test loss: 3.3864493370056152 \n",
      "Epoch: 1/10:  mini-batch 1815/4459:  Train loss: 3.22952938079834  Test loss: 3.3865630626678467 \n",
      "Epoch: 1/10:  mini-batch 1816/4459:  Train loss: 3.2497074604034424  Test loss: 3.3866498470306396 \n",
      "Epoch: 1/10:  mini-batch 1817/4459:  Train loss: 3.673706293106079  Test loss: 3.386514186859131 \n",
      "Epoch: 1/10:  mini-batch 1818/4459:  Train loss: 3.301981210708618  Test loss: 3.3866372108459473 \n",
      "Epoch: 1/10:  mini-batch 1819/4459:  Train loss: 3.3594603538513184  Test loss: 3.385972499847412 \n",
      "Epoch: 1/10:  mini-batch 1820/4459:  Train loss: 3.28898024559021  Test loss: 3.3851146697998047 \n",
      "Epoch: 1/10:  mini-batch 1821/4459:  Train loss: 3.4379942417144775  Test loss: 3.3844642639160156 \n",
      "Epoch: 1/10:  mini-batch 1822/4459:  Train loss: 3.4465253353118896  Test loss: 3.383591890335083 \n",
      "Epoch: 1/10:  mini-batch 1823/4459:  Train loss: 3.556558132171631  Test loss: 3.3828814029693604 \n",
      "Epoch: 1/10:  mini-batch 1824/4459:  Train loss: 3.1374897956848145  Test loss: 3.3818793296813965 \n",
      "Epoch: 1/10:  mini-batch 1825/4459:  Train loss: 3.1227922439575195  Test loss: 3.380751609802246 \n",
      "Epoch: 1/10:  mini-batch 1826/4459:  Train loss: 3.2942867279052734  Test loss: 3.379331588745117 \n",
      "Epoch: 1/10:  mini-batch 1827/4459:  Train loss: 3.7258682250976562  Test loss: 3.3781898021698 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1828/4459:  Train loss: 3.133175849914551  Test loss: 3.3771421909332275 \n",
      "Epoch: 1/10:  mini-batch 1829/4459:  Train loss: 3.190979480743408  Test loss: 3.376148223876953 \n",
      "Epoch: 1/10:  mini-batch 1830/4459:  Train loss: 3.4715936183929443  Test loss: 3.3751049041748047 \n",
      "Epoch: 1/10:  mini-batch 1831/4459:  Train loss: 3.1721928119659424  Test loss: 3.374277353286743 \n",
      "Epoch: 1/10:  mini-batch 1832/4459:  Train loss: 3.5246622562408447  Test loss: 3.373784303665161 \n",
      "Epoch: 1/10:  mini-batch 1833/4459:  Train loss: 3.5605688095092773  Test loss: 3.3731462955474854 \n",
      "Epoch: 1/10:  mini-batch 1834/4459:  Train loss: 3.635711669921875  Test loss: 3.3724465370178223 \n",
      "Epoch: 1/10:  mini-batch 1835/4459:  Train loss: 3.2397513389587402  Test loss: 3.3721718788146973 \n",
      "Epoch: 1/10:  mini-batch 1836/4459:  Train loss: 3.5910422801971436  Test loss: 3.371929883956909 \n",
      "Epoch: 1/10:  mini-batch 1837/4459:  Train loss: 3.2771317958831787  Test loss: 3.3713386058807373 \n",
      "Epoch: 1/10:  mini-batch 1838/4459:  Train loss: 3.3550000190734863  Test loss: 3.371121883392334 \n",
      "Epoch: 1/10:  mini-batch 1839/4459:  Train loss: 3.123504877090454  Test loss: 3.3705451488494873 \n",
      "Epoch: 1/10:  mini-batch 1840/4459:  Train loss: 3.3715507984161377  Test loss: 3.3703227043151855 \n",
      "Epoch: 1/10:  mini-batch 1841/4459:  Train loss: 3.4655728340148926  Test loss: 3.369771957397461 \n",
      "Epoch: 1/10:  mini-batch 1842/4459:  Train loss: 3.220667839050293  Test loss: 3.3694941997528076 \n",
      "Epoch: 1/10:  mini-batch 1843/4459:  Train loss: 3.3047566413879395  Test loss: 3.3691043853759766 \n",
      "Epoch: 1/10:  mini-batch 1844/4459:  Train loss: 3.5048890113830566  Test loss: 3.368520975112915 \n",
      "Epoch: 1/10:  mini-batch 1845/4459:  Train loss: 3.695847988128662  Test loss: 3.367978572845459 \n",
      "Epoch: 1/10:  mini-batch 1846/4459:  Train loss: 3.110928535461426  Test loss: 3.36783766746521 \n",
      "Epoch: 1/10:  mini-batch 1847/4459:  Train loss: 3.2697479724884033  Test loss: 3.367926597595215 \n",
      "Epoch: 1/10:  mini-batch 1848/4459:  Train loss: 3.1750235557556152  Test loss: 3.36796236038208 \n",
      "Epoch: 1/10:  mini-batch 1849/4459:  Train loss: 3.18527889251709  Test loss: 3.3677830696105957 \n",
      "Epoch: 1/10:  mini-batch 1850/4459:  Train loss: 3.145540952682495  Test loss: 3.3678784370422363 \n",
      "Epoch: 1/10:  mini-batch 1851/4459:  Train loss: 3.225062847137451  Test loss: 3.3684322834014893 \n",
      "Epoch: 1/10:  mini-batch 1852/4459:  Train loss: 3.463266134262085  Test loss: 3.368891716003418 \n",
      "Epoch: 1/10:  mini-batch 1853/4459:  Train loss: 3.3957526683807373  Test loss: 3.369745969772339 \n",
      "Epoch: 1/10:  mini-batch 1854/4459:  Train loss: 3.654106616973877  Test loss: 3.3704662322998047 \n",
      "Epoch: 1/10:  mini-batch 1855/4459:  Train loss: 3.6573781967163086  Test loss: 3.3714427947998047 \n",
      "Epoch: 1/10:  mini-batch 1856/4459:  Train loss: 3.5192484855651855  Test loss: 3.3720178604125977 \n",
      "Epoch: 1/10:  mini-batch 1857/4459:  Train loss: 3.3521006107330322  Test loss: 3.372494697570801 \n",
      "Epoch: 1/10:  mini-batch 1858/4459:  Train loss: 3.4759719371795654  Test loss: 3.3729166984558105 \n",
      "Epoch: 1/10:  mini-batch 1859/4459:  Train loss: 3.419157028198242  Test loss: 3.3735899925231934 \n",
      "Epoch: 1/10:  mini-batch 1860/4459:  Train loss: 3.4876112937927246  Test loss: 3.373856544494629 \n",
      "Epoch: 1/10:  mini-batch 1861/4459:  Train loss: 3.2774784564971924  Test loss: 3.374410629272461 \n",
      "Epoch: 1/10:  mini-batch 1862/4459:  Train loss: 3.291524648666382  Test loss: 3.375136375427246 \n",
      "Epoch: 1/10:  mini-batch 1863/4459:  Train loss: 3.347832679748535  Test loss: 3.3753223419189453 \n",
      "Epoch: 1/10:  mini-batch 1864/4459:  Train loss: 3.2824292182922363  Test loss: 3.375739574432373 \n",
      "Epoch: 1/10:  mini-batch 1865/4459:  Train loss: 3.0252060890197754  Test loss: 3.3758723735809326 \n",
      "Epoch: 1/10:  mini-batch 1866/4459:  Train loss: 3.1936211585998535  Test loss: 3.375814199447632 \n",
      "Epoch: 1/10:  mini-batch 1867/4459:  Train loss: 3.298938035964966  Test loss: 3.376065969467163 \n",
      "Epoch: 1/10:  mini-batch 1868/4459:  Train loss: 3.4526710510253906  Test loss: 3.375917911529541 \n",
      "Epoch: 1/10:  mini-batch 1869/4459:  Train loss: 3.372000217437744  Test loss: 3.3756275177001953 \n",
      "Epoch: 1/10:  mini-batch 1870/4459:  Train loss: 3.2689738273620605  Test loss: 3.375368356704712 \n",
      "Epoch: 1/10:  mini-batch 1871/4459:  Train loss: 3.1723923683166504  Test loss: 3.374875068664551 \n",
      "Epoch: 1/10:  mini-batch 1872/4459:  Train loss: 3.2394886016845703  Test loss: 3.3748621940612793 \n",
      "Epoch: 1/10:  mini-batch 1873/4459:  Train loss: 3.5247280597686768  Test loss: 3.3748397827148438 \n",
      "Epoch: 1/10:  mini-batch 1874/4459:  Train loss: 3.6497464179992676  Test loss: 3.3750243186950684 \n",
      "Epoch: 1/10:  mini-batch 1875/4459:  Train loss: 3.192347526550293  Test loss: 3.3749325275421143 \n",
      "Epoch: 1/10:  mini-batch 1876/4459:  Train loss: 3.2873997688293457  Test loss: 3.3746590614318848 \n",
      "Epoch: 1/10:  mini-batch 1877/4459:  Train loss: 3.6431870460510254  Test loss: 3.3742260932922363 \n",
      "Epoch: 1/10:  mini-batch 1878/4459:  Train loss: 3.9641709327697754  Test loss: 3.373917579650879 \n",
      "Epoch: 1/10:  mini-batch 1879/4459:  Train loss: 3.5099964141845703  Test loss: 3.373453140258789 \n",
      "Epoch: 1/10:  mini-batch 1880/4459:  Train loss: 3.3257148265838623  Test loss: 3.3732428550720215 \n",
      "Epoch: 1/10:  mini-batch 1881/4459:  Train loss: 3.37271785736084  Test loss: 3.373305320739746 \n",
      "Epoch: 1/10:  mini-batch 1882/4459:  Train loss: 3.2860219478607178  Test loss: 3.3732810020446777 \n",
      "Epoch: 1/10:  mini-batch 1883/4459:  Train loss: 3.1273162364959717  Test loss: 3.3729212284088135 \n",
      "Epoch: 1/10:  mini-batch 1884/4459:  Train loss: 3.3060302734375  Test loss: 3.3727715015411377 \n",
      "Epoch: 1/10:  mini-batch 1885/4459:  Train loss: 3.452720880508423  Test loss: 3.3729333877563477 \n",
      "Epoch: 1/10:  mini-batch 1886/4459:  Train loss: 3.410565137863159  Test loss: 3.373063087463379 \n",
      "Epoch: 1/10:  mini-batch 1887/4459:  Train loss: 3.404322624206543  Test loss: 3.3732378482818604 \n",
      "Epoch: 1/10:  mini-batch 1888/4459:  Train loss: 3.367741107940674  Test loss: 3.3731729984283447 \n",
      "Epoch: 1/10:  mini-batch 1889/4459:  Train loss: 3.613213300704956  Test loss: 3.373363494873047 \n",
      "Epoch: 1/10:  mini-batch 1890/4459:  Train loss: 3.3823657035827637  Test loss: 3.3743443489074707 \n",
      "Epoch: 1/10:  mini-batch 1891/4459:  Train loss: 3.2541098594665527  Test loss: 3.3751978874206543 \n",
      "Epoch: 1/10:  mini-batch 1892/4459:  Train loss: 3.3249974250793457  Test loss: 3.3758962154388428 \n",
      "Epoch: 1/10:  mini-batch 1893/4459:  Train loss: 3.4403069019317627  Test loss: 3.3766565322875977 \n",
      "Epoch: 1/10:  mini-batch 1894/4459:  Train loss: 3.4336066246032715  Test loss: 3.377455711364746 \n",
      "Epoch: 1/10:  mini-batch 1895/4459:  Train loss: 3.640340805053711  Test loss: 3.3781609535217285 \n",
      "Epoch: 1/10:  mini-batch 1896/4459:  Train loss: 3.379298448562622  Test loss: 3.378854274749756 \n",
      "Epoch: 1/10:  mini-batch 1897/4459:  Train loss: 3.8488125801086426  Test loss: 3.3798270225524902 \n",
      "Epoch: 1/10:  mini-batch 1898/4459:  Train loss: 3.8448617458343506  Test loss: 3.3812248706817627 \n",
      "Epoch: 1/10:  mini-batch 1899/4459:  Train loss: 3.4537205696105957  Test loss: 3.3827102184295654 \n",
      "Epoch: 1/10:  mini-batch 1900/4459:  Train loss: 3.3896594047546387  Test loss: 3.384183645248413 \n",
      "Epoch: 1/10:  mini-batch 1901/4459:  Train loss: 3.2971510887145996  Test loss: 3.385371208190918 \n",
      "Epoch: 1/10:  mini-batch 1902/4459:  Train loss: 3.330785036087036  Test loss: 3.3864073753356934 \n",
      "Epoch: 1/10:  mini-batch 1903/4459:  Train loss: 3.4472172260284424  Test loss: 3.387683868408203 \n",
      "Epoch: 1/10:  mini-batch 1904/4459:  Train loss: 3.455801248550415  Test loss: 3.3891427516937256 \n",
      "Epoch: 1/10:  mini-batch 1905/4459:  Train loss: 3.5237600803375244  Test loss: 3.3905563354492188 \n",
      "Epoch: 1/10:  mini-batch 1906/4459:  Train loss: 3.3536128997802734  Test loss: 3.3917741775512695 \n",
      "Epoch: 1/10:  mini-batch 1907/4459:  Train loss: 3.180062770843506  Test loss: 3.3925061225891113 \n",
      "Epoch: 1/10:  mini-batch 1908/4459:  Train loss: 3.09450364112854  Test loss: 3.3927412033081055 \n",
      "Epoch: 1/10:  mini-batch 1909/4459:  Train loss: 3.437756061553955  Test loss: 3.393171787261963 \n",
      "Epoch: 1/10:  mini-batch 1910/4459:  Train loss: 3.668328285217285  Test loss: 3.394071102142334 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 1911/4459:  Train loss: 3.3923821449279785  Test loss: 3.3951988220214844 \n",
      "Epoch: 1/10:  mini-batch 1912/4459:  Train loss: 3.7465147972106934  Test loss: 3.396655797958374 \n",
      "Epoch: 1/10:  mini-batch 1913/4459:  Train loss: 3.43310284614563  Test loss: 3.3981075286865234 \n",
      "Epoch: 1/10:  mini-batch 1914/4459:  Train loss: 3.402585506439209  Test loss: 3.399509906768799 \n",
      "Epoch: 1/10:  mini-batch 1915/4459:  Train loss: 3.2366151809692383  Test loss: 3.400686502456665 \n",
      "Epoch: 1/10:  mini-batch 1916/4459:  Train loss: 3.3559248447418213  Test loss: 3.401594638824463 \n",
      "Epoch: 1/10:  mini-batch 1917/4459:  Train loss: 3.345242977142334  Test loss: 3.402235507965088 \n",
      "Epoch: 1/10:  mini-batch 1918/4459:  Train loss: 3.6574478149414062  Test loss: 3.402846574783325 \n",
      "Epoch: 1/10:  mini-batch 1919/4459:  Train loss: 3.3125627040863037  Test loss: 3.4033303260803223 \n",
      "Epoch: 1/10:  mini-batch 1920/4459:  Train loss: 3.2307639122009277  Test loss: 3.403655767440796 \n",
      "Epoch: 1/10:  mini-batch 1921/4459:  Train loss: 3.147554874420166  Test loss: 3.403716564178467 \n",
      "Epoch: 1/10:  mini-batch 1922/4459:  Train loss: 3.1833608150482178  Test loss: 3.4036176204681396 \n",
      "Epoch: 1/10:  mini-batch 1923/4459:  Train loss: 3.2548770904541016  Test loss: 3.4033827781677246 \n",
      "Epoch: 1/10:  mini-batch 1924/4459:  Train loss: 3.362379312515259  Test loss: 3.4030892848968506 \n",
      "Epoch: 1/10:  mini-batch 1925/4459:  Train loss: 3.185201644897461  Test loss: 3.402609348297119 \n",
      "Epoch: 1/10:  mini-batch 1926/4459:  Train loss: 3.517923355102539  Test loss: 3.402510643005371 \n",
      "Epoch: 1/10:  mini-batch 1927/4459:  Train loss: 3.663581371307373  Test loss: 3.40252423286438 \n",
      "Epoch: 1/10:  mini-batch 1928/4459:  Train loss: 3.3836171627044678  Test loss: 3.402318000793457 \n",
      "Epoch: 1/10:  mini-batch 1953/4459:  Train loss: 3.0961403846740723  Test loss: 3.390139102935791 \n",
      "Epoch: 1/10:  mini-batch 1954/4459:  Train loss: 3.07374906539917  Test loss: 3.389741897583008 \n",
      "Epoch: 1/10:  mini-batch 1955/4459:  Train loss: 3.1069040298461914  Test loss: 3.389000415802002 \n",
      "Epoch: 1/10:  mini-batch 1956/4459:  Train loss: 3.2652997970581055  Test loss: 3.38893985748291 \n",
      "Epoch: 1/10:  mini-batch 1957/4459:  Train loss: 3.516921043395996  Test loss: 3.3882291316986084 \n",
      "Epoch: 1/10:  mini-batch 1958/4459:  Train loss: 3.3305187225341797  Test loss: 3.3873133659362793 \n",
      "Epoch: 1/10:  mini-batch 1959/4459:  Train loss: 3.6521799564361572  Test loss: 3.3863871097564697 \n",
      "Epoch: 1/10:  mini-batch 1960/4459:  Train loss: 3.5610623359680176  Test loss: 3.38580060005188 \n",
      "Epoch: 1/10:  mini-batch 1961/4459:  Train loss: 3.4682681560516357  Test loss: 3.38590669631958 \n",
      "Epoch: 1/10:  mini-batch 1962/4459:  Train loss: 3.1850109100341797  Test loss: 3.3863325119018555 \n",
      "Epoch: 1/10:  mini-batch 1963/4459:  Train loss: 3.379885196685791  Test loss: 3.386842727661133 \n",
      "Epoch: 1/10:  mini-batch 1964/4459:  Train loss: 3.0886306762695312  Test loss: 3.388228178024292 \n",
      "Epoch: 1/10:  mini-batch 1965/4459:  Train loss: 3.196303367614746  Test loss: 3.390293598175049 \n",
      "Epoch: 1/10:  mini-batch 1966/4459:  Train loss: 3.1918210983276367  Test loss: 3.39239501953125 \n",
      "Epoch: 1/10:  mini-batch 1967/4459:  Train loss: 3.45046329498291  Test loss: 3.394282817840576 \n",
      "Epoch: 1/10:  mini-batch 1968/4459:  Train loss: 3.774066209793091  Test loss: 3.3957407474517822 \n",
      "Epoch: 1/10:  mini-batch 1969/4459:  Train loss: 3.178574562072754  Test loss: 3.397510528564453 \n",
      "Epoch: 1/10:  mini-batch 1970/4459:  Train loss: 3.2677643299102783  Test loss: 3.3990299701690674 \n",
      "Epoch: 1/10:  mini-batch 1971/4459:  Train loss: 3.6462149620056152  Test loss: 3.400404214859009 \n",
      "Epoch: 1/10:  mini-batch 1972/4459:  Train loss: 3.3733296394348145  Test loss: 3.4014809131622314 \n",
      "Epoch: 1/10:  mini-batch 1973/4459:  Train loss: 3.3035314083099365  Test loss: 3.402174711227417 \n",
      "Epoch: 1/10:  mini-batch 1974/4459:  Train loss: 3.4356465339660645  Test loss: 3.40270733833313 \n",
      "Epoch: 1/10:  mini-batch 1975/4459:  Train loss: 3.426705837249756  Test loss: 3.4031920433044434 \n",
      "Epoch: 1/10:  mini-batch 1976/4459:  Train loss: 3.419701337814331  Test loss: 3.4036478996276855 \n",
      "Epoch: 1/10:  mini-batch 1977/4459:  Train loss: 3.5061440467834473  Test loss: 3.403794288635254 \n",
      "Epoch: 1/10:  mini-batch 1978/4459:  Train loss: 3.1589293479919434  Test loss: 3.4041244983673096 \n",
      "Epoch: 1/10:  mini-batch 1979/4459:  Train loss: 3.145503044128418  Test loss: 3.4049410820007324 \n",
      "Epoch: 1/10:  mini-batch 1980/4459:  Train loss: 3.4441370964050293  Test loss: 3.405757427215576 \n",
      "Epoch: 1/10:  mini-batch 1981/4459:  Train loss: 3.0835609436035156  Test loss: 3.4071764945983887 \n",
      "Epoch: 1/10:  mini-batch 1982/4459:  Train loss: 3.635047674179077  Test loss: 3.40830659866333 \n",
      "Epoch: 1/10:  mini-batch 1983/4459:  Train loss: 3.6433799266815186  Test loss: 3.409668207168579 \n",
      "Epoch: 1/10:  mini-batch 1984/4459:  Train loss: 3.3055200576782227  Test loss: 3.410945415496826 \n",
      "Epoch: 1/10:  mini-batch 1985/4459:  Train loss: 3.1587371826171875  Test loss: 3.412468433380127 \n",
      "Epoch: 1/10:  mini-batch 1986/4459:  Train loss: 3.2485430240631104  Test loss: 3.414447546005249 \n",
      "Epoch: 1/10:  mini-batch 1987/4459:  Train loss: 3.521244525909424  Test loss: 3.4163103103637695 \n",
      "Epoch: 1/10:  mini-batch 1988/4459:  Train loss: 3.290478229522705  Test loss: 3.418083429336548 \n",
      "Epoch: 1/10:  mini-batch 1989/4459:  Train loss: 3.3436803817749023  Test loss: 3.4195029735565186 \n",
      "Epoch: 1/10:  mini-batch 1990/4459:  Train loss: 3.2320308685302734  Test loss: 3.4201853275299072 \n",
      "Epoch: 1/10:  mini-batch 1991/4459:  Train loss: 3.210296630859375  Test loss: 3.420874834060669 \n",
      "Epoch: 1/10:  mini-batch 1992/4459:  Train loss: 3.314276695251465  Test loss: 3.421095371246338 \n",
      "Epoch: 1/10:  mini-batch 1993/4459:  Train loss: 3.1944384574890137  Test loss: 3.4209513664245605 \n",
      "Epoch: 1/10:  mini-batch 1994/4459:  Train loss: 3.123446226119995  Test loss: 3.4207687377929688 \n",
      "Epoch: 1/10:  mini-batch 1995/4459:  Train loss: 3.3683178424835205  Test loss: 3.4205873012542725 \n",
      "Epoch: 1/10:  mini-batch 1996/4459:  Train loss: 3.220177173614502  Test loss: 3.420971155166626 \n",
      "Epoch: 1/10:  mini-batch 1997/4459:  Train loss: 3.1812984943389893  Test loss: 3.421790599822998 \n",
      "Epoch: 1/10:  mini-batch 1998/4459:  Train loss: 3.5260353088378906  Test loss: 3.422189712524414 \n",
      "Epoch: 1/10:  mini-batch 1999/4459:  Train loss: 3.4958696365356445  Test loss: 3.4225592613220215 \n",
      "Epoch: 1/10:  mini-batch 2000/4459:  Train loss: 3.7075839042663574  Test loss: 3.4228618144989014 \n",
      "Epoch: 1/10:  mini-batch 2001/4459:  Train loss: 3.2364919185638428  Test loss: 3.4234964847564697 \n",
      "Epoch: 1/10:  mini-batch 2002/4459:  Train loss: 3.524580240249634  Test loss: 3.4242467880249023 \n",
      "Epoch: 1/10:  mini-batch 2003/4459:  Train loss: 3.4724464416503906  Test loss: 3.4245405197143555 \n",
      "Epoch: 1/10:  mini-batch 2004/4459:  Train loss: 3.462794780731201  Test loss: 3.4248886108398438 \n",
      "Epoch: 1/10:  mini-batch 2005/4459:  Train loss: 3.3544723987579346  Test loss: 3.425185203552246 \n",
      "Epoch: 1/10:  mini-batch 2006/4459:  Train loss: 3.311995506286621  Test loss: 3.425269603729248 \n",
      "Epoch: 1/10:  mini-batch 2007/4459:  Train loss: 3.623969554901123  Test loss: 3.425650119781494 \n",
      "Epoch: 1/10:  mini-batch 2008/4459:  Train loss: 3.1876144409179688  Test loss: 3.426509380340576 \n",
      "Epoch: 1/10:  mini-batch 2009/4459:  Train loss: 3.1011829376220703  Test loss: 3.4276885986328125 \n",
      "Epoch: 1/10:  mini-batch 2010/4459:  Train loss: 3.250387668609619  Test loss: 3.428595781326294 \n",
      "Epoch: 1/10:  mini-batch 2011/4459:  Train loss: 3.2786033153533936  Test loss: 3.429760456085205 \n",
      "Epoch: 1/10:  mini-batch 2012/4459:  Train loss: 3.135545015335083  Test loss: 3.4312093257904053 \n",
      "Epoch: 1/10:  mini-batch 2013/4459:  Train loss: 3.009084701538086  Test loss: 3.43302321434021 \n",
      "Epoch: 1/10:  mini-batch 2014/4459:  Train loss: 3.1540780067443848  Test loss: 3.435002565383911 \n",
      "Epoch: 1/10:  mini-batch 2015/4459:  Train loss: 3.2065975666046143  Test loss: 3.437103748321533 \n",
      "Epoch: 1/10:  mini-batch 2016/4459:  Train loss: 3.2789998054504395  Test loss: 3.438991069793701 \n",
      "Epoch: 1/10:  mini-batch 2017/4459:  Train loss: 3.4290904998779297  Test loss: 3.440706253051758 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2018/4459:  Train loss: 3.4803826808929443  Test loss: 3.442293405532837 \n",
      "Epoch: 1/10:  mini-batch 2019/4459:  Train loss: 3.32086181640625  Test loss: 3.4439539909362793 \n",
      "Epoch: 1/10:  mini-batch 2020/4459:  Train loss: 3.083660125732422  Test loss: 3.445883274078369 \n",
      "Epoch: 1/10:  mini-batch 2021/4459:  Train loss: 3.0303850173950195  Test loss: 3.4484944343566895 \n",
      "Epoch: 1/10:  mini-batch 2022/4459:  Train loss: 3.3548388481140137  Test loss: 3.450599193572998 \n",
      "Epoch: 1/10:  mini-batch 2023/4459:  Train loss: 3.415175199508667  Test loss: 3.45198130607605 \n",
      "Epoch: 1/10:  mini-batch 2024/4459:  Train loss: 3.5673274993896484  Test loss: 3.452418088912964 \n",
      "Epoch: 1/10:  mini-batch 2025/4459:  Train loss: 3.283029794692993  Test loss: 3.4531898498535156 \n",
      "Epoch: 1/10:  mini-batch 2026/4459:  Train loss: 3.8327200412750244  Test loss: 3.452193021774292 \n",
      "Epoch: 1/10:  mini-batch 2027/4459:  Train loss: 3.4885520935058594  Test loss: 3.450660228729248 \n",
      "Epoch: 1/10:  mini-batch 2028/4459:  Train loss: 3.3455235958099365  Test loss: 3.4496331214904785 \n",
      "Epoch: 1/10:  mini-batch 2029/4459:  Train loss: 3.4312756061553955  Test loss: 3.4486875534057617 \n",
      "Epoch: 1/10:  mini-batch 2030/4459:  Train loss: 3.186054229736328  Test loss: 3.4480624198913574 \n",
      "Epoch: 1/10:  mini-batch 2031/4459:  Train loss: 3.3267009258270264  Test loss: 3.446587085723877 \n",
      "Epoch: 1/10:  mini-batch 2032/4459:  Train loss: 3.4455089569091797  Test loss: 3.444037437438965 \n",
      "Epoch: 1/10:  mini-batch 2033/4459:  Train loss: 3.373816967010498  Test loss: 3.441342353820801 \n",
      "Epoch: 1/10:  mini-batch 2034/4459:  Train loss: 3.5463008880615234  Test loss: 3.438508987426758 \n",
      "Epoch: 1/10:  mini-batch 2035/4459:  Train loss: 3.0320146083831787  Test loss: 3.436516284942627 \n",
      "Epoch: 1/10:  mini-batch 2036/4459:  Train loss: 3.368299961090088  Test loss: 3.434598922729492 \n",
      "Epoch: 1/10:  mini-batch 2037/4459:  Train loss: 3.7112464904785156  Test loss: 3.432288408279419 \n",
      "Epoch: 1/10:  mini-batch 2038/4459:  Train loss: 3.575174331665039  Test loss: 3.43031644821167 \n",
      "Epoch: 1/10:  mini-batch 2039/4459:  Train loss: 3.017401695251465  Test loss: 3.4293203353881836 \n",
      "Epoch: 1/10:  mini-batch 2040/4459:  Train loss: 3.2894232273101807  Test loss: 3.4286792278289795 \n",
      "Epoch: 1/10:  mini-batch 2041/4459:  Train loss: 2.938201427459717  Test loss: 3.4286482334136963 \n",
      "Epoch: 1/10:  mini-batch 2042/4459:  Train loss: 3.0706663131713867  Test loss: 3.4288532733917236 \n",
      "Epoch: 1/10:  mini-batch 2043/4459:  Train loss: 3.174826145172119  Test loss: 3.4290428161621094 \n",
      "Epoch: 1/10:  mini-batch 2044/4459:  Train loss: 3.226134777069092  Test loss: 3.4289348125457764 \n",
      "Epoch: 1/10:  mini-batch 2045/4459:  Train loss: 3.1925392150878906  Test loss: 3.428964614868164 \n",
      "Epoch: 1/10:  mini-batch 2046/4459:  Train loss: 3.4903275966644287  Test loss: 3.429255962371826 \n",
      "Epoch: 1/10:  mini-batch 2047/4459:  Train loss: 3.1717662811279297  Test loss: 3.429596185684204 \n",
      "Epoch: 1/10:  mini-batch 2048/4459:  Train loss: 3.0894219875335693  Test loss: 3.4301934242248535 \n",
      "Epoch: 1/10:  mini-batch 2049/4459:  Train loss: 3.6323459148406982  Test loss: 3.4305295944213867 \n",
      "Epoch: 1/10:  mini-batch 2050/4459:  Train loss: 3.0133044719696045  Test loss: 3.4314920902252197 \n",
      "Epoch: 1/10:  mini-batch 2051/4459:  Train loss: 3.110107183456421  Test loss: 3.432583808898926 \n",
      "Epoch: 1/10:  mini-batch 2052/4459:  Train loss: 3.0402626991271973  Test loss: 3.4339699745178223 \n",
      "Epoch: 1/10:  mini-batch 2053/4459:  Train loss: 3.082643508911133  Test loss: 3.43576717376709 \n",
      "Epoch: 1/10:  mini-batch 2054/4459:  Train loss: 3.196096897125244  Test loss: 3.4373016357421875 \n",
      "Epoch: 1/10:  mini-batch 2055/4459:  Train loss: 3.338325023651123  Test loss: 3.438964366912842 \n",
      "Epoch: 1/10:  mini-batch 2056/4459:  Train loss: 3.184053897857666  Test loss: 3.44094181060791 \n",
      "Epoch: 1/10:  mini-batch 2057/4459:  Train loss: 3.748476266860962  Test loss: 3.440577983856201 \n",
      "Epoch: 1/10:  mini-batch 2058/4459:  Train loss: 3.440854072570801  Test loss: 3.440469980239868 \n",
      "Epoch: 1/10:  mini-batch 2059/4459:  Train loss: 3.388644218444824  Test loss: 3.4395222663879395 \n",
      "Epoch: 1/10:  mini-batch 2060/4459:  Train loss: 3.140855312347412  Test loss: 3.439154624938965 \n",
      "Epoch: 1/10:  mini-batch 2061/4459:  Train loss: 3.345508098602295  Test loss: 3.4388084411621094 \n",
      "Epoch: 1/10:  mini-batch 2062/4459:  Train loss: 3.543759346008301  Test loss: 3.4378607273101807 \n",
      "Epoch: 1/10:  mini-batch 2063/4459:  Train loss: 3.2641143798828125  Test loss: 3.436410665512085 \n",
      "Epoch: 1/10:  mini-batch 2064/4459:  Train loss: 3.6520445346832275  Test loss: 3.434722423553467 \n",
      "Epoch: 1/10:  mini-batch 2065/4459:  Train loss: 3.481383800506592  Test loss: 3.4330520629882812 \n",
      "Epoch: 1/10:  mini-batch 2066/4459:  Train loss: 3.21592378616333  Test loss: 3.4315574169158936 \n",
      "Epoch: 1/10:  mini-batch 2067/4459:  Train loss: 2.950057029724121  Test loss: 3.4308736324310303 \n",
      "Epoch: 1/10:  mini-batch 2068/4459:  Train loss: 3.3481149673461914  Test loss: 3.4298813343048096 \n",
      "Epoch: 1/10:  mini-batch 2069/4459:  Train loss: 3.0285744667053223  Test loss: 3.4289393424987793 \n",
      "Epoch: 1/10:  mini-batch 2070/4459:  Train loss: 3.4548122882843018  Test loss: 3.427889108657837 \n",
      "Epoch: 1/10:  mini-batch 2071/4459:  Train loss: 3.305352210998535  Test loss: 3.4262990951538086 \n",
      "Epoch: 1/10:  mini-batch 2072/4459:  Train loss: 3.4115004539489746  Test loss: 3.423832416534424 \n",
      "Epoch: 1/10:  mini-batch 2073/4459:  Train loss: 3.535463809967041  Test loss: 3.4212303161621094 \n",
      "Epoch: 1/10:  mini-batch 2074/4459:  Train loss: 3.1267058849334717  Test loss: 3.419414758682251 \n",
      "Epoch: 1/10:  mini-batch 2075/4459:  Train loss: 3.417187213897705  Test loss: 3.417564868927002 \n",
      "Epoch: 1/10:  mini-batch 2076/4459:  Train loss: 3.763838291168213  Test loss: 3.4150514602661133 \n",
      "Epoch: 1/10:  mini-batch 2077/4459:  Train loss: 3.400857925415039  Test loss: 3.4127230644226074 \n",
      "Epoch: 1/10:  mini-batch 2078/4459:  Train loss: 3.6935501098632812  Test loss: 3.410212993621826 \n",
      "Epoch: 1/10:  mini-batch 2079/4459:  Train loss: 3.354745388031006  Test loss: 3.408383846282959 \n",
      "Epoch: 1/10:  mini-batch 2080/4459:  Train loss: 3.5209031105041504  Test loss: 3.4069597721099854 \n",
      "Epoch: 1/10:  mini-batch 2081/4459:  Train loss: 3.174703359603882  Test loss: 3.406266927719116 \n",
      "Epoch: 1/10:  mini-batch 2082/4459:  Train loss: 3.538761615753174  Test loss: 3.4056644439697266 \n",
      "Epoch: 1/10:  mini-batch 2083/4459:  Train loss: 3.3179030418395996  Test loss: 3.4053940773010254 \n",
      "Epoch: 1/10:  mini-batch 2084/4459:  Train loss: 3.2041146755218506  Test loss: 3.4049885272979736 \n",
      "Epoch: 1/10:  mini-batch 2085/4459:  Train loss: 3.3656094074249268  Test loss: 3.404445171356201 \n",
      "Epoch: 1/10:  mini-batch 2086/4459:  Train loss: 3.2791950702667236  Test loss: 3.403985023498535 \n",
      "Epoch: 1/10:  mini-batch 2087/4459:  Train loss: 3.2949724197387695  Test loss: 3.403670310974121 \n",
      "Epoch: 1/10:  mini-batch 2088/4459:  Train loss: 3.1312360763549805  Test loss: 3.4032092094421387 \n",
      "Epoch: 1/10:  mini-batch 2089/4459:  Train loss: 3.4715681076049805  Test loss: 3.402820348739624 \n",
      "Epoch: 1/10:  mini-batch 2090/4459:  Train loss: 3.359053373336792  Test loss: 3.40220308303833 \n",
      "Epoch: 1/10:  mini-batch 2091/4459:  Train loss: 3.1705732345581055  Test loss: 3.4011940956115723 \n",
      "Epoch: 1/10:  mini-batch 2092/4459:  Train loss: 3.700847864151001  Test loss: 3.400379180908203 \n",
      "Epoch: 1/10:  mini-batch 2093/4459:  Train loss: 3.1711676120758057  Test loss: 3.4001593589782715 \n",
      "Epoch: 1/10:  mini-batch 2094/4459:  Train loss: 3.1617350578308105  Test loss: 3.400394916534424 \n",
      "Epoch: 1/10:  mini-batch 2095/4459:  Train loss: 3.235694169998169  Test loss: 3.4008302688598633 \n",
      "Epoch: 1/10:  mini-batch 2096/4459:  Train loss: 3.536782741546631  Test loss: 3.401165246963501 \n",
      "Epoch: 1/10:  mini-batch 2097/4459:  Train loss: 3.4941515922546387  Test loss: 3.401444673538208 \n",
      "Epoch: 1/10:  mini-batch 2098/4459:  Train loss: 3.3436388969421387  Test loss: 3.4013335704803467 \n",
      "Epoch: 1/10:  mini-batch 2099/4459:  Train loss: 3.553273916244507  Test loss: 3.4009737968444824 \n",
      "Epoch: 1/10:  mini-batch 2100/4459:  Train loss: 3.3391835689544678  Test loss: 3.4008255004882812 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2101/4459:  Train loss: 3.1581850051879883  Test loss: 3.400658130645752 \n",
      "Epoch: 1/10:  mini-batch 2102/4459:  Train loss: 3.5640106201171875  Test loss: 3.4004287719726562 \n",
      "Epoch: 1/10:  mini-batch 2103/4459:  Train loss: 3.3798418045043945  Test loss: 3.4001622200012207 \n",
      "Epoch: 1/10:  mini-batch 2104/4459:  Train loss: 3.2960898876190186  Test loss: 3.4000134468078613 \n",
      "Epoch: 1/10:  mini-batch 2105/4459:  Train loss: 3.388918399810791  Test loss: 3.3999788761138916 \n",
      "Epoch: 1/10:  mini-batch 2106/4459:  Train loss: 3.400017738342285  Test loss: 3.3999900817871094 \n",
      "Epoch: 1/10:  mini-batch 2107/4459:  Train loss: 3.4448134899139404  Test loss: 3.3999552726745605 \n",
      "Epoch: 1/10:  mini-batch 2108/4459:  Train loss: 3.526395797729492  Test loss: 3.4004909992218018 \n",
      "Epoch: 1/10:  mini-batch 2109/4459:  Train loss: 3.547996997833252  Test loss: 3.4013547897338867 \n",
      "Epoch: 1/10:  mini-batch 2110/4459:  Train loss: 3.1261510848999023  Test loss: 3.4023566246032715 \n",
      "Epoch: 1/10:  mini-batch 2111/4459:  Train loss: 3.008957862854004  Test loss: 3.4035251140594482 \n",
      "Epoch: 1/10:  mini-batch 2112/4459:  Train loss: 3.5686159133911133  Test loss: 3.404550313949585 \n",
      "Epoch: 1/10:  mini-batch 2113/4459:  Train loss: 3.3399691581726074  Test loss: 3.406123399734497 \n",
      "Epoch: 1/10:  mini-batch 2114/4459:  Train loss: 3.265953779220581  Test loss: 3.4072980880737305 \n",
      "Epoch: 1/10:  mini-batch 2115/4459:  Train loss: 3.2277212142944336  Test loss: 3.4082517623901367 \n",
      "Epoch: 1/10:  mini-batch 2116/4459:  Train loss: 3.3412933349609375  Test loss: 3.4089322090148926 \n",
      "Epoch: 1/10:  mini-batch 2117/4459:  Train loss: 3.494558572769165  Test loss: 3.4094953536987305 \n",
      "Epoch: 1/10:  mini-batch 2118/4459:  Train loss: 3.075671672821045  Test loss: 3.4098691940307617 \n",
      "Epoch: 1/10:  mini-batch 2119/4459:  Train loss: 3.249847412109375  Test loss: 3.409817695617676 \n",
      "Epoch: 1/10:  mini-batch 2120/4459:  Train loss: 3.4121885299682617  Test loss: 3.408656358718872 \n",
      "Epoch: 1/10:  mini-batch 2121/4459:  Train loss: 3.307358503341675  Test loss: 3.407346725463867 \n",
      "Epoch: 1/10:  mini-batch 2122/4459:  Train loss: 4.059174060821533  Test loss: 3.4057624340057373 \n",
      "Epoch: 1/10:  mini-batch 2123/4459:  Train loss: 3.1787877082824707  Test loss: 3.4041309356689453 \n",
      "Epoch: 1/10:  mini-batch 2124/4459:  Train loss: 3.069694757461548  Test loss: 3.4027597904205322 \n",
      "Epoch: 1/10:  mini-batch 2125/4459:  Train loss: 3.2146406173706055  Test loss: 3.4017128944396973 \n",
      "Epoch: 1/10:  mini-batch 2126/4459:  Train loss: 3.3948144912719727  Test loss: 3.4007387161254883 \n",
      "Epoch: 1/10:  mini-batch 2127/4459:  Train loss: 3.875849723815918  Test loss: 3.399190664291382 \n",
      "Epoch: 1/10:  mini-batch 2128/4459:  Train loss: 2.956467390060425  Test loss: 3.3978474140167236 \n",
      "Epoch: 1/10:  mini-batch 2129/4459:  Train loss: 3.4869470596313477  Test loss: 3.396543025970459 \n",
      "Epoch: 1/10:  mini-batch 2130/4459:  Train loss: 3.321582555770874  Test loss: 3.3951385021209717 \n",
      "Epoch: 1/10:  mini-batch 2131/4459:  Train loss: 3.561420202255249  Test loss: 3.3934144973754883 \n",
      "Epoch: 1/10:  mini-batch 2132/4459:  Train loss: 3.2423806190490723  Test loss: 3.391411304473877 \n",
      "Epoch: 1/10:  mini-batch 2133/4459:  Train loss: 3.283053398132324  Test loss: 3.389739513397217 \n",
      "Epoch: 1/10:  mini-batch 2134/4459:  Train loss: 3.123931646347046  Test loss: 3.3890421390533447 \n",
      "Epoch: 1/10:  mini-batch 2135/4459:  Train loss: 3.1120409965515137  Test loss: 3.3880839347839355 \n",
      "Epoch: 1/10:  mini-batch 2136/4459:  Train loss: 3.002194404602051  Test loss: 3.3872008323669434 \n",
      "Epoch: 1/10:  mini-batch 2137/4459:  Train loss: 2.9586493968963623  Test loss: 3.386882781982422 \n",
      "Epoch: 1/10:  mini-batch 2138/4459:  Train loss: 3.253201961517334  Test loss: 3.386521577835083 \n",
      "Epoch: 1/10:  mini-batch 2139/4459:  Train loss: 3.3785228729248047  Test loss: 3.385897159576416 \n",
      "Epoch: 1/10:  mini-batch 2140/4459:  Train loss: 3.483558416366577  Test loss: 3.3850436210632324 \n",
      "Epoch: 1/10:  mini-batch 2141/4459:  Train loss: 3.2482755184173584  Test loss: 3.383570909500122 \n",
      "Epoch: 1/10:  mini-batch 2142/4459:  Train loss: 3.3619744777679443  Test loss: 3.382502555847168 \n",
      "Epoch: 1/10:  mini-batch 2143/4459:  Train loss: 3.6729795932769775  Test loss: 3.3812971115112305 \n",
      "Epoch: 1/10:  mini-batch 2144/4459:  Train loss: 3.375682830810547  Test loss: 3.3804378509521484 \n",
      "Epoch: 1/10:  mini-batch 2145/4459:  Train loss: 3.3884024620056152  Test loss: 3.379669189453125 \n",
      "Epoch: 1/10:  mini-batch 2146/4459:  Train loss: 3.5704262256622314  Test loss: 3.378469944000244 \n",
      "Epoch: 1/10:  mini-batch 2147/4459:  Train loss: 3.323965549468994  Test loss: 3.3772153854370117 \n",
      "Epoch: 1/10:  mini-batch 2148/4459:  Train loss: 3.236811637878418  Test loss: 3.3758416175842285 \n",
      "Epoch: 1/10:  mini-batch 2149/4459:  Train loss: 3.1523046493530273  Test loss: 3.3750431537628174 \n",
      "Epoch: 1/10:  mini-batch 2150/4459:  Train loss: 3.201681613922119  Test loss: 3.373959541320801 \n",
      "Epoch: 1/10:  mini-batch 2151/4459:  Train loss: 3.3692893981933594  Test loss: 3.373011827468872 \n",
      "Epoch: 1/10:  mini-batch 2152/4459:  Train loss: 3.1867496967315674  Test loss: 3.3721816539764404 \n",
      "Epoch: 1/10:  mini-batch 2153/4459:  Train loss: 3.3542068004608154  Test loss: 3.371121883392334 \n",
      "Epoch: 1/10:  mini-batch 2154/4459:  Train loss: 3.5711774826049805  Test loss: 3.370197296142578 \n",
      "Epoch: 1/10:  mini-batch 2155/4459:  Train loss: 3.4738197326660156  Test loss: 3.3691787719726562 \n",
      "Epoch: 1/10:  mini-batch 2156/4459:  Train loss: 3.471071481704712  Test loss: 3.3681929111480713 \n",
      "Epoch: 1/10:  mini-batch 2157/4459:  Train loss: 3.1682825088500977  Test loss: 3.367689609527588 \n",
      "Epoch: 1/10:  mini-batch 2158/4459:  Train loss: 3.2289044857025146  Test loss: 3.367119550704956 \n",
      "Epoch: 1/10:  mini-batch 2187/4459:  Train loss: 3.358095407485962  Test loss: 3.3581888675689697 \n",
      "Epoch: 1/10:  mini-batch 2188/4459:  Train loss: 3.2857375144958496  Test loss: 3.358598470687866 \n",
      "Epoch: 1/10:  mini-batch 2189/4459:  Train loss: 3.358753204345703  Test loss: 3.3585076332092285 \n",
      "Epoch: 1/10:  mini-batch 2190/4459:  Train loss: 3.535149097442627  Test loss: 3.3584625720977783 \n",
      "Epoch: 1/10:  mini-batch 2191/4459:  Train loss: 3.5469298362731934  Test loss: 3.3587095737457275 \n",
      "Epoch: 1/10:  mini-batch 2192/4459:  Train loss: 3.5007336139678955  Test loss: 3.3591737747192383 \n",
      "Epoch: 1/10:  mini-batch 2193/4459:  Train loss: 3.5112152099609375  Test loss: 3.3597493171691895 \n",
      "Epoch: 1/10:  mini-batch 2194/4459:  Train loss: 3.432781219482422  Test loss: 3.360626220703125 \n",
      "Epoch: 1/10:  mini-batch 2195/4459:  Train loss: 3.385427951812744  Test loss: 3.3615012168884277 \n",
      "Epoch: 1/10:  mini-batch 2196/4459:  Train loss: 3.398200750350952  Test loss: 3.362584114074707 \n",
      "Epoch: 1/10:  mini-batch 2197/4459:  Train loss: 3.224001884460449  Test loss: 3.3636069297790527 \n",
      "Epoch: 1/10:  mini-batch 2198/4459:  Train loss: 3.4179470539093018  Test loss: 3.3648428916931152 \n",
      "Epoch: 1/10:  mini-batch 2199/4459:  Train loss: 3.4250240325927734  Test loss: 3.3657305240631104 \n",
      "Epoch: 1/10:  mini-batch 2200/4459:  Train loss: 3.35064697265625  Test loss: 3.36677885055542 \n",
      "Epoch: 1/10:  mini-batch 2201/4459:  Train loss: 3.303311347961426  Test loss: 3.3680715560913086 \n",
      "Epoch: 1/10:  mini-batch 2202/4459:  Train loss: 3.294569969177246  Test loss: 3.3691439628601074 \n",
      "Epoch: 1/10:  mini-batch 2203/4459:  Train loss: 3.26222562789917  Test loss: 3.3701696395874023 \n",
      "Epoch: 1/10:  mini-batch 2204/4459:  Train loss: 3.6568901538848877  Test loss: 3.3710532188415527 \n",
      "Epoch: 1/10:  mini-batch 2205/4459:  Train loss: 3.52829647064209  Test loss: 3.3724069595336914 \n",
      "Epoch: 1/10:  mini-batch 2206/4459:  Train loss: 3.436483383178711  Test loss: 3.373819351196289 \n",
      "Epoch: 1/10:  mini-batch 2207/4459:  Train loss: 3.4972188472747803  Test loss: 3.3750076293945312 \n",
      "Epoch: 1/10:  mini-batch 2208/4459:  Train loss: 3.610607147216797  Test loss: 3.3762335777282715 \n",
      "Epoch: 1/10:  mini-batch 2209/4459:  Train loss: 3.286616802215576  Test loss: 3.3773508071899414 \n",
      "Epoch: 1/10:  mini-batch 2210/4459:  Train loss: 3.29777193069458  Test loss: 3.378488063812256 \n",
      "Epoch: 1/10:  mini-batch 2211/4459:  Train loss: 3.5281825065612793  Test loss: 3.379767417907715 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2212/4459:  Train loss: 3.369248867034912  Test loss: 3.3810975551605225 \n",
      "Epoch: 1/10:  mini-batch 2213/4459:  Train loss: 3.4437999725341797  Test loss: 3.3821616172790527 \n",
      "Epoch: 1/10:  mini-batch 2214/4459:  Train loss: 3.4704723358154297  Test loss: 3.38314151763916 \n",
      "Epoch: 1/10:  mini-batch 2215/4459:  Train loss: 3.2725436687469482  Test loss: 3.383807420730591 \n",
      "Epoch: 1/10:  mini-batch 2216/4459:  Train loss: 3.3903841972351074  Test loss: 3.384158134460449 \n",
      "Epoch: 1/10:  mini-batch 2217/4459:  Train loss: 3.406458854675293  Test loss: 3.385040283203125 \n",
      "Epoch: 1/10:  mini-batch 2218/4459:  Train loss: 3.229647397994995  Test loss: 3.385558605194092 \n",
      "Epoch: 1/10:  mini-batch 2219/4459:  Train loss: 3.170383930206299  Test loss: 3.3861513137817383 \n",
      "Epoch: 1/10:  mini-batch 2220/4459:  Train loss: 3.1978678703308105  Test loss: 3.386975049972534 \n",
      "Epoch: 1/10:  mini-batch 2221/4459:  Train loss: 3.293297290802002  Test loss: 3.3878939151763916 \n",
      "Epoch: 1/10:  mini-batch 2222/4459:  Train loss: 3.0960516929626465  Test loss: 3.3885936737060547 \n",
      "Epoch: 1/10:  mini-batch 2223/4459:  Train loss: 3.390843391418457  Test loss: 3.3892781734466553 \n",
      "Epoch: 1/10:  mini-batch 2224/4459:  Train loss: 3.298689126968384  Test loss: 3.3900561332702637 \n",
      "Epoch: 1/10:  mini-batch 2225/4459:  Train loss: 3.1089823246002197  Test loss: 3.39085054397583 \n",
      "Epoch: 1/10:  mini-batch 2226/4459:  Train loss: 3.115170955657959  Test loss: 3.3916003704071045 \n",
      "Epoch: 1/10:  mini-batch 2227/4459:  Train loss: 3.129164695739746  Test loss: 3.392429828643799 \n",
      "Epoch: 1/10:  mini-batch 2228/4459:  Train loss: 3.575529098510742  Test loss: 3.39339017868042 \n",
      "Epoch: 1/10:  mini-batch 2229/4459:  Train loss: 3.3015241622924805  Test loss: 3.394394874572754 \n",
      "Epoch: 1/10:  mini-batch 2230/4459:  Train loss: 3.4747297763824463  Test loss: 3.395319938659668 \n",
      "Epoch: 1/10:  mini-batch 2231/4459:  Train loss: 3.0616772174835205  Test loss: 3.396702766418457 \n",
      "Epoch: 1/10:  mini-batch 2232/4459:  Train loss: 3.4312100410461426  Test loss: 3.398087978363037 \n",
      "Epoch: 1/10:  mini-batch 2233/4459:  Train loss: 3.4177937507629395  Test loss: 3.3995189666748047 \n",
      "Epoch: 1/10:  mini-batch 2234/4459:  Train loss: 3.536095142364502  Test loss: 3.4009170532226562 \n",
      "Epoch: 1/10:  mini-batch 2235/4459:  Train loss: 3.2364988327026367  Test loss: 3.4022693634033203 \n",
      "Epoch: 1/10:  mini-batch 2236/4459:  Train loss: 3.32245135307312  Test loss: 3.402909755706787 \n",
      "Epoch: 1/10:  mini-batch 2237/4459:  Train loss: 3.25714111328125  Test loss: 3.403446674346924 \n",
      "Epoch: 1/10:  mini-batch 2238/4459:  Train loss: 3.303328514099121  Test loss: 3.4040369987487793 \n",
      "Epoch: 1/10:  mini-batch 2239/4459:  Train loss: 3.045438528060913  Test loss: 3.4051055908203125 \n",
      "Epoch: 1/10:  mini-batch 2240/4459:  Train loss: 3.269984483718872  Test loss: 3.406083822250366 \n",
      "Epoch: 1/10:  mini-batch 2241/4459:  Train loss: 3.4261674880981445  Test loss: 3.406444787979126 \n",
      "Epoch: 1/10:  mini-batch 2242/4459:  Train loss: 3.4998838901519775  Test loss: 3.405874490737915 \n",
      "Epoch: 1/10:  mini-batch 2243/4459:  Train loss: 3.2378334999084473  Test loss: 3.40517520904541 \n",
      "Epoch: 1/10:  mini-batch 2244/4459:  Train loss: 3.2406797409057617  Test loss: 3.404890298843384 \n",
      "Epoch: 1/10:  mini-batch 2245/4459:  Train loss: 3.1489522457122803  Test loss: 3.4045681953430176 \n",
      "Epoch: 1/10:  mini-batch 2246/4459:  Train loss: 3.218874454498291  Test loss: 3.40452241897583 \n",
      "Epoch: 1/10:  mini-batch 2247/4459:  Train loss: 3.1227171421051025  Test loss: 3.4047317504882812 \n",
      "Epoch: 1/10:  mini-batch 2248/4459:  Train loss: 3.1132707595825195  Test loss: 3.4056551456451416 \n",
      "Epoch: 1/10:  mini-batch 2249/4459:  Train loss: 3.3790335655212402  Test loss: 3.4066274166107178 \n",
      "Epoch: 1/10:  mini-batch 2250/4459:  Train loss: 3.1772499084472656  Test loss: 3.4078431129455566 \n",
      "Epoch: 1/10:  mini-batch 2251/4459:  Train loss: 3.446173667907715  Test loss: 3.40901255607605 \n",
      "Epoch: 1/10:  mini-batch 2252/4459:  Train loss: 3.507800817489624  Test loss: 3.4092373847961426 \n",
      "Epoch: 1/10:  mini-batch 2253/4459:  Train loss: 2.982158660888672  Test loss: 3.410503387451172 \n",
      "Epoch: 1/10:  mini-batch 2254/4459:  Train loss: 3.4797210693359375  Test loss: 3.4109458923339844 \n",
      "Epoch: 1/10:  mini-batch 2255/4459:  Train loss: 3.7111244201660156  Test loss: 3.410645008087158 \n",
      "Epoch: 1/10:  mini-batch 2256/4459:  Train loss: 3.4849069118499756  Test loss: 3.4104464054107666 \n",
      "Epoch: 1/10:  mini-batch 2257/4459:  Train loss: 3.1700057983398438  Test loss: 3.4100375175476074 \n",
      "Epoch: 1/10:  mini-batch 2258/4459:  Train loss: 3.2500405311584473  Test loss: 3.409750461578369 \n",
      "Epoch: 1/10:  mini-batch 2259/4459:  Train loss: 3.335998058319092  Test loss: 3.409775495529175 \n",
      "Epoch: 1/10:  mini-batch 2260/4459:  Train loss: 3.5870070457458496  Test loss: 3.4090335369110107 \n",
      "Epoch: 1/10:  mini-batch 2261/4459:  Train loss: 3.5502147674560547  Test loss: 3.4078421592712402 \n",
      "Epoch: 1/10:  mini-batch 2262/4459:  Train loss: 3.05094575881958  Test loss: 3.407179355621338 \n",
      "Epoch: 1/10:  mini-batch 2263/4459:  Train loss: 3.267077684402466  Test loss: 3.4066367149353027 \n",
      "Epoch: 1/10:  mini-batch 2264/4459:  Train loss: 3.451568126678467  Test loss: 3.405853033065796 \n",
      "Epoch: 1/10:  mini-batch 2265/4459:  Train loss: 3.2217259407043457  Test loss: 3.4047582149505615 \n",
      "Epoch: 1/10:  mini-batch 2266/4459:  Train loss: 3.420806646347046  Test loss: 3.4039831161499023 \n",
      "Epoch: 1/10:  mini-batch 2267/4459:  Train loss: 3.382077693939209  Test loss: 3.4034037590026855 \n",
      "Epoch: 1/10:  mini-batch 2268/4459:  Train loss: 3.1434459686279297  Test loss: 3.4035744667053223 \n",
      "Epoch: 1/10:  mini-batch 2269/4459:  Train loss: 3.6020803451538086  Test loss: 3.4034082889556885 \n",
      "Epoch: 1/10:  mini-batch 2270/4459:  Train loss: 4.047653675079346  Test loss: 3.4025216102600098 \n",
      "Epoch: 1/10:  mini-batch 2271/4459:  Train loss: 3.670318126678467  Test loss: 3.4018402099609375 \n",
      "Epoch: 1/10:  mini-batch 2272/4459:  Train loss: 3.2435054779052734  Test loss: 3.4017090797424316 \n",
      "Epoch: 1/10:  mini-batch 2273/4459:  Train loss: 3.5068039894104004  Test loss: 3.401555061340332 \n",
      "Epoch: 1/10:  mini-batch 2274/4459:  Train loss: 3.2246782779693604  Test loss: 3.401529312133789 \n",
      "Epoch: 1/10:  mini-batch 2275/4459:  Train loss: 3.279012441635132  Test loss: 3.4015939235687256 \n",
      "Epoch: 1/10:  mini-batch 2276/4459:  Train loss: 3.4332170486450195  Test loss: 3.4019594192504883 \n",
      "Epoch: 1/10:  mini-batch 2277/4459:  Train loss: 3.301769733428955  Test loss: 3.4021992683410645 \n",
      "Epoch: 1/10:  mini-batch 2278/4459:  Train loss: 3.224708318710327  Test loss: 3.4025187492370605 \n",
      "Epoch: 1/10:  mini-batch 2279/4459:  Train loss: 3.0922818183898926  Test loss: 3.4030261039733887 \n",
      "Epoch: 1/10:  mini-batch 2280/4459:  Train loss: 3.348515033721924  Test loss: 3.4035584926605225 \n",
      "Epoch: 1/10:  mini-batch 2281/4459:  Train loss: 3.450659990310669  Test loss: 3.4036519527435303 \n",
      "Epoch: 1/10:  mini-batch 2282/4459:  Train loss: 3.1870839595794678  Test loss: 3.4038867950439453 \n",
      "Epoch: 1/10:  mini-batch 2283/4459:  Train loss: 3.508244514465332  Test loss: 3.404123067855835 \n",
      "Epoch: 1/10:  mini-batch 2284/4459:  Train loss: 3.6177968978881836  Test loss: 3.404292106628418 \n",
      "Epoch: 1/10:  mini-batch 2285/4459:  Train loss: 3.2761433124542236  Test loss: 3.404172420501709 \n",
      "Epoch: 1/10:  mini-batch 2286/4459:  Train loss: 2.965567111968994  Test loss: 3.4041714668273926 \n",
      "Epoch: 1/10:  mini-batch 2287/4459:  Train loss: 3.3432559967041016  Test loss: 3.4043493270874023 \n",
      "Epoch: 1/10:  mini-batch 2288/4459:  Train loss: 3.3170387744903564  Test loss: 3.404538154602051 \n",
      "Epoch: 1/10:  mini-batch 2289/4459:  Train loss: 3.1569983959198  Test loss: 3.404787063598633 \n",
      "Epoch: 1/10:  mini-batch 2290/4459:  Train loss: 3.238032579421997  Test loss: 3.4050869941711426 \n",
      "Epoch: 1/10:  mini-batch 2291/4459:  Train loss: 3.170872211456299  Test loss: 3.4054155349731445 \n",
      "Epoch: 1/10:  mini-batch 2292/4459:  Train loss: 3.246727228164673  Test loss: 3.4056851863861084 \n",
      "Epoch: 1/10:  mini-batch 2293/4459:  Train loss: 3.1701955795288086  Test loss: 3.406165838241577 \n",
      "Epoch: 1/10:  mini-batch 2294/4459:  Train loss: 3.2333292961120605  Test loss: 3.406663656234741 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2295/4459:  Train loss: 3.235778331756592  Test loss: 3.4076738357543945 \n",
      "Epoch: 1/10:  mini-batch 2296/4459:  Train loss: 3.5144426822662354  Test loss: 3.408494710922241 \n",
      "Epoch: 1/10:  mini-batch 2297/4459:  Train loss: 3.347569227218628  Test loss: 3.4096522331237793 \n",
      "Epoch: 1/10:  mini-batch 2298/4459:  Train loss: 3.213994026184082  Test loss: 3.411283016204834 \n",
      "Epoch: 1/10:  mini-batch 2299/4459:  Train loss: 3.2780675888061523  Test loss: 3.4130990505218506 \n",
      "Epoch: 1/10:  mini-batch 2300/4459:  Train loss: 3.225250244140625  Test loss: 3.4146039485931396 \n",
      "Epoch: 1/10:  mini-batch 2301/4459:  Train loss: 3.085536479949951  Test loss: 3.416673183441162 \n",
      "Epoch: 1/10:  mini-batch 2302/4459:  Train loss: 3.1829771995544434  Test loss: 3.418905019760132 \n",
      "Epoch: 1/10:  mini-batch 2303/4459:  Train loss: 3.0365052223205566  Test loss: 3.421581983566284 \n",
      "Epoch: 1/10:  mini-batch 2304/4459:  Train loss: 3.2585878372192383  Test loss: 3.4245095252990723 \n",
      "Epoch: 1/10:  mini-batch 2305/4459:  Train loss: 3.4199514389038086  Test loss: 3.4266409873962402 \n",
      "Epoch: 1/10:  mini-batch 2306/4459:  Train loss: 3.304593086242676  Test loss: 3.4286088943481445 \n",
      "Epoch: 1/10:  mini-batch 2307/4459:  Train loss: 3.399986505508423  Test loss: 3.4300756454467773 \n",
      "Epoch: 1/10:  mini-batch 2308/4459:  Train loss: 3.0729446411132812  Test loss: 3.4318575859069824 \n",
      "Epoch: 1/10:  mini-batch 2309/4459:  Train loss: 3.0040338039398193  Test loss: 3.434164524078369 \n",
      "Epoch: 1/10:  mini-batch 2310/4459:  Train loss: 3.2940456867218018  Test loss: 3.4351484775543213 \n",
      "Epoch: 1/10:  mini-batch 2311/4459:  Train loss: 3.7798662185668945  Test loss: 3.434140682220459 \n",
      "Epoch: 1/10:  mini-batch 2312/4459:  Train loss: 3.454158067703247  Test loss: 3.4323296546936035 \n",
      "Epoch: 1/10:  mini-batch 2313/4459:  Train loss: 3.137058734893799  Test loss: 3.431222677230835 \n",
      "Epoch: 1/10:  mini-batch 2314/4459:  Train loss: 2.9827072620391846  Test loss: 3.4309396743774414 \n",
      "Epoch: 1/10:  mini-batch 2315/4459:  Train loss: 3.372460126876831  Test loss: 3.4298672676086426 \n",
      "Epoch: 1/10:  mini-batch 2316/4459:  Train loss: 3.7373342514038086  Test loss: 3.4271810054779053 \n",
      "Epoch: 1/10:  mini-batch 2317/4459:  Train loss: 3.247860908508301  Test loss: 3.4250054359436035 \n",
      "Epoch: 1/10:  mini-batch 2318/4459:  Train loss: 3.628706932067871  Test loss: 3.422254800796509 \n",
      "Epoch: 1/10:  mini-batch 2319/4459:  Train loss: 3.2394111156463623  Test loss: 3.419753074645996 \n",
      "Epoch: 1/10:  mini-batch 2320/4459:  Train loss: 3.60546875  Test loss: 3.4170267581939697 \n",
      "Epoch: 1/10:  mini-batch 2321/4459:  Train loss: 3.6605753898620605  Test loss: 3.4143455028533936 \n",
      "Epoch: 1/10:  mini-batch 2322/4459:  Train loss: 3.0322132110595703  Test loss: 3.4131078720092773 \n",
      "Epoch: 1/10:  mini-batch 2323/4459:  Train loss: 3.776585102081299  Test loss: 3.4112119674682617 \n",
      "Epoch: 1/10:  mini-batch 2324/4459:  Train loss: 3.3809568881988525  Test loss: 3.409547805786133 \n",
      "Epoch: 1/10:  mini-batch 2325/4459:  Train loss: 3.6585779190063477  Test loss: 3.4078457355499268 \n",
      "Epoch: 1/10:  mini-batch 2326/4459:  Train loss: 3.4994254112243652  Test loss: 3.4060540199279785 \n",
      "Epoch: 1/10:  mini-batch 2327/4459:  Train loss: 2.9264533519744873  Test loss: 3.405123233795166 \n",
      "Epoch: 1/10:  mini-batch 2328/4459:  Train loss: 2.9536657333374023  Test loss: 3.404607057571411 \n",
      "Epoch: 1/10:  mini-batch 2329/4459:  Train loss: 3.635761022567749  Test loss: 3.4039392471313477 \n",
      "Epoch: 1/10:  mini-batch 2330/4459:  Train loss: 3.7580156326293945  Test loss: 3.4032537937164307 \n",
      "Epoch: 1/10:  mini-batch 2331/4459:  Train loss: 3.455850601196289  Test loss: 3.4026613235473633 \n",
      "Epoch: 1/10:  mini-batch 2332/4459:  Train loss: 3.5114188194274902  Test loss: 3.4021964073181152 \n",
      "Epoch: 1/10:  mini-batch 2333/4459:  Train loss: 2.9842891693115234  Test loss: 3.4017910957336426 \n",
      "Epoch: 1/10:  mini-batch 2334/4459:  Train loss: 3.4954118728637695  Test loss: 3.401421308517456 \n",
      "Epoch: 1/10:  mini-batch 2335/4459:  Train loss: 3.600219249725342  Test loss: 3.4011526107788086 \n",
      "Epoch: 1/10:  mini-batch 2336/4459:  Train loss: 3.427185297012329  Test loss: 3.4008095264434814 \n",
      "Epoch: 1/10:  mini-batch 2337/4459:  Train loss: 3.4116501808166504  Test loss: 3.4008846282958984 \n",
      "Epoch: 1/10:  mini-batch 2338/4459:  Train loss: 3.107802629470825  Test loss: 3.4009416103363037 \n",
      "Epoch: 1/10:  mini-batch 2339/4459:  Train loss: 3.603313446044922  Test loss: 3.401362180709839 \n",
      "Epoch: 1/10:  mini-batch 2340/4459:  Train loss: 3.3089051246643066  Test loss: 3.401524066925049 \n",
      "Epoch: 1/10:  mini-batch 2341/4459:  Train loss: 3.41843843460083  Test loss: 3.4015121459960938 \n",
      "Epoch: 1/10:  mini-batch 2342/4459:  Train loss: 3.3249361515045166  Test loss: 3.401447296142578 \n",
      "Epoch: 1/10:  mini-batch 2343/4459:  Train loss: 3.5635011196136475  Test loss: 3.401430606842041 \n",
      "Epoch: 1/10:  mini-batch 2344/4459:  Train loss: 3.2860448360443115  Test loss: 3.4015069007873535 \n",
      "Epoch: 1/10:  mini-batch 2345/4459:  Train loss: 3.197117328643799  Test loss: 3.4015963077545166 \n",
      "Epoch: 1/10:  mini-batch 2346/4459:  Train loss: 3.6419734954833984  Test loss: 3.401883125305176 \n",
      "Epoch: 1/10:  mini-batch 2347/4459:  Train loss: 3.2389087677001953  Test loss: 3.4021353721618652 \n",
      "Epoch: 1/10:  mini-batch 2348/4459:  Train loss: 3.0963170528411865  Test loss: 3.402341365814209 \n",
      "Epoch: 1/10:  mini-batch 2349/4459:  Train loss: 3.4608006477355957  Test loss: 3.402477264404297 \n",
      "Epoch: 1/10:  mini-batch 2350/4459:  Train loss: 3.3526434898376465  Test loss: 3.402580738067627 \n",
      "Epoch: 1/10:  mini-batch 2351/4459:  Train loss: 3.5477395057678223  Test loss: 3.402886390686035 \n",
      "Epoch: 1/10:  mini-batch 2352/4459:  Train loss: 3.5804800987243652  Test loss: 3.403179407119751 \n",
      "Epoch: 1/10:  mini-batch 2353/4459:  Train loss: 3.5312294960021973  Test loss: 3.4036123752593994 \n",
      "Epoch: 1/10:  mini-batch 2354/4459:  Train loss: 3.288547992706299  Test loss: 3.4041929244995117 \n",
      "Epoch: 1/10:  mini-batch 2355/4459:  Train loss: 3.28452730178833  Test loss: 3.4048399925231934 \n",
      "Epoch: 1/10:  mini-batch 2356/4459:  Train loss: 3.0670971870422363  Test loss: 3.405263900756836 \n",
      "Epoch: 1/10:  mini-batch 2357/4459:  Train loss: 3.3787922859191895  Test loss: 3.4053869247436523 \n",
      "Epoch: 1/10:  mini-batch 2358/4459:  Train loss: 3.3577346801757812  Test loss: 3.4055862426757812 \n",
      "Epoch: 1/10:  mini-batch 2359/4459:  Train loss: 3.4007205963134766  Test loss: 3.405852794647217 \n",
      "Epoch: 1/10:  mini-batch 2360/4459:  Train loss: 3.4955244064331055  Test loss: 3.40615177154541 \n",
      "Epoch: 1/10:  mini-batch 2361/4459:  Train loss: 3.2761740684509277  Test loss: 3.406369924545288 \n",
      "Epoch: 1/10:  mini-batch 2362/4459:  Train loss: 3.337040424346924  Test loss: 3.4068191051483154 \n",
      "Epoch: 1/10:  mini-batch 2363/4459:  Train loss: 3.2827367782592773  Test loss: 3.4074044227600098 \n",
      "Epoch: 1/10:  mini-batch 2364/4459:  Train loss: 3.0671792030334473  Test loss: 3.407958984375 \n",
      "Epoch: 1/10:  mini-batch 2365/4459:  Train loss: 3.3129382133483887  Test loss: 3.408626079559326 \n",
      "Epoch: 1/10:  mini-batch 2366/4459:  Train loss: 3.436615467071533  Test loss: 3.4092750549316406 \n",
      "Epoch: 1/10:  mini-batch 2367/4459:  Train loss: 3.07723069190979  Test loss: 3.4100358486175537 \n",
      "Epoch: 1/10:  mini-batch 2368/4459:  Train loss: 3.184277296066284  Test loss: 3.4107651710510254 \n",
      "Epoch: 1/10:  mini-batch 2369/4459:  Train loss: 3.0662474632263184  Test loss: 3.4113502502441406 \n",
      "Epoch: 1/10:  mini-batch 2370/4459:  Train loss: 3.1409647464752197  Test loss: 3.411567211151123 \n",
      "Epoch: 1/10:  mini-batch 2371/4459:  Train loss: 3.5347602367401123  Test loss: 3.4114255905151367 \n",
      "Epoch: 1/10:  mini-batch 2372/4459:  Train loss: 3.2870888710021973  Test loss: 3.4115421772003174 \n",
      "Epoch: 1/10:  mini-batch 2373/4459:  Train loss: 3.178502321243286  Test loss: 3.4117789268493652 \n",
      "Epoch: 1/10:  mini-batch 2374/4459:  Train loss: 3.146392822265625  Test loss: 3.4126970767974854 \n",
      "Epoch: 1/10:  mini-batch 2375/4459:  Train loss: 3.3352980613708496  Test loss: 3.413689136505127 \n",
      "Epoch: 1/10:  mini-batch 2376/4459:  Train loss: 3.322246551513672  Test loss: 3.4147284030914307 \n",
      "Epoch: 1/10:  mini-batch 2377/4459:  Train loss: 3.2939586639404297  Test loss: 3.41568660736084 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2378/4459:  Train loss: 3.417630195617676  Test loss: 3.415773868560791 \n",
      "Epoch: 1/10:  mini-batch 2379/4459:  Train loss: 3.365030288696289  Test loss: 3.4159624576568604 \n",
      "Epoch: 1/10:  mini-batch 2380/4459:  Train loss: 2.9892773628234863  Test loss: 3.4168801307678223 \n",
      "Epoch: 1/10:  mini-batch 2381/4459:  Train loss: 3.2788169384002686  Test loss: 3.4180636405944824 \n",
      "Epoch: 1/10:  mini-batch 2382/4459:  Train loss: 3.8080391883850098  Test loss: 3.4186248779296875 \n",
      "Epoch: 1/10:  mini-batch 2383/4459:  Train loss: 3.4599123001098633  Test loss: 3.4193873405456543 \n",
      "Epoch: 1/10:  mini-batch 2384/4459:  Train loss: 3.1418769359588623  Test loss: 3.4207725524902344 \n",
      "Epoch: 1/10:  mini-batch 2385/4459:  Train loss: 2.7851054668426514  Test loss: 3.4231972694396973 \n",
      "Epoch: 1/10:  mini-batch 2386/4459:  Train loss: 3.050187587738037  Test loss: 3.426175117492676 \n",
      "Epoch: 1/10:  mini-batch 2387/4459:  Train loss: 2.752187490463257  Test loss: 3.4302592277526855 \n",
      "Epoch: 1/10:  mini-batch 2388/4459:  Train loss: 3.4921844005584717  Test loss: 3.433835744857788 \n",
      "Epoch: 1/10:  mini-batch 2389/4459:  Train loss: 2.938227415084839  Test loss: 3.437537431716919 \n",
      "Epoch: 1/10:  mini-batch 2390/4459:  Train loss: 3.256068229675293  Test loss: 3.4411375522613525 \n",
      "Epoch: 1/10:  mini-batch 2391/4459:  Train loss: 3.1263227462768555  Test loss: 3.4445595741271973 \n",
      "Epoch: 1/10:  mini-batch 2392/4459:  Train loss: 3.469789505004883  Test loss: 3.446190357208252 \n",
      "Epoch: 1/10:  mini-batch 2393/4459:  Train loss: 3.630802869796753  Test loss: 3.4460482597351074 \n",
      "Epoch: 1/10:  mini-batch 2394/4459:  Train loss: 2.8986308574676514  Test loss: 3.4469175338745117 \n",
      "Epoch: 1/10:  mini-batch 2395/4459:  Train loss: 3.306412696838379  Test loss: 3.4475226402282715 \n",
      "Epoch: 1/10:  mini-batch 2396/4459:  Train loss: 3.4074442386627197  Test loss: 3.4474434852600098 \n",
      "Epoch: 1/10:  mini-batch 2397/4459:  Train loss: 3.4652090072631836  Test loss: 3.447338581085205 \n",
      "Epoch: 1/10:  mini-batch 2398/4459:  Train loss: 3.620595693588257  Test loss: 3.4461421966552734 \n",
      "Epoch: 1/10:  mini-batch 2399/4459:  Train loss: 3.38645339012146  Test loss: 3.4444077014923096 \n",
      "Epoch: 1/10:  mini-batch 2400/4459:  Train loss: 3.7189691066741943  Test loss: 3.4411773681640625 \n",
      "Epoch: 1/10:  mini-batch 2401/4459:  Train loss: 3.5419254302978516  Test loss: 3.4374194145202637 \n",
      "Epoch: 1/10:  mini-batch 2402/4459:  Train loss: 3.3662257194519043  Test loss: 3.433835744857788 \n",
      "Epoch: 1/10:  mini-batch 2403/4459:  Train loss: 3.513075590133667  Test loss: 3.4306440353393555 \n",
      "Epoch: 1/10:  mini-batch 2404/4459:  Train loss: 3.3178293704986572  Test loss: 3.427830219268799 \n",
      "Epoch: 1/10:  mini-batch 2405/4459:  Train loss: 3.6677703857421875  Test loss: 3.4252023696899414 \n",
      "Epoch: 1/10:  mini-batch 2406/4459:  Train loss: 3.06516170501709  Test loss: 3.423745632171631 \n",
      "Epoch: 1/10:  mini-batch 2407/4459:  Train loss: 3.3898110389709473  Test loss: 3.4225823879241943 \n",
      "Epoch: 1/10:  mini-batch 2408/4459:  Train loss: 3.7032933235168457  Test loss: 3.421123504638672 \n",
      "Epoch: 1/10:  mini-batch 2409/4459:  Train loss: 3.6182494163513184  Test loss: 3.419170618057251 \n",
      "Epoch: 1/10:  mini-batch 2410/4459:  Train loss: 3.4815478324890137  Test loss: 3.417635440826416 \n",
      "Epoch: 1/10:  mini-batch 2411/4459:  Train loss: 3.467609405517578  Test loss: 3.4168336391448975 \n",
      "Epoch: 1/10:  mini-batch 2412/4459:  Train loss: 3.352644205093384  Test loss: 3.4161412715911865 \n",
      "Epoch: 1/10:  mini-batch 2413/4459:  Train loss: 3.531790256500244  Test loss: 3.4154882431030273 \n",
      "Epoch: 1/10:  mini-batch 2414/4459:  Train loss: 3.057344675064087  Test loss: 3.4152424335479736 \n",
      "Epoch: 1/10:  mini-batch 2415/4459:  Train loss: 3.2611801624298096  Test loss: 3.415102005004883 \n",
      "Epoch: 1/10:  mini-batch 2416/4459:  Train loss: 3.3199400901794434  Test loss: 3.415066719055176 \n",
      "Epoch: 1/10:  mini-batch 2417/4459:  Train loss: 3.361515998840332  Test loss: 3.4152350425720215 \n",
      "Epoch: 1/10:  mini-batch 2418/4459:  Train loss: 3.3236162662506104  Test loss: 3.4155421257019043 \n",
      "Epoch: 1/10:  mini-batch 2419/4459:  Train loss: 3.466386556625366  Test loss: 3.415755271911621 \n",
      "Epoch: 1/10:  mini-batch 2420/4459:  Train loss: 3.1869516372680664  Test loss: 3.4161853790283203 \n",
      "Epoch: 1/10:  mini-batch 2421/4459:  Train loss: 3.445878505706787  Test loss: 3.4166932106018066 \n",
      "Epoch: 1/10:  mini-batch 2422/4459:  Train loss: 3.171198844909668  Test loss: 3.4170448780059814 \n",
      "Epoch: 1/10:  mini-batch 2423/4459:  Train loss: 3.6336774826049805  Test loss: 3.4172534942626953 \n",
      "Epoch: 1/10:  mini-batch 2424/4459:  Train loss: 3.082335948944092  Test loss: 3.4175007343292236 \n",
      "Epoch: 1/10:  mini-batch 2425/4459:  Train loss: 3.4158942699432373  Test loss: 3.4172863960266113 \n",
      "Epoch: 1/10:  mini-batch 2426/4459:  Train loss: 3.332421064376831  Test loss: 3.417175769805908 \n",
      "Epoch: 1/10:  mini-batch 2427/4459:  Train loss: 3.7917985916137695  Test loss: 3.416822910308838 \n",
      "Epoch: 1/10:  mini-batch 2428/4459:  Train loss: 3.567833423614502  Test loss: 3.416746139526367 \n",
      "Epoch: 1/10:  mini-batch 2429/4459:  Train loss: 3.5565969944000244  Test loss: 3.416602611541748 \n",
      "Epoch: 1/10:  mini-batch 2430/4459:  Train loss: 3.5788469314575195  Test loss: 3.4167919158935547 \n",
      "Epoch: 1/10:  mini-batch 2431/4459:  Train loss: 3.4370219707489014  Test loss: 3.416828155517578 \n",
      "Epoch: 1/10:  mini-batch 2432/4459:  Train loss: 3.6133737564086914  Test loss: 3.416808843612671 \n",
      "Epoch: 1/10:  mini-batch 2433/4459:  Train loss: 3.5549521446228027  Test loss: 3.416754961013794 \n",
      "Epoch: 1/10:  mini-batch 2434/4459:  Train loss: 3.3305840492248535  Test loss: 3.4168457984924316 \n",
      "Epoch: 1/10:  mini-batch 2435/4459:  Train loss: 3.405972480773926  Test loss: 3.4169039726257324 \n",
      "Epoch: 1/10:  mini-batch 2436/4459:  Train loss: 3.5270180702209473  Test loss: 3.416755199432373 \n",
      "Epoch: 1/10:  mini-batch 2437/4459:  Train loss: 3.384410858154297  Test loss: 3.4164934158325195 \n",
      "Epoch: 1/10:  mini-batch 2438/4459:  Train loss: 3.234633684158325  Test loss: 3.4161999225616455 \n",
      "Epoch: 1/10:  mini-batch 2439/4459:  Train loss: 3.375295639038086  Test loss: 3.4156532287597656 \n",
      "Epoch: 1/10:  mini-batch 2440/4459:  Train loss: 3.2036778926849365  Test loss: 3.414736270904541 \n",
      "Epoch: 1/10:  mini-batch 2441/4459:  Train loss: 3.178636312484741  Test loss: 3.413896083831787 \n",
      "Epoch: 1/10:  mini-batch 2442/4459:  Train loss: 3.3945417404174805  Test loss: 3.4129953384399414 \n",
      "Epoch: 1/10:  mini-batch 2443/4459:  Train loss: 3.2017664909362793  Test loss: 3.412008285522461 \n",
      "Epoch: 1/10:  mini-batch 2444/4459:  Train loss: 3.427539110183716  Test loss: 3.411301374435425 \n",
      "Epoch: 1/10:  mini-batch 2445/4459:  Train loss: 3.6522161960601807  Test loss: 3.410830020904541 \n",
      "Epoch: 1/10:  mini-batch 2446/4459:  Train loss: 3.339691162109375  Test loss: 3.4106264114379883 \n",
      "Epoch: 1/10:  mini-batch 2447/4459:  Train loss: 3.3521342277526855  Test loss: 3.4103527069091797 \n",
      "Epoch: 1/10:  mini-batch 2448/4459:  Train loss: 3.2858285903930664  Test loss: 3.4104573726654053 \n",
      "Epoch: 1/10:  mini-batch 2449/4459:  Train loss: 3.081784725189209  Test loss: 3.4101274013519287 \n",
      "Epoch: 1/10:  mini-batch 2450/4459:  Train loss: 2.9826924800872803  Test loss: 3.4098877906799316 \n",
      "Epoch: 1/10:  mini-batch 2451/4459:  Train loss: 3.136617660522461  Test loss: 3.4095559120178223 \n",
      "Epoch: 1/10:  mini-batch 2452/4459:  Train loss: 3.27646541595459  Test loss: 3.409579277038574 \n",
      "Epoch: 1/10:  mini-batch 2453/4459:  Train loss: 3.092060089111328  Test loss: 3.4093704223632812 \n",
      "Epoch: 1/10:  mini-batch 2454/4459:  Train loss: 3.199012279510498  Test loss: 3.409497022628784 \n",
      "Epoch: 1/10:  mini-batch 2455/4459:  Train loss: 3.2691965103149414  Test loss: 3.4099395275115967 \n",
      "Epoch: 1/10:  mini-batch 2456/4459:  Train loss: 3.4076507091522217  Test loss: 3.4109508991241455 \n",
      "Epoch: 1/10:  mini-batch 2457/4459:  Train loss: 3.023366928100586  Test loss: 3.411787748336792 \n",
      "Epoch: 1/10:  mini-batch 2458/4459:  Train loss: 3.330247640609741  Test loss: 3.412933826446533 \n",
      "Epoch: 1/10:  mini-batch 2459/4459:  Train loss: 2.9637069702148438  Test loss: 3.4142227172851562 \n",
      "Epoch: 1/10:  mini-batch 2460/4459:  Train loss: 3.3414466381073  Test loss: 3.4163060188293457 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2461/4459:  Train loss: 3.4728524684906006  Test loss: 3.418905258178711 \n",
      "Epoch: 1/10:  mini-batch 2462/4459:  Train loss: 3.0336954593658447  Test loss: 3.422255039215088 \n",
      "Epoch: 1/10:  mini-batch 2463/4459:  Train loss: 3.358656406402588  Test loss: 3.4254465103149414 \n",
      "Epoch: 1/10:  mini-batch 2464/4459:  Train loss: 3.4693102836608887  Test loss: 3.428236484527588 \n",
      "Epoch: 1/10:  mini-batch 2465/4459:  Train loss: 3.8325233459472656  Test loss: 3.430032968521118 \n",
      "Epoch: 1/10:  mini-batch 2466/4459:  Train loss: 3.1107025146484375  Test loss: 3.431903123855591 \n",
      "Epoch: 1/10:  mini-batch 2467/4459:  Train loss: 3.5245347023010254  Test loss: 3.43397855758667 \n",
      "Epoch: 1/10:  mini-batch 2468/4459:  Train loss: 3.498342990875244  Test loss: 3.435940742492676 \n",
      "Epoch: 1/10:  mini-batch 2469/4459:  Train loss: 2.9722824096679688  Test loss: 3.4374449253082275 \n",
      "Epoch: 1/10:  mini-batch 2470/4459:  Train loss: 2.9952492713928223  Test loss: 3.439093828201294 \n",
      "Epoch: 1/10:  mini-batch 2471/4459:  Train loss: 3.401820421218872  Test loss: 3.440314769744873 \n",
      "Epoch: 1/10:  mini-batch 2472/4459:  Train loss: 2.941457748413086  Test loss: 3.4432761669158936 \n",
      "Epoch: 1/10:  mini-batch 2473/4459:  Train loss: 2.862931489944458  Test loss: 3.447019100189209 \n",
      "Epoch: 1/10:  mini-batch 2474/4459:  Train loss: 3.8276143074035645  Test loss: 3.4480254650115967 \n",
      "Epoch: 1/10:  mini-batch 2475/4459:  Train loss: 3.2562034130096436  Test loss: 3.4485995769500732 \n",
      "Epoch: 1/10:  mini-batch 2476/4459:  Train loss: 3.1835997104644775  Test loss: 3.449129581451416 \n",
      "Epoch: 1/10:  mini-batch 2477/4459:  Train loss: 3.165658950805664  Test loss: 3.449618339538574 \n",
      "Epoch: 1/10:  mini-batch 2478/4459:  Train loss: 3.0667247772216797  Test loss: 3.450847625732422 \n",
      "Epoch: 1/10:  mini-batch 2479/4459:  Train loss: 3.8306944370269775  Test loss: 3.4505863189697266 \n",
      "Epoch: 1/10:  mini-batch 2480/4459:  Train loss: 3.355839490890503  Test loss: 3.450042724609375 \n",
      "Epoch: 1/10:  mini-batch 2481/4459:  Train loss: 3.6142053604125977  Test loss: 3.4485886096954346 \n",
      "Epoch: 1/10:  mini-batch 2482/4459:  Train loss: 3.154329299926758  Test loss: 3.447627544403076 \n",
      "Epoch: 1/10:  mini-batch 2483/4459:  Train loss: 3.217040777206421  Test loss: 3.446523666381836 \n",
      "Epoch: 1/10:  mini-batch 2484/4459:  Train loss: 3.5613439083099365  Test loss: 3.4450576305389404 \n",
      "Epoch: 1/10:  mini-batch 2485/4459:  Train loss: 3.09601092338562  Test loss: 3.4437661170959473 \n",
      "Epoch: 1/10:  mini-batch 2486/4459:  Train loss: 2.937091827392578  Test loss: 3.44335675239563 \n",
      "Epoch: 1/10:  mini-batch 2487/4459:  Train loss: 3.2472686767578125  Test loss: 3.442626714706421 \n",
      "Epoch: 1/10:  mini-batch 2488/4459:  Train loss: 3.233232021331787  Test loss: 3.442432165145874 \n",
      "Epoch: 1/10:  mini-batch 2489/4459:  Train loss: 2.917240619659424  Test loss: 3.4431138038635254 \n",
      "Epoch: 1/10:  mini-batch 2490/4459:  Train loss: 3.5455164909362793  Test loss: 3.442286968231201 \n",
      "Epoch: 1/10:  mini-batch 2491/4459:  Train loss: 3.501575231552124  Test loss: 3.4407033920288086 \n",
      "Epoch: 1/10:  mini-batch 2492/4459:  Train loss: 3.2258458137512207  Test loss: 3.439269542694092 \n",
      "Epoch: 1/10:  mini-batch 2493/4459:  Train loss: 3.4491236209869385  Test loss: 3.437342643737793 \n",
      "Epoch: 1/10:  mini-batch 2494/4459:  Train loss: 3.0696451663970947  Test loss: 3.43615984916687 \n",
      "Epoch: 1/10:  mini-batch 2495/4459:  Train loss: 3.4542489051818848  Test loss: 3.4342832565307617 \n",
      "Epoch: 1/10:  mini-batch 2496/4459:  Train loss: 3.3855955600738525  Test loss: 3.4323768615722656 \n",
      "Epoch: 1/10:  mini-batch 2497/4459:  Train loss: 3.5765085220336914  Test loss: 3.430511236190796 \n",
      "Epoch: 1/10:  mini-batch 2498/4459:  Train loss: 3.4559216499328613  Test loss: 3.428725242614746 \n",
      "Epoch: 1/10:  mini-batch 2499/4459:  Train loss: 3.18035888671875  Test loss: 3.427164077758789 \n",
      "Epoch: 1/10:  mini-batch 2500/4459:  Train loss: 3.176816463470459  Test loss: 3.4259285926818848 \n",
      "Epoch: 1/10:  mini-batch 2501/4459:  Train loss: 3.1591153144836426  Test loss: 3.4252991676330566 \n",
      "Epoch: 1/10:  mini-batch 2502/4459:  Train loss: 3.3855957984924316  Test loss: 3.4249582290649414 \n",
      "Epoch: 1/10:  mini-batch 2503/4459:  Train loss: 3.47664213180542  Test loss: 3.4244415760040283 \n",
      "Epoch: 1/10:  mini-batch 2504/4459:  Train loss: 3.331758499145508  Test loss: 3.424123764038086 \n",
      "Epoch: 1/10:  mini-batch 2505/4459:  Train loss: 3.4504384994506836  Test loss: 3.423610210418701 \n",
      "Epoch: 1/10:  mini-batch 2506/4459:  Train loss: 3.464219331741333  Test loss: 3.423305034637451 \n",
      "Epoch: 1/10:  mini-batch 2507/4459:  Train loss: 3.574456214904785  Test loss: 3.4229941368103027 \n",
      "Epoch: 1/10:  mini-batch 2508/4459:  Train loss: 3.515979766845703  Test loss: 3.4223713874816895 \n",
      "Epoch: 1/10:  mini-batch 2509/4459:  Train loss: 3.5106801986694336  Test loss: 3.4217934608459473 \n",
      "Epoch: 1/10:  mini-batch 2510/4459:  Train loss: 3.4810144901275635  Test loss: 3.421792507171631 \n",
      "Epoch: 1/10:  mini-batch 2511/4459:  Train loss: 3.3787591457366943  Test loss: 3.421812057495117 \n",
      "Epoch: 1/10:  mini-batch 2512/4459:  Train loss: 3.2842905521392822  Test loss: 3.4221701622009277 \n",
      "Epoch: 1/10:  mini-batch 2513/4459:  Train loss: 3.1616783142089844  Test loss: 3.422438144683838 \n",
      "Epoch: 1/10:  mini-batch 2514/4459:  Train loss: 3.4148902893066406  Test loss: 3.422727346420288 \n",
      "Epoch: 1/10:  mini-batch 2515/4459:  Train loss: 3.130091667175293  Test loss: 3.4229564666748047 \n",
      "Epoch: 1/10:  mini-batch 2516/4459:  Train loss: 3.3665950298309326  Test loss: 3.422990560531616 \n",
      "Epoch: 1/10:  mini-batch 2517/4459:  Train loss: 3.4828076362609863  Test loss: 3.4227583408355713 \n",
      "Epoch: 1/10:  mini-batch 2518/4459:  Train loss: 3.44014835357666  Test loss: 3.422679901123047 \n",
      "Epoch: 1/10:  mini-batch 2519/4459:  Train loss: 3.2952511310577393  Test loss: 3.4229109287261963 \n",
      "Epoch: 1/10:  mini-batch 2520/4459:  Train loss: 3.3299341201782227  Test loss: 3.4235568046569824 \n",
      "Epoch: 1/10:  mini-batch 2521/4459:  Train loss: 3.3980023860931396  Test loss: 3.424126625061035 \n",
      "Epoch: 1/10:  mini-batch 2522/4459:  Train loss: 3.3942670822143555  Test loss: 3.424398422241211 \n",
      "Epoch: 1/10:  mini-batch 2523/4459:  Train loss: 3.3629167079925537  Test loss: 3.4238791465759277 \n",
      "Epoch: 1/10:  mini-batch 2524/4459:  Train loss: 3.2790963649749756  Test loss: 3.4230597019195557 \n",
      "Epoch: 1/10:  mini-batch 2525/4459:  Train loss: 3.2499403953552246  Test loss: 3.4217958450317383 \n",
      "Epoch: 1/10:  mini-batch 2526/4459:  Train loss: 3.233053684234619  Test loss: 3.420832633972168 \n",
      "Epoch: 1/10:  mini-batch 2527/4459:  Train loss: 3.1922671794891357  Test loss: 3.4200668334960938 \n",
      "Epoch: 1/10:  mini-batch 2528/4459:  Train loss: 3.280653476715088  Test loss: 3.4197998046875 \n",
      "Epoch: 1/10:  mini-batch 2529/4459:  Train loss: 3.267179012298584  Test loss: 3.4194207191467285 \n",
      "Epoch: 1/10:  mini-batch 2530/4459:  Train loss: 3.2468278408050537  Test loss: 3.419495105743408 \n",
      "Epoch: 1/10:  mini-batch 2531/4459:  Train loss: 3.2904739379882812  Test loss: 3.4195058345794678 \n",
      "Epoch: 1/10:  mini-batch 2532/4459:  Train loss: 3.2117435932159424  Test loss: 3.4194209575653076 \n",
      "Epoch: 1/10:  mini-batch 2533/4459:  Train loss: 3.582751750946045  Test loss: 3.4191207885742188 \n",
      "Epoch: 1/10:  mini-batch 2534/4459:  Train loss: 3.498500347137451  Test loss: 3.418877601623535 \n",
      "Epoch: 1/10:  mini-batch 2535/4459:  Train loss: 3.5361785888671875  Test loss: 3.4187607765197754 \n",
      "Epoch: 1/10:  mini-batch 2536/4459:  Train loss: 3.2531540393829346  Test loss: 3.418656826019287 \n",
      "Epoch: 1/10:  mini-batch 2537/4459:  Train loss: 3.335631847381592  Test loss: 3.4188151359558105 \n",
      "Epoch: 1/10:  mini-batch 2538/4459:  Train loss: 3.588794708251953  Test loss: 3.4184181690216064 \n",
      "Epoch: 1/10:  mini-batch 2539/4459:  Train loss: 3.140317916870117  Test loss: 3.4179539680480957 \n",
      "Epoch: 1/10:  mini-batch 2540/4459:  Train loss: 3.4126343727111816  Test loss: 3.4172463417053223 \n",
      "Epoch: 1/10:  mini-batch 2541/4459:  Train loss: 3.636645793914795  Test loss: 3.416360378265381 \n",
      "Epoch: 1/10:  mini-batch 2542/4459:  Train loss: 3.073035717010498  Test loss: 3.415760040283203 \n",
      "Epoch: 1/10:  mini-batch 2543/4459:  Train loss: 3.3336005210876465  Test loss: 3.4153318405151367 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2544/4459:  Train loss: 3.20994234085083  Test loss: 3.414905548095703 \n",
      "Epoch: 1/10:  mini-batch 2545/4459:  Train loss: 3.394371271133423  Test loss: 3.4148998260498047 \n",
      "Epoch: 1/10:  mini-batch 2546/4459:  Train loss: 3.121065855026245  Test loss: 3.41534686088562 \n",
      "Epoch: 1/10:  mini-batch 2547/4459:  Train loss: 3.2961854934692383  Test loss: 3.4160680770874023 \n",
      "Epoch: 1/10:  mini-batch 2548/4459:  Train loss: 3.6799964904785156  Test loss: 3.4164390563964844 \n",
      "Epoch: 1/10:  mini-batch 2549/4459:  Train loss: 3.6339774131774902  Test loss: 3.416835308074951 \n",
      "Epoch: 1/10:  mini-batch 2550/4459:  Train loss: 3.334155321121216  Test loss: 3.4173583984375 \n",
      "Epoch: 1/10:  mini-batch 2551/4459:  Train loss: 3.0240659713745117  Test loss: 3.417750120162964 \n",
      "Epoch: 1/10:  mini-batch 2552/4459:  Train loss: 3.2738540172576904  Test loss: 3.418492555618286 \n",
      "Epoch: 1/10:  mini-batch 2553/4459:  Train loss: 3.2324650287628174  Test loss: 3.4192163944244385 \n",
      "Epoch: 1/10:  mini-batch 2554/4459:  Train loss: 2.934638023376465  Test loss: 3.4204187393188477 \n",
      "Epoch: 1/10:  mini-batch 2555/4459:  Train loss: 3.016244888305664  Test loss: 3.421541213989258 \n",
      "Epoch: 1/10:  mini-batch 2556/4459:  Train loss: 3.230173110961914  Test loss: 3.422976493835449 \n",
      "Epoch: 1/10:  mini-batch 2557/4459:  Train loss: 3.1659398078918457  Test loss: 3.4244439601898193 \n",
      "Epoch: 1/10:  mini-batch 2558/4459:  Train loss: 3.474459409713745  Test loss: 3.425401210784912 \n",
      "Epoch: 1/10:  mini-batch 2559/4459:  Train loss: 3.128859519958496  Test loss: 3.426635980606079 \n",
      "Epoch: 1/10:  mini-batch 2560/4459:  Train loss: 3.475184917449951  Test loss: 3.4272079467773438 \n",
      "Epoch: 1/10:  mini-batch 2561/4459:  Train loss: 3.498659133911133  Test loss: 3.427163600921631 \n",
      "Epoch: 1/10:  mini-batch 2562/4459:  Train loss: 3.374835968017578  Test loss: 3.427686929702759 \n",
      "Epoch: 1/10:  mini-batch 2563/4459:  Train loss: 3.3549325466156006  Test loss: 3.4278576374053955 \n",
      "Epoch: 1/10:  mini-batch 2564/4459:  Train loss: 3.818716526031494  Test loss: 3.427159070968628 \n",
      "Epoch: 1/10:  mini-batch 2565/4459:  Train loss: 3.3877251148223877  Test loss: 3.426659107208252 \n",
      "Epoch: 1/10:  mini-batch 2566/4459:  Train loss: 3.108430862426758  Test loss: 3.426757574081421 \n",
      "Epoch: 1/10:  mini-batch 2567/4459:  Train loss: 3.2547168731689453  Test loss: 3.427180290222168 \n",
      "Epoch: 1/10:  mini-batch 2568/4459:  Train loss: 3.302194833755493  Test loss: 3.4277071952819824 \n",
      "Epoch: 1/10:  mini-batch 2569/4459:  Train loss: 3.6291277408599854  Test loss: 3.427899122238159 \n",
      "Epoch: 1/10:  mini-batch 2570/4459:  Train loss: 3.422374725341797  Test loss: 3.427903175354004 \n",
      "Epoch: 1/10:  mini-batch 2571/4459:  Train loss: 3.433570384979248  Test loss: 3.4279184341430664 \n",
      "Epoch: 1/10:  mini-batch 2572/4459:  Train loss: 3.534641981124878  Test loss: 3.4271903038024902 \n",
      "Epoch: 1/10:  mini-batch 2573/4459:  Train loss: 3.2978804111480713  Test loss: 3.426386594772339 \n",
      "Epoch: 1/10:  mini-batch 2574/4459:  Train loss: 3.1618216037750244  Test loss: 3.425727128982544 \n",
      "Epoch: 1/10:  mini-batch 2575/4459:  Train loss: 3.5155251026153564  Test loss: 3.4247548580169678 \n",
      "Epoch: 1/10:  mini-batch 2576/4459:  Train loss: 3.06231689453125  Test loss: 3.4241549968719482 \n",
      "Epoch: 1/10:  mini-batch 2577/4459:  Train loss: 3.3181893825531006  Test loss: 3.423755645751953 \n",
      "Epoch: 1/10:  mini-batch 2578/4459:  Train loss: 3.72186541557312  Test loss: 3.4228932857513428 \n",
      "Epoch: 1/10:  mini-batch 2579/4459:  Train loss: 3.140517473220825  Test loss: 3.4220778942108154 \n",
      "Epoch: 1/10:  mini-batch 2580/4459:  Train loss: 3.1107349395751953  Test loss: 3.4215731620788574 \n",
      "Epoch: 1/10:  mini-batch 2581/4459:  Train loss: 3.280435562133789  Test loss: 3.420830488204956 \n",
      "Epoch: 1/10:  mini-batch 2582/4459:  Train loss: 3.2113332748413086  Test loss: 3.4207966327667236 \n",
      "Epoch: 1/10:  mini-batch 2583/4459:  Train loss: 3.070340633392334  Test loss: 3.420748710632324 \n",
      "Epoch: 1/10:  mini-batch 2584/4459:  Train loss: 3.2109813690185547  Test loss: 3.420501470565796 \n",
      "Epoch: 1/10:  mini-batch 2585/4459:  Train loss: 3.67431902885437  Test loss: 3.4201507568359375 \n",
      "Epoch: 1/10:  mini-batch 2586/4459:  Train loss: 3.7066171169281006  Test loss: 3.4194021224975586 \n",
      "Epoch: 1/10:  mini-batch 2587/4459:  Train loss: 3.079562187194824  Test loss: 3.4189066886901855 \n",
      "Epoch: 1/10:  mini-batch 2588/4459:  Train loss: 3.24648380279541  Test loss: 3.4181127548217773 \n",
      "Epoch: 1/10:  mini-batch 2589/4459:  Train loss: 3.697538375854492  Test loss: 3.416933059692383 \n",
      "Epoch: 1/10:  mini-batch 2590/4459:  Train loss: 3.5927295684814453  Test loss: 3.415639877319336 \n",
      "Epoch: 1/10:  mini-batch 2591/4459:  Train loss: 3.5775375366210938  Test loss: 3.4143457412719727 \n",
      "Epoch: 1/10:  mini-batch 2592/4459:  Train loss: 3.306562900543213  Test loss: 3.4132511615753174 \n",
      "Epoch: 1/10:  mini-batch 2593/4459:  Train loss: 3.460447311401367  Test loss: 3.412196397781372 \n",
      "Epoch: 1/10:  mini-batch 2594/4459:  Train loss: 3.354456663131714  Test loss: 3.4113166332244873 \n",
      "Epoch: 1/10:  mini-batch 2595/4459:  Train loss: 3.469602584838867  Test loss: 3.410324811935425 \n",
      "Epoch: 1/10:  mini-batch 2596/4459:  Train loss: 3.470970392227173  Test loss: 3.4094362258911133 \n",
      "Epoch: 1/10:  mini-batch 2597/4459:  Train loss: 3.4405136108398438  Test loss: 3.408780574798584 \n",
      "Epoch: 1/10:  mini-batch 2598/4459:  Train loss: 3.198291301727295  Test loss: 3.4082071781158447 \n",
      "Epoch: 1/10:  mini-batch 2599/4459:  Train loss: 3.478423595428467  Test loss: 3.407576322555542 \n",
      "Epoch: 1/10:  mini-batch 2600/4459:  Train loss: 3.3091087341308594  Test loss: 3.407228469848633 \n",
      "Epoch: 1/10:  mini-batch 2601/4459:  Train loss: 3.3367669582366943  Test loss: 3.407151937484741 \n",
      "Epoch: 1/10:  mini-batch 2602/4459:  Train loss: 3.106241226196289  Test loss: 3.4074277877807617 \n",
      "Epoch: 1/10:  mini-batch 2603/4459:  Train loss: 3.0485477447509766  Test loss: 3.4075512886047363 \n",
      "Epoch: 1/10:  mini-batch 2604/4459:  Train loss: 3.5386459827423096  Test loss: 3.4077870845794678 \n",
      "Epoch: 1/10:  mini-batch 2605/4459:  Train loss: 3.410722494125366  Test loss: 3.4078643321990967 \n",
      "Epoch: 1/10:  mini-batch 2606/4459:  Train loss: 3.4557695388793945  Test loss: 3.4082584381103516 \n",
      "Epoch: 1/10:  mini-batch 2607/4459:  Train loss: 3.046980857849121  Test loss: 3.4088821411132812 \n",
      "Epoch: 1/10:  mini-batch 2608/4459:  Train loss: 3.2517385482788086  Test loss: 3.4095256328582764 \n",
      "Epoch: 1/10:  mini-batch 2609/4459:  Train loss: 3.050666332244873  Test loss: 3.4100162982940674 \n",
      "Epoch: 1/10:  mini-batch 2610/4459:  Train loss: 2.993889331817627  Test loss: 3.410506248474121 \n",
      "Epoch: 1/10:  mini-batch 2611/4459:  Train loss: 3.568582534790039  Test loss: 3.411165714263916 \n",
      "Epoch: 1/10:  mini-batch 2612/4459:  Train loss: 3.3120198249816895  Test loss: 3.4118223190307617 \n",
      "Epoch: 1/10:  mini-batch 2613/4459:  Train loss: 3.303025245666504  Test loss: 3.4123764038085938 \n",
      "Epoch: 1/10:  mini-batch 2614/4459:  Train loss: 3.206789016723633  Test loss: 3.4129741191864014 \n",
      "Epoch: 1/10:  mini-batch 2615/4459:  Train loss: 3.181643486022949  Test loss: 3.413632392883301 \n",
      "Epoch: 1/10:  mini-batch 2616/4459:  Train loss: 3.67771053314209  Test loss: 3.4138598442077637 \n",
      "Epoch: 1/10:  mini-batch 2617/4459:  Train loss: 3.3569464683532715  Test loss: 3.4142398834228516 \n",
      "Epoch: 1/10:  mini-batch 2618/4459:  Train loss: 3.2396328449249268  Test loss: 3.4146065711975098 \n",
      "Epoch: 1/10:  mini-batch 2619/4459:  Train loss: 3.23683762550354  Test loss: 3.4146475791931152 \n",
      "Epoch: 1/10:  mini-batch 2620/4459:  Train loss: 3.410264015197754  Test loss: 3.4145612716674805 \n",
      "Epoch: 1/10:  mini-batch 2621/4459:  Train loss: 3.328080415725708  Test loss: 3.414574146270752 \n",
      "Epoch: 1/10:  mini-batch 2622/4459:  Train loss: 3.2061474323272705  Test loss: 3.414433479309082 \n",
      "Epoch: 1/10:  mini-batch 2623/4459:  Train loss: 3.692492723464966  Test loss: 3.4141550064086914 \n",
      "Epoch: 1/10:  mini-batch 2624/4459:  Train loss: 3.4144721031188965  Test loss: 3.4139060974121094 \n",
      "Epoch: 1/10:  mini-batch 2625/4459:  Train loss: 3.487731456756592  Test loss: 3.4129114151000977 \n",
      "Epoch: 1/10:  mini-batch 2626/4459:  Train loss: 3.33730411529541  Test loss: 3.4118247032165527 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2627/4459:  Train loss: 3.3072986602783203  Test loss: 3.410764694213867 \n",
      "Epoch: 1/10:  mini-batch 2628/4459:  Train loss: 3.376990795135498  Test loss: 3.4096310138702393 \n",
      "Epoch: 1/10:  mini-batch 2629/4459:  Train loss: 3.0258560180664062  Test loss: 3.408588171005249 \n",
      "Epoch: 1/10:  mini-batch 2630/4459:  Train loss: 3.1943612098693848  Test loss: 3.4079160690307617 \n",
      "Epoch: 1/10:  mini-batch 2631/4459:  Train loss: 3.338965892791748  Test loss: 3.407268762588501 \n",
      "Epoch: 1/10:  mini-batch 2632/4459:  Train loss: 3.1359353065490723  Test loss: 3.4071192741394043 \n",
      "Epoch: 1/10:  mini-batch 2633/4459:  Train loss: 3.0390286445617676  Test loss: 3.407182216644287 \n",
      "Epoch: 1/10:  mini-batch 2634/4459:  Train loss: 3.541231870651245  Test loss: 3.40704607963562 \n",
      "Epoch: 1/10:  mini-batch 2635/4459:  Train loss: 3.5402138233184814  Test loss: 3.406975746154785 \n",
      "Epoch: 1/10:  mini-batch 2636/4459:  Train loss: 2.957566022872925  Test loss: 3.407573938369751 \n",
      "Epoch: 1/10:  mini-batch 2637/4459:  Train loss: 3.7596006393432617  Test loss: 3.407505750656128 \n",
      "Epoch: 1/10:  mini-batch 2638/4459:  Train loss: 3.6273133754730225  Test loss: 3.4069604873657227 \n",
      "Epoch: 1/10:  mini-batch 2639/4459:  Train loss: 3.326488971710205  Test loss: 3.40659499168396 \n",
      "Epoch: 1/10:  mini-batch 2640/4459:  Train loss: 3.2332425117492676  Test loss: 3.4061968326568604 \n",
      "Epoch: 1/10:  mini-batch 2641/4459:  Train loss: 2.9045333862304688  Test loss: 3.4061694145202637 \n",
      "Epoch: 1/10:  mini-batch 2642/4459:  Train loss: 3.4056878089904785  Test loss: 3.405933380126953 \n",
      "Epoch: 1/10:  mini-batch 2643/4459:  Train loss: 3.54270076751709  Test loss: 3.4053916931152344 \n",
      "Epoch: 1/10:  mini-batch 2644/4459:  Train loss: 3.5000240802764893  Test loss: 3.4046173095703125 \n",
      "Epoch: 1/10:  mini-batch 2645/4459:  Train loss: 3.49252986907959  Test loss: 3.4035961627960205 \n",
      "Epoch: 1/10:  mini-batch 2646/4459:  Train loss: 3.540822744369507  Test loss: 3.4027161598205566 \n",
      "Epoch: 1/10:  mini-batch 2647/4459:  Train loss: 3.5664567947387695  Test loss: 3.4019055366516113 \n",
      "Epoch: 1/10:  mini-batch 2648/4459:  Train loss: 3.310838222503662  Test loss: 3.4014158248901367 \n",
      "Epoch: 1/10:  mini-batch 2649/4459:  Train loss: 3.133577585220337  Test loss: 3.400908946990967 \n",
      "Epoch: 1/10:  mini-batch 2650/4459:  Train loss: 3.1677119731903076  Test loss: 3.4004623889923096 \n",
      "Epoch: 1/10:  mini-batch 2651/4459:  Train loss: 3.2084391117095947  Test loss: 3.400113344192505 \n",
      "Epoch: 1/10:  mini-batch 2652/4459:  Train loss: 3.2570395469665527  Test loss: 3.3994412422180176 \n",
      "Epoch: 1/10:  mini-batch 2653/4459:  Train loss: 3.473361015319824  Test loss: 3.399017810821533 \n",
      "Epoch: 1/10:  mini-batch 2654/4459:  Train loss: 3.7556095123291016  Test loss: 3.3986058235168457 \n",
      "Epoch: 1/10:  mini-batch 2655/4459:  Train loss: 3.0287375450134277  Test loss: 3.3982601165771484 \n",
      "Epoch: 1/10:  mini-batch 2656/4459:  Train loss: 2.9744505882263184  Test loss: 3.3984789848327637 \n",
      "Epoch: 1/10:  mini-batch 2657/4459:  Train loss: 2.985198497772217  Test loss: 3.3984227180480957 \n",
      "Epoch: 1/10:  mini-batch 2658/4459:  Train loss: 2.8917734622955322  Test loss: 3.398777484893799 \n",
      "Epoch: 1/10:  mini-batch 2659/4459:  Train loss: 3.3400816917419434  Test loss: 3.3988513946533203 \n",
      "Epoch: 1/10:  mini-batch 2660/4459:  Train loss: 3.5920376777648926  Test loss: 3.3985977172851562 \n",
      "Epoch: 1/10:  mini-batch 2661/4459:  Train loss: 3.2467856407165527  Test loss: 3.398834466934204 \n",
      "Epoch: 1/10:  mini-batch 2662/4459:  Train loss: 3.3882498741149902  Test loss: 3.3992228507995605 \n",
      "Epoch: 1/10:  mini-batch 2663/4459:  Train loss: 2.9468512535095215  Test loss: 3.400526523590088 \n",
      "Epoch: 1/10:  mini-batch 2664/4459:  Train loss: 3.2064976692199707  Test loss: 3.401318073272705 \n",
      "Epoch: 1/10:  mini-batch 2665/4459:  Train loss: 3.2538485527038574  Test loss: 3.402985095977783 \n",
      "Epoch: 1/10:  mini-batch 2666/4459:  Train loss: 3.273958921432495  Test loss: 3.4047560691833496 \n",
      "Epoch: 1/10:  mini-batch 2667/4459:  Train loss: 3.11384654045105  Test loss: 3.407050848007202 \n",
      "Epoch: 1/10:  mini-batch 2668/4459:  Train loss: 2.8141565322875977  Test loss: 3.410153388977051 \n",
      "Epoch: 1/10:  mini-batch 2669/4459:  Train loss: 3.36663818359375  Test loss: 3.412853479385376 \n",
      "Epoch: 1/10:  mini-batch 2670/4459:  Train loss: 3.568695545196533  Test loss: 3.415513038635254 \n",
      "Epoch: 1/10:  mini-batch 2671/4459:  Train loss: 2.943305730819702  Test loss: 3.418443202972412 \n",
      "Epoch: 1/10:  mini-batch 2672/4459:  Train loss: 3.6082816123962402  Test loss: 3.420198440551758 \n",
      "Epoch: 1/10:  mini-batch 2673/4459:  Train loss: 3.518324851989746  Test loss: 3.421473741531372 \n",
      "Epoch: 1/10:  mini-batch 2674/4459:  Train loss: 2.832752227783203  Test loss: 3.4234156608581543 \n",
      "Epoch: 1/10:  mini-batch 2675/4459:  Train loss: 3.0564441680908203  Test loss: 3.4256348609924316 \n",
      "Epoch: 1/10:  mini-batch 2676/4459:  Train loss: 3.500488758087158  Test loss: 3.427353858947754 \n",
      "Epoch: 1/10:  mini-batch 2677/4459:  Train loss: 3.5255327224731445  Test loss: 3.4278697967529297 \n",
      "Epoch: 1/10:  mini-batch 2678/4459:  Train loss: 3.478220224380493  Test loss: 3.4282140731811523 \n",
      "Epoch: 1/10:  mini-batch 2679/4459:  Train loss: 3.0411343574523926  Test loss: 3.4289653301239014 \n",
      "Epoch: 1/10:  mini-batch 2680/4459:  Train loss: 3.0399370193481445  Test loss: 3.4298510551452637 \n",
      "Epoch: 1/10:  mini-batch 2681/4459:  Train loss: 3.4711132049560547  Test loss: 3.4299774169921875 \n",
      "Epoch: 1/10:  mini-batch 2682/4459:  Train loss: 3.65285587310791  Test loss: 3.4292819499969482 \n",
      "Epoch: 1/10:  mini-batch 2683/4459:  Train loss: 3.066296339035034  Test loss: 3.4292235374450684 \n",
      "Epoch: 1/10:  mini-batch 2684/4459:  Train loss: 3.427459716796875  Test loss: 3.428736686706543 \n",
      "Epoch: 1/10:  mini-batch 2685/4459:  Train loss: 3.212547779083252  Test loss: 3.42893385887146 \n",
      "Epoch: 1/10:  mini-batch 2686/4459:  Train loss: 3.281493663787842  Test loss: 3.4286577701568604 \n",
      "Epoch: 1/10:  mini-batch 2687/4459:  Train loss: 3.446302652359009  Test loss: 3.4271438121795654 \n",
      "Epoch: 1/10:  mini-batch 2688/4459:  Train loss: 3.7092294692993164  Test loss: 3.424717426300049 \n",
      "Epoch: 1/10:  mini-batch 2689/4459:  Train loss: 3.652402400970459  Test loss: 3.4215617179870605 \n",
      "Epoch: 1/10:  mini-batch 2690/4459:  Train loss: 3.3452725410461426  Test loss: 3.418661594390869 \n",
      "Epoch: 1/10:  mini-batch 2691/4459:  Train loss: 3.386849880218506  Test loss: 3.4160799980163574 \n",
      "Epoch: 1/10:  mini-batch 2692/4459:  Train loss: 3.3744606971740723  Test loss: 3.414011240005493 \n",
      "Epoch: 1/10:  mini-batch 2693/4459:  Train loss: 3.4760804176330566  Test loss: 3.4134650230407715 \n",
      "Epoch: 1/10:  mini-batch 2694/4459:  Train loss: 3.160719633102417  Test loss: 3.413726568222046 \n",
      "Epoch: 1/10:  mini-batch 2695/4459:  Train loss: 3.2732553482055664  Test loss: 3.4145193099975586 \n",
      "Epoch: 1/10:  mini-batch 2696/4459:  Train loss: 3.2514216899871826  Test loss: 3.415529251098633 \n",
      "Epoch: 1/10:  mini-batch 2697/4459:  Train loss: 3.684030294418335  Test loss: 3.4161365032196045 \n",
      "Epoch: 1/10:  mini-batch 2698/4459:  Train loss: 3.3437612056732178  Test loss: 3.4167277812957764 \n",
      "Epoch: 1/10:  mini-batch 2699/4459:  Train loss: 3.2444558143615723  Test loss: 3.4172115325927734 \n",
      "Epoch: 1/10:  mini-batch 2700/4459:  Train loss: 3.5112133026123047  Test loss: 3.41819429397583 \n",
      "Epoch: 1/10:  mini-batch 2701/4459:  Train loss: 3.510488271713257  Test loss: 3.41872501373291 \n",
      "Epoch: 1/10:  mini-batch 2702/4459:  Train loss: 3.102118730545044  Test loss: 3.419560432434082 \n",
      "Epoch: 1/10:  mini-batch 2703/4459:  Train loss: 3.396859645843506  Test loss: 3.420382022857666 \n",
      "Epoch: 1/10:  mini-batch 2704/4459:  Train loss: 2.9867911338806152  Test loss: 3.4216198921203613 \n",
      "Epoch: 1/10:  mini-batch 2705/4459:  Train loss: 3.1366114616394043  Test loss: 3.4228549003601074 \n",
      "Epoch: 1/10:  mini-batch 2706/4459:  Train loss: 3.2995426654815674  Test loss: 3.4242405891418457 \n",
      "Epoch: 1/10:  mini-batch 2707/4459:  Train loss: 3.2756495475769043  Test loss: 3.425377607345581 \n",
      "Epoch: 1/10:  mini-batch 2708/4459:  Train loss: 3.3148860931396484  Test loss: 3.4264073371887207 \n",
      "Epoch: 1/10:  mini-batch 2709/4459:  Train loss: 3.1027119159698486  Test loss: 3.4278178215026855 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2710/4459:  Train loss: 3.479790210723877  Test loss: 3.428720712661743 \n",
      "Epoch: 1/10:  mini-batch 2711/4459:  Train loss: 3.41454815864563  Test loss: 3.429497003555298 \n",
      "Epoch: 1/10:  mini-batch 2712/4459:  Train loss: 3.3009047508239746  Test loss: 3.429959297180176 \n",
      "Epoch: 1/10:  mini-batch 2713/4459:  Train loss: 3.082293748855591  Test loss: 3.4305450916290283 \n",
      "Epoch: 1/10:  mini-batch 2714/4459:  Train loss: 2.853724479675293  Test loss: 3.431549072265625 \n",
      "Epoch: 1/10:  mini-batch 2715/4459:  Train loss: 3.114959716796875  Test loss: 3.4327521324157715 \n",
      "Epoch: 1/10:  mini-batch 2716/4459:  Train loss: 3.0169084072113037  Test loss: 3.4344968795776367 \n",
      "Epoch: 1/10:  mini-batch 2717/4459:  Train loss: 3.523226499557495  Test loss: 3.43601393699646 \n",
      "Epoch: 1/10:  mini-batch 2718/4459:  Train loss: 3.485105037689209  Test loss: 3.4374840259552 \n",
      "Epoch: 1/10:  mini-batch 2719/4459:  Train loss: 3.382331371307373  Test loss: 3.4388515949249268 \n",
      "Epoch: 1/10:  mini-batch 2720/4459:  Train loss: 3.1966052055358887  Test loss: 3.440504312515259 \n",
      "Epoch: 1/10:  mini-batch 2721/4459:  Train loss: 3.1856696605682373  Test loss: 3.4421558380126953 \n",
      "Epoch: 1/10:  mini-batch 2722/4459:  Train loss: 3.3931093215942383  Test loss: 3.443185567855835 \n",
      "Epoch: 1/10:  mini-batch 2723/4459:  Train loss: 3.112840414047241  Test loss: 3.4443929195404053 \n",
      "Epoch: 1/10:  mini-batch 2724/4459:  Train loss: 3.389589309692383  Test loss: 3.4454636573791504 \n",
      "Epoch: 1/10:  mini-batch 2725/4459:  Train loss: 3.7454535961151123  Test loss: 3.4452438354492188 \n",
      "Epoch: 1/10:  mini-batch 2726/4459:  Train loss: 3.9371390342712402  Test loss: 3.4437172412872314 \n",
      "Epoch: 1/10:  mini-batch 2727/4459:  Train loss: 3.3476760387420654  Test loss: 3.4423272609710693 \n",
      "Epoch: 1/10:  mini-batch 2728/4459:  Train loss: 3.2231621742248535  Test loss: 3.440871238708496 \n",
      "Epoch: 1/10:  mini-batch 2729/4459:  Train loss: 3.307398796081543  Test loss: 3.439500331878662 \n",
      "Epoch: 1/10:  mini-batch 2730/4459:  Train loss: 3.1758790016174316  Test loss: 3.4386403560638428 \n",
      "Epoch: 1/10:  mini-batch 2731/4459:  Train loss: 3.34537410736084  Test loss: 3.4379796981811523 \n",
      "Epoch: 1/10:  mini-batch 2732/4459:  Train loss: 3.0725553035736084  Test loss: 3.4376585483551025 \n",
      "Epoch: 1/10:  mini-batch 2733/4459:  Train loss: 3.738665819168091  Test loss: 3.436140775680542 \n",
      "Epoch: 1/10:  mini-batch 2734/4459:  Train loss: 3.5986709594726562  Test loss: 3.434121608734131 \n",
      "Epoch: 1/10:  mini-batch 2735/4459:  Train loss: 3.144432783126831  Test loss: 3.4323320388793945 \n",
      "Epoch: 1/10:  mini-batch 2736/4459:  Train loss: 2.793174982070923  Test loss: 3.4317312240600586 \n",
      "Epoch: 1/10:  mini-batch 2737/4459:  Train loss: 2.864969491958618  Test loss: 3.4312682151794434 \n",
      "Epoch: 1/10:  mini-batch 2738/4459:  Train loss: 3.4166617393493652  Test loss: 3.4305508136749268 \n",
      "Epoch: 1/10:  mini-batch 2739/4459:  Train loss: 2.9412946701049805  Test loss: 3.4305386543273926 \n",
      "Epoch: 1/10:  mini-batch 2740/4459:  Train loss: 2.9312844276428223  Test loss: 3.4310860633850098 \n",
      "Epoch: 1/10:  mini-batch 2741/4459:  Train loss: 3.4377942085266113  Test loss: 3.4310619831085205 \n",
      "Epoch: 1/10:  mini-batch 2742/4459:  Train loss: 3.2094321250915527  Test loss: 3.430629253387451 \n",
      "Epoch: 1/10:  mini-batch 2743/4459:  Train loss: 3.290745735168457  Test loss: 3.430479049682617 \n",
      "Epoch: 1/10:  mini-batch 2744/4459:  Train loss: 3.356419563293457  Test loss: 3.429935932159424 \n",
      "Epoch: 1/10:  mini-batch 2745/4459:  Train loss: 3.3287229537963867  Test loss: 3.4296584129333496 \n",
      "Epoch: 1/10:  mini-batch 2746/4459:  Train loss: 3.2688658237457275  Test loss: 3.429107666015625 \n",
      "Epoch: 1/10:  mini-batch 2747/4459:  Train loss: 3.4562511444091797  Test loss: 3.4281392097473145 \n",
      "Epoch: 1/10:  mini-batch 2748/4459:  Train loss: 3.862596273422241  Test loss: 3.4259419441223145 \n",
      "Epoch: 1/10:  mini-batch 2749/4459:  Train loss: 3.4392402172088623  Test loss: 3.4235758781433105 \n",
      "Epoch: 1/10:  mini-batch 2750/4459:  Train loss: 3.5172746181488037  Test loss: 3.421191453933716 \n",
      "Epoch: 1/10:  mini-batch 2751/4459:  Train loss: 3.4597771167755127  Test loss: 3.4187545776367188 \n",
      "Epoch: 1/10:  mini-batch 2752/4459:  Train loss: 3.3030624389648438  Test loss: 3.416100025177002 \n",
      "Epoch: 1/10:  mini-batch 2753/4459:  Train loss: 3.071075677871704  Test loss: 3.4144582748413086 \n",
      "Epoch: 1/10:  mini-batch 2754/4459:  Train loss: 3.3096020221710205  Test loss: 3.4133191108703613 \n",
      "Epoch: 1/10:  mini-batch 2755/4459:  Train loss: 3.0311131477355957  Test loss: 3.4127073287963867 \n",
      "Epoch: 1/10:  mini-batch 2756/4459:  Train loss: 3.549323797225952  Test loss: 3.41165828704834 \n",
      "Epoch: 1/10:  mini-batch 2757/4459:  Train loss: 3.004117727279663  Test loss: 3.411221981048584 \n",
      "Epoch: 1/10:  mini-batch 2758/4459:  Train loss: 3.0341782569885254  Test loss: 3.410529136657715 \n",
      "Epoch: 1/10:  mini-batch 2759/4459:  Train loss: 3.7292773723602295  Test loss: 3.409379482269287 \n",
      "Epoch: 1/10:  mini-batch 2760/4459:  Train loss: 3.704606294631958  Test loss: 3.408182144165039 \n",
      "Epoch: 1/10:  mini-batch 2761/4459:  Train loss: 3.2370810508728027  Test loss: 3.4073963165283203 \n",
      "Epoch: 1/10:  mini-batch 2762/4459:  Train loss: 3.0319461822509766  Test loss: 3.406770706176758 \n",
      "Epoch: 1/10:  mini-batch 2763/4459:  Train loss: 3.5226335525512695  Test loss: 3.4062342643737793 \n",
      "Epoch: 1/10:  mini-batch 2764/4459:  Train loss: 3.0393171310424805  Test loss: 3.406395673751831 \n",
      "Epoch: 1/10:  mini-batch 2765/4459:  Train loss: 3.365360736846924  Test loss: 3.406290054321289 \n",
      "Epoch: 1/10:  mini-batch 2766/4459:  Train loss: 3.3996458053588867  Test loss: 3.406266212463379 \n",
      "Epoch: 1/10:  mini-batch 2767/4459:  Train loss: 3.4394209384918213  Test loss: 3.406388282775879 \n",
      "Epoch: 1/10:  mini-batch 2768/4459:  Train loss: 3.484282970428467  Test loss: 3.4064979553222656 \n",
      "Epoch: 1/10:  mini-batch 2769/4459:  Train loss: 3.4568936824798584  Test loss: 3.406635046005249 \n",
      "Epoch: 1/10:  mini-batch 2770/4459:  Train loss: 3.196866035461426  Test loss: 3.4062488079071045 \n",
      "Epoch: 1/10:  mini-batch 2771/4459:  Train loss: 2.942145347595215  Test loss: 3.4065113067626953 \n",
      "Epoch: 1/10:  mini-batch 2772/4459:  Train loss: 3.0764963626861572  Test loss: 3.407179832458496 \n",
      "Epoch: 1/10:  mini-batch 2773/4459:  Train loss: 3.419254779815674  Test loss: 3.4076786041259766 \n",
      "Epoch: 1/10:  mini-batch 2774/4459:  Train loss: 3.3463573455810547  Test loss: 3.408323287963867 \n",
      "Epoch: 1/10:  mini-batch 2775/4459:  Train loss: 3.119793653488159  Test loss: 3.4092555046081543 \n",
      "Epoch: 1/10:  mini-batch 2776/4459:  Train loss: 2.9786040782928467  Test loss: 3.410707712173462 \n",
      "Epoch: 1/10:  mini-batch 2777/4459:  Train loss: 3.0670416355133057  Test loss: 3.412198543548584 \n",
      "Epoch: 1/10:  mini-batch 2778/4459:  Train loss: 3.406059503555298  Test loss: 3.4134929180145264 \n",
      "Epoch: 1/10:  mini-batch 2779/4459:  Train loss: 3.18589448928833  Test loss: 3.414858341217041 \n",
      "Epoch: 1/10:  mini-batch 2780/4459:  Train loss: 2.678313732147217  Test loss: 3.4174232482910156 \n",
      "Epoch: 1/10:  mini-batch 2781/4459:  Train loss: 3.588747978210449  Test loss: 3.419386863708496 \n",
      "Epoch: 1/10:  mini-batch 2782/4459:  Train loss: 3.6554155349731445  Test loss: 3.4210519790649414 \n",
      "Epoch: 1/10:  mini-batch 2783/4459:  Train loss: 3.247927665710449  Test loss: 3.4225997924804688 \n",
      "Epoch: 1/10:  mini-batch 2784/4459:  Train loss: 3.7892251014709473  Test loss: 3.4232709407806396 \n",
      "Epoch: 1/10:  mini-batch 2785/4459:  Train loss: 3.605713367462158  Test loss: 3.423553466796875 \n",
      "Epoch: 1/10:  mini-batch 2786/4459:  Train loss: 3.110222339630127  Test loss: 3.4240036010742188 \n",
      "Epoch: 1/10:  mini-batch 2787/4459:  Train loss: 3.12849760055542  Test loss: 3.4245033264160156 \n",
      "Epoch: 1/10:  mini-batch 2788/4459:  Train loss: 2.973367929458618  Test loss: 3.4248411655426025 \n",
      "Epoch: 1/10:  mini-batch 2789/4459:  Train loss: 3.5416038036346436  Test loss: 3.424899101257324 \n",
      "Epoch: 1/10:  mini-batch 2790/4459:  Train loss: 3.2586402893066406  Test loss: 3.4250640869140625 \n",
      "Epoch: 1/10:  mini-batch 2791/4459:  Train loss: 3.248365879058838  Test loss: 3.4245316982269287 \n",
      "Epoch: 1/10:  mini-batch 2792/4459:  Train loss: 3.3692922592163086  Test loss: 3.423659563064575 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2793/4459:  Train loss: 3.117701768875122  Test loss: 3.423248291015625 \n",
      "Epoch: 1/10:  mini-batch 2794/4459:  Train loss: 2.6866326332092285  Test loss: 3.423983335494995 \n",
      "Epoch: 1/10:  mini-batch 2795/4459:  Train loss: 3.683034658432007  Test loss: 3.424804210662842 \n",
      "Epoch: 1/10:  mini-batch 2796/4459:  Train loss: 3.6694512367248535  Test loss: 3.424771785736084 \n",
      "Epoch: 1/10:  mini-batch 2797/4459:  Train loss: 3.196962356567383  Test loss: 3.4242727756500244 \n",
      "Epoch: 1/10:  mini-batch 2798/4459:  Train loss: 3.1902060508728027  Test loss: 3.4239208698272705 \n",
      "Epoch: 1/10:  mini-batch 2799/4459:  Train loss: 3.4873220920562744  Test loss: 3.4235281944274902 \n",
      "Epoch: 1/10:  mini-batch 2800/4459:  Train loss: 3.131650447845459  Test loss: 3.4236679077148438 \n",
      "Epoch: 1/10:  mini-batch 2801/4459:  Train loss: 3.877074956893921  Test loss: 3.4226510524749756 \n",
      "Epoch: 1/10:  mini-batch 2802/4459:  Train loss: 3.106067180633545  Test loss: 3.4217119216918945 \n",
      "Epoch: 1/10:  mini-batch 2803/4459:  Train loss: 3.6245546340942383  Test loss: 3.4207797050476074 \n",
      "Epoch: 1/10:  mini-batch 2804/4459:  Train loss: 3.2835254669189453  Test loss: 3.4200944900512695 \n",
      "Epoch: 1/10:  mini-batch 2805/4459:  Train loss: 3.2341361045837402  Test loss: 3.4192121028900146 \n",
      "Epoch: 1/10:  mini-batch 2806/4459:  Train loss: 3.20192289352417  Test loss: 3.418309450149536 \n",
      "Epoch: 1/10:  mini-batch 2807/4459:  Train loss: 3.0758891105651855  Test loss: 3.417551040649414 \n",
      "Epoch: 1/10:  mini-batch 2808/4459:  Train loss: 2.720792293548584  Test loss: 3.41825795173645 \n",
      "Epoch: 1/10:  mini-batch 2809/4459:  Train loss: 3.5309953689575195  Test loss: 3.4186980724334717 \n",
      "Epoch: 1/10:  mini-batch 2810/4459:  Train loss: 3.521230697631836  Test loss: 3.419142723083496 \n",
      "Epoch: 1/10:  mini-batch 2811/4459:  Train loss: 3.049656391143799  Test loss: 3.4200100898742676 \n",
      "Epoch: 1/10:  mini-batch 2812/4459:  Train loss: 3.380064010620117  Test loss: 3.4202053546905518 \n",
      "Epoch: 1/10:  mini-batch 2813/4459:  Train loss: 4.259885311126709  Test loss: 3.41861891746521 \n",
      "Epoch: 1/10:  mini-batch 2814/4459:  Train loss: 3.048621654510498  Test loss: 3.417314291000366 \n",
      "Epoch: 1/10:  mini-batch 2815/4459:  Train loss: 3.073732376098633  Test loss: 3.4163284301757812 \n",
      "Epoch: 1/10:  mini-batch 2816/4459:  Train loss: 3.1637725830078125  Test loss: 3.4153409004211426 \n",
      "Epoch: 1/10:  mini-batch 2817/4459:  Train loss: 3.3848190307617188  Test loss: 3.414102792739868 \n",
      "Epoch: 1/10:  mini-batch 2818/4459:  Train loss: 3.9315056800842285  Test loss: 3.4124016761779785 \n",
      "Epoch: 1/10:  mini-batch 2819/4459:  Train loss: 3.491831064224243  Test loss: 3.4107770919799805 \n",
      "Epoch: 1/10:  mini-batch 2820/4459:  Train loss: 2.968285083770752  Test loss: 3.409675359725952 \n",
      "Epoch: 1/10:  mini-batch 2821/4459:  Train loss: 3.121278762817383  Test loss: 3.408637523651123 \n",
      "Epoch: 1/10:  mini-batch 2822/4459:  Train loss: 3.2412633895874023  Test loss: 3.407576560974121 \n",
      "Epoch: 1/10:  mini-batch 2823/4459:  Train loss: 3.459519386291504  Test loss: 3.4065070152282715 \n",
      "Epoch: 1/10:  mini-batch 2824/4459:  Train loss: 3.515385150909424  Test loss: 3.4053921699523926 \n",
      "Epoch: 1/10:  mini-batch 2825/4459:  Train loss: 3.3961751461029053  Test loss: 3.403799533843994 \n",
      "Epoch: 1/10:  mini-batch 2826/4459:  Train loss: 3.2124619483947754  Test loss: 3.4023241996765137 \n",
      "Epoch: 1/10:  mini-batch 2827/4459:  Train loss: 3.397378921508789  Test loss: 3.4003427028656006 \n",
      "Epoch: 1/10:  mini-batch 2828/4459:  Train loss: 3.343264579772949  Test loss: 3.3981919288635254 \n",
      "Epoch: 1/10:  mini-batch 2829/4459:  Train loss: 3.2496933937072754  Test loss: 3.396251678466797 \n",
      "Epoch: 1/10:  mini-batch 2830/4459:  Train loss: 3.1397769451141357  Test loss: 3.3943772315979004 \n",
      "Epoch: 1/10:  mini-batch 2831/4459:  Train loss: 3.2773537635803223  Test loss: 3.3927555084228516 \n",
      "Epoch: 1/10:  mini-batch 2832/4459:  Train loss: 2.854978561401367  Test loss: 3.3917737007141113 \n",
      "Epoch: 1/10:  mini-batch 2833/4459:  Train loss: 3.5096633434295654  Test loss: 3.3912417888641357 \n",
      "Epoch: 1/10:  mini-batch 2834/4459:  Train loss: 3.775150775909424  Test loss: 3.3906524181365967 \n",
      "Epoch: 1/10:  mini-batch 2835/4459:  Train loss: 3.320972442626953  Test loss: 3.389861583709717 \n",
      "Epoch: 1/10:  mini-batch 2836/4459:  Train loss: 3.1950907707214355  Test loss: 3.3890907764434814 \n",
      "Epoch: 1/10:  mini-batch 2837/4459:  Train loss: 3.14504337310791  Test loss: 3.3887062072753906 \n",
      "Epoch: 1/10:  mini-batch 2838/4459:  Train loss: 3.456714153289795  Test loss: 3.3879234790802 \n",
      "Epoch: 1/10:  mini-batch 2839/4459:  Train loss: 3.6259031295776367  Test loss: 3.387089729309082 \n",
      "Epoch: 1/10:  mini-batch 2840/4459:  Train loss: 3.4600305557250977  Test loss: 3.3860278129577637 \n",
      "Epoch: 1/10:  mini-batch 2841/4459:  Train loss: 2.9116153717041016  Test loss: 3.385227918624878 \n",
      "Epoch: 1/10:  mini-batch 2842/4459:  Train loss: 2.982464075088501  Test loss: 3.3843867778778076 \n",
      "Epoch: 1/10:  mini-batch 2843/4459:  Train loss: 3.4351918697357178  Test loss: 3.3833982944488525 \n",
      "Epoch: 1/10:  mini-batch 2844/4459:  Train loss: 3.0580649375915527  Test loss: 3.3828225135803223 \n",
      "Epoch: 1/10:  mini-batch 2845/4459:  Train loss: 3.2921698093414307  Test loss: 3.382155418395996 \n",
      "Epoch: 1/10:  mini-batch 2846/4459:  Train loss: 3.461047887802124  Test loss: 3.3815391063690186 \n",
      "Epoch: 1/10:  mini-batch 2847/4459:  Train loss: 3.4249441623687744  Test loss: 3.3813228607177734 \n",
      "Epoch: 1/10:  mini-batch 2848/4459:  Train loss: 4.117804527282715  Test loss: 3.3804781436920166 \n",
      "Epoch: 1/10:  mini-batch 2849/4459:  Train loss: 3.5063023567199707  Test loss: 3.3798811435699463 \n",
      "Epoch: 1/10:  mini-batch 2850/4459:  Train loss: 3.0389559268951416  Test loss: 3.3794760704040527 \n",
      "Epoch: 1/10:  mini-batch 2851/4459:  Train loss: 3.1971445083618164  Test loss: 3.379027843475342 \n",
      "Epoch: 1/10:  mini-batch 2852/4459:  Train loss: 3.314938545227051  Test loss: 3.378291606903076 \n",
      "Epoch: 1/10:  mini-batch 2853/4459:  Train loss: 3.384486198425293  Test loss: 3.3778626918792725 \n",
      "Epoch: 1/10:  mini-batch 2854/4459:  Train loss: 3.074676513671875  Test loss: 3.3773560523986816 \n",
      "Epoch: 1/10:  mini-batch 2855/4459:  Train loss: 3.0714118480682373  Test loss: 3.3768057823181152 \n",
      "Epoch: 1/10:  mini-batch 2856/4459:  Train loss: 3.729844093322754  Test loss: 3.3758411407470703 \n",
      "Epoch: 1/10:  mini-batch 2857/4459:  Train loss: 3.5972232818603516  Test loss: 3.375159978866577 \n",
      "Epoch: 1/10:  mini-batch 2858/4459:  Train loss: 3.2372989654541016  Test loss: 3.374171733856201 \n",
      "Epoch: 1/10:  mini-batch 2859/4459:  Train loss: 3.3566956520080566  Test loss: 3.373170852661133 \n",
      "Epoch: 1/10:  mini-batch 2860/4459:  Train loss: 3.004833221435547  Test loss: 3.3720905780792236 \n",
      "Epoch: 1/10:  mini-batch 2861/4459:  Train loss: 3.9624249935150146  Test loss: 3.3707611560821533 \n",
      "Epoch: 1/10:  mini-batch 2862/4459:  Train loss: 3.571199893951416  Test loss: 3.3697595596313477 \n",
      "Epoch: 1/10:  mini-batch 2863/4459:  Train loss: 3.286613941192627  Test loss: 3.3689332008361816 \n",
      "Epoch: 1/10:  mini-batch 2864/4459:  Train loss: 3.1315739154815674  Test loss: 3.368494987487793 \n",
      "Epoch: 1/10:  mini-batch 2865/4459:  Train loss: 3.3974356651306152  Test loss: 3.3681020736694336 \n",
      "Epoch: 1/10:  mini-batch 2866/4459:  Train loss: 3.5092029571533203  Test loss: 3.367591142654419 \n",
      "Epoch: 1/10:  mini-batch 2867/4459:  Train loss: 3.1604251861572266  Test loss: 3.366927146911621 \n",
      "Epoch: 1/10:  mini-batch 2868/4459:  Train loss: 3.3032054901123047  Test loss: 3.366089344024658 \n",
      "Epoch: 1/10:  mini-batch 2869/4459:  Train loss: 3.041213274002075  Test loss: 3.365358352661133 \n",
      "Epoch: 1/10:  mini-batch 2870/4459:  Train loss: 3.4501848220825195  Test loss: 3.3646390438079834 \n",
      "Epoch: 1/10:  mini-batch 2871/4459:  Train loss: 3.6669485569000244  Test loss: 3.36437726020813 \n",
      "Epoch: 1/10:  mini-batch 2872/4459:  Train loss: 3.3401989936828613  Test loss: 3.3639297485351562 \n",
      "Epoch: 1/10:  mini-batch 2873/4459:  Train loss: 3.152604341506958  Test loss: 3.36367130279541 \n",
      "Epoch: 1/10:  mini-batch 2874/4459:  Train loss: 3.034670352935791  Test loss: 3.363171100616455 \n",
      "Epoch: 1/10:  mini-batch 2875/4459:  Train loss: 2.933716297149658  Test loss: 3.3625268936157227 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2876/4459:  Train loss: 3.1689321994781494  Test loss: 3.361985445022583 \n",
      "Epoch: 1/10:  mini-batch 2877/4459:  Train loss: 3.434452533721924  Test loss: 3.361595630645752 \n",
      "Epoch: 1/10:  mini-batch 2878/4459:  Train loss: 3.42974853515625  Test loss: 3.3614373207092285 \n",
      "Epoch: 1/10:  mini-batch 2879/4459:  Train loss: 3.4460251331329346  Test loss: 3.3614072799682617 \n",
      "Epoch: 1/10:  mini-batch 2880/4459:  Train loss: 3.0061540603637695  Test loss: 3.3618690967559814 \n",
      "Epoch: 1/10:  mini-batch 2881/4459:  Train loss: 3.521426200866699  Test loss: 3.3624987602233887 \n",
      "Epoch: 1/10:  mini-batch 2882/4459:  Train loss: 3.715141773223877  Test loss: 3.363457202911377 \n",
      "Epoch: 1/10:  mini-batch 2883/4459:  Train loss: 2.9775378704071045  Test loss: 3.3644847869873047 \n",
      "Epoch: 1/10:  mini-batch 2884/4459:  Train loss: 2.79087233543396  Test loss: 3.3658077716827393 \n",
      "Epoch: 1/10:  mini-batch 2885/4459:  Train loss: 3.1745827198028564  Test loss: 3.367182493209839 \n",
      "Epoch: 1/10:  mini-batch 2886/4459:  Train loss: 3.1254031658172607  Test loss: 3.3682684898376465 \n",
      "Epoch: 1/10:  mini-batch 2887/4459:  Train loss: 3.286083698272705  Test loss: 3.3691673278808594 \n",
      "Epoch: 1/10:  mini-batch 2888/4459:  Train loss: 3.3188862800598145  Test loss: 3.3699817657470703 \n",
      "Epoch: 1/10:  mini-batch 2889/4459:  Train loss: 3.5646090507507324  Test loss: 3.370725393295288 \n",
      "Epoch: 1/10:  mini-batch 2890/4459:  Train loss: 3.430150032043457  Test loss: 3.371544361114502 \n",
      "Epoch: 1/10:  mini-batch 2891/4459:  Train loss: 2.7086875438690186  Test loss: 3.3732337951660156 \n",
      "Epoch: 1/10:  mini-batch 2892/4459:  Train loss: 4.0059685707092285  Test loss: 3.373847484588623 \n",
      "Epoch: 1/10:  mini-batch 2893/4459:  Train loss: 3.104703187942505  Test loss: 3.374206781387329 \n",
      "Epoch: 1/10:  mini-batch 2894/4459:  Train loss: 2.8633675575256348  Test loss: 3.375074625015259 \n",
      "Epoch: 1/10:  mini-batch 2895/4459:  Train loss: 3.1942529678344727  Test loss: 3.376195192337036 \n",
      "Epoch: 1/10:  mini-batch 2896/4459:  Train loss: 2.708437919616699  Test loss: 3.3784990310668945 \n",
      "Epoch: 1/10:  mini-batch 2897/4459:  Train loss: 3.288287401199341  Test loss: 3.38069748878479 \n",
      "Epoch: 1/10:  mini-batch 2898/4459:  Train loss: 3.7941982746124268  Test loss: 3.381971836090088 \n",
      "Epoch: 1/10:  mini-batch 2899/4459:  Train loss: 3.1905252933502197  Test loss: 3.3832483291625977 \n",
      "Epoch: 1/10:  mini-batch 2900/4459:  Train loss: 3.1921026706695557  Test loss: 3.3843533992767334 \n",
      "Epoch: 1/10:  mini-batch 2901/4459:  Train loss: 3.338252544403076  Test loss: 3.3850746154785156 \n",
      "Epoch: 1/10:  mini-batch 2902/4459:  Train loss: 3.1430857181549072  Test loss: 3.385746479034424 \n",
      "Epoch: 1/10:  mini-batch 2903/4459:  Train loss: 3.3726794719696045  Test loss: 3.3861048221588135 \n",
      "Epoch: 1/10:  mini-batch 2904/4459:  Train loss: 3.1645450592041016  Test loss: 3.3866090774536133 \n",
      "Epoch: 1/10:  mini-batch 2905/4459:  Train loss: 3.563685894012451  Test loss: 3.3863773345947266 \n",
      "Epoch: 1/10:  mini-batch 2906/4459:  Train loss: 3.0034384727478027  Test loss: 3.38657283782959 \n",
      "Epoch: 1/10:  mini-batch 2907/4459:  Train loss: 3.3510539531707764  Test loss: 3.386216878890991 \n",
      "Epoch: 1/10:  mini-batch 2908/4459:  Train loss: 3.1998343467712402  Test loss: 3.3856754302978516 \n",
      "Epoch: 1/10:  mini-batch 2909/4459:  Train loss: 3.2233505249023438  Test loss: 3.3856325149536133 \n",
      "Epoch: 1/10:  mini-batch 2910/4459:  Train loss: 3.2598414421081543  Test loss: 3.386535882949829 \n",
      "Epoch: 1/10:  mini-batch 2911/4459:  Train loss: 3.0245466232299805  Test loss: 3.3873672485351562 \n",
      "Epoch: 1/10:  mini-batch 2912/4459:  Train loss: 3.5138792991638184  Test loss: 3.388070821762085 \n",
      "Epoch: 1/10:  mini-batch 2913/4459:  Train loss: 3.45741868019104  Test loss: 3.38863468170166 \n",
      "Epoch: 1/10:  mini-batch 2914/4459:  Train loss: 3.287937641143799  Test loss: 3.3893747329711914 \n",
      "Epoch: 1/10:  mini-batch 2915/4459:  Train loss: 2.595890522003174  Test loss: 3.391873359680176 \n",
      "Epoch: 1/10:  mini-batch 2916/4459:  Train loss: 3.1804354190826416  Test loss: 3.3940367698669434 \n",
      "Epoch: 1/10:  mini-batch 2917/4459:  Train loss: 2.8932199478149414  Test loss: 3.3971195220947266 \n",
      "Epoch: 1/10:  mini-batch 2918/4459:  Train loss: 3.538205146789551  Test loss: 3.3986635208129883 \n",
      "Epoch: 1/10:  mini-batch 2919/4459:  Train loss: 3.3573057651519775  Test loss: 3.399707317352295 \n",
      "Epoch: 1/10:  mini-batch 2920/4459:  Train loss: 3.6957011222839355  Test loss: 3.3990674018859863 \n",
      "Epoch: 1/10:  mini-batch 2921/4459:  Train loss: 3.3571653366088867  Test loss: 3.398226737976074 \n",
      "Epoch: 1/10:  mini-batch 2922/4459:  Train loss: 3.677523136138916  Test loss: 3.395921230316162 \n",
      "Epoch: 1/10:  mini-batch 2923/4459:  Train loss: 3.3276705741882324  Test loss: 3.3935065269470215 \n",
      "Epoch: 1/10:  mini-batch 2924/4459:  Train loss: 3.417754650115967  Test loss: 3.3913960456848145 \n",
      "Epoch: 1/10:  mini-batch 2925/4459:  Train loss: 2.657853126525879  Test loss: 3.391017436981201 \n",
      "Epoch: 1/10:  mini-batch 2926/4459:  Train loss: 3.008227586746216  Test loss: 3.3916823863983154 \n",
      "Epoch: 1/10:  mini-batch 2927/4459:  Train loss: 3.1006667613983154  Test loss: 3.3920905590057373 \n",
      "Epoch: 1/10:  mini-batch 2928/4459:  Train loss: 3.5032360553741455  Test loss: 3.3919074535369873 \n",
      "Epoch: 1/10:  mini-batch 2929/4459:  Train loss: 3.669440507888794  Test loss: 3.391310691833496 \n",
      "Epoch: 1/10:  mini-batch 2930/4459:  Train loss: 3.3429388999938965  Test loss: 3.390911102294922 \n",
      "Epoch: 1/10:  mini-batch 2931/4459:  Train loss: 3.1611690521240234  Test loss: 3.3908679485321045 \n",
      "Epoch: 1/10:  mini-batch 2932/4459:  Train loss: 3.4163436889648438  Test loss: 3.3902523517608643 \n",
      "Epoch: 1/10:  mini-batch 2933/4459:  Train loss: 3.435159683227539  Test loss: 3.3891818523406982 \n",
      "Epoch: 1/10:  mini-batch 2934/4459:  Train loss: 3.4350152015686035  Test loss: 3.388453245162964 \n",
      "Epoch: 1/10:  mini-batch 2935/4459:  Train loss: 3.667243719100952  Test loss: 3.3875303268432617 \n",
      "Epoch: 1/10:  mini-batch 2936/4459:  Train loss: 3.4551005363464355  Test loss: 3.387176275253296 \n",
      "Epoch: 1/10:  mini-batch 2937/4459:  Train loss: 3.3432865142822266  Test loss: 3.3874480724334717 \n",
      "Epoch: 1/10:  mini-batch 2938/4459:  Train loss: 3.1433191299438477  Test loss: 3.388099193572998 \n",
      "Epoch: 1/10:  mini-batch 2939/4459:  Train loss: 3.667757034301758  Test loss: 3.3884072303771973 \n",
      "Epoch: 1/10:  mini-batch 2940/4459:  Train loss: 3.551307201385498  Test loss: 3.387932300567627 \n",
      "Epoch: 1/10:  mini-batch 2941/4459:  Train loss: 3.2682104110717773  Test loss: 3.3879408836364746 \n",
      "Epoch: 1/10:  mini-batch 2942/4459:  Train loss: 3.2734155654907227  Test loss: 3.388476610183716 \n",
      "Epoch: 1/10:  mini-batch 2943/4459:  Train loss: 3.2950823307037354  Test loss: 3.3889923095703125 \n",
      "Epoch: 1/10:  mini-batch 2944/4459:  Train loss: 3.463270902633667  Test loss: 3.3892412185668945 \n",
      "Epoch: 1/10:  mini-batch 2945/4459:  Train loss: 3.3266305923461914  Test loss: 3.390031337738037 \n",
      "Epoch: 1/10:  mini-batch 2946/4459:  Train loss: 3.6636343002319336  Test loss: 3.3902015686035156 \n",
      "Epoch: 1/10:  mini-batch 2947/4459:  Train loss: 3.0983223915100098  Test loss: 3.3905060291290283 \n",
      "Epoch: 1/10:  mini-batch 2948/4459:  Train loss: 3.451481580734253  Test loss: 3.390639305114746 \n",
      "Epoch: 1/10:  mini-batch 2949/4459:  Train loss: 3.177657127380371  Test loss: 3.390455484390259 \n",
      "Epoch: 1/10:  mini-batch 2950/4459:  Train loss: 3.3356399536132812  Test loss: 3.390061140060425 \n",
      "Epoch: 1/10:  mini-batch 2951/4459:  Train loss: 3.070399761199951  Test loss: 3.3903017044067383 \n",
      "Epoch: 1/10:  mini-batch 2952/4459:  Train loss: 3.071735143661499  Test loss: 3.390437126159668 \n",
      "Epoch: 1/10:  mini-batch 2953/4459:  Train loss: 2.961942672729492  Test loss: 3.390434741973877 \n",
      "Epoch: 1/10:  mini-batch 2954/4459:  Train loss: 3.328558921813965  Test loss: 3.390603542327881 \n",
      "Epoch: 1/10:  mini-batch 2955/4459:  Train loss: 3.28666353225708  Test loss: 3.390618324279785 \n",
      "Epoch: 1/10:  mini-batch 2956/4459:  Train loss: 2.9059464931488037  Test loss: 3.3910348415374756 \n",
      "Epoch: 1/10:  mini-batch 2957/4459:  Train loss: 3.329185724258423  Test loss: 3.391849994659424 \n",
      "Epoch: 1/10:  mini-batch 2958/4459:  Train loss: 3.3904225826263428  Test loss: 3.392601490020752 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 2959/4459:  Train loss: 3.1752989292144775  Test loss: 3.3933792114257812 \n",
      "Epoch: 1/10:  mini-batch 2960/4459:  Train loss: 3.4487226009368896  Test loss: 3.393601179122925 \n",
      "Epoch: 1/10:  mini-batch 2961/4459:  Train loss: 3.641836166381836  Test loss: 3.393193244934082 \n",
      "Epoch: 1/10:  mini-batch 2962/4459:  Train loss: 3.72627592086792  Test loss: 3.392587423324585 \n",
      "Epoch: 1/10:  mini-batch 2963/4459:  Train loss: 3.6091933250427246  Test loss: 3.3919119834899902 \n",
      "Epoch: 1/10:  mini-batch 2964/4459:  Train loss: 3.521292209625244  Test loss: 3.39111065864563 \n",
      "Epoch: 1/10:  mini-batch 2965/4459:  Train loss: 3.510977268218994  Test loss: 3.3902995586395264 \n",
      "Epoch: 1/10:  mini-batch 2966/4459:  Train loss: 2.9019408226013184  Test loss: 3.3903231620788574 \n",
      "Epoch: 1/10:  mini-batch 2967/4459:  Train loss: 3.1646900177001953  Test loss: 3.3907339572906494 \n",
      "Epoch: 1/10:  mini-batch 2968/4459:  Train loss: 3.297361135482788  Test loss: 3.39125919342041 \n",
      "Epoch: 1/10:  mini-batch 2969/4459:  Train loss: 3.512755870819092  Test loss: 3.3913888931274414 \n",
      "Epoch: 1/10:  mini-batch 2970/4459:  Train loss: 3.8987574577331543  Test loss: 3.3911218643188477 \n",
      "Epoch: 1/10:  mini-batch 2971/4459:  Train loss: 3.9224863052368164  Test loss: 3.390796184539795 \n",
      "Epoch: 1/10:  mini-batch 2972/4459:  Train loss: 3.5770177841186523  Test loss: 3.3905253410339355 \n",
      "Epoch: 1/10:  mini-batch 2973/4459:  Train loss: 3.1077256202697754  Test loss: 3.3906757831573486 \n",
      "Epoch: 1/10:  mini-batch 2974/4459:  Train loss: 3.4293665885925293  Test loss: 3.3906590938568115 \n",
      "Epoch: 1/10:  mini-batch 2975/4459:  Train loss: 3.740048408508301  Test loss: 3.390624761581421 \n",
      "Epoch: 1/10:  mini-batch 2976/4459:  Train loss: 3.333407402038574  Test loss: 3.390489101409912 \n",
      "Epoch: 1/10:  mini-batch 2977/4459:  Train loss: 3.182480573654175  Test loss: 3.3907392024993896 \n",
      "Epoch: 1/10:  mini-batch 2978/4459:  Train loss: 3.4880170822143555  Test loss: 3.3910574913024902 \n",
      "Epoch: 1/10:  mini-batch 2979/4459:  Train loss: 3.849280834197998  Test loss: 3.3916568756103516 \n",
      "Epoch: 1/10:  mini-batch 2980/4459:  Train loss: 3.746307611465454  Test loss: 3.3925082683563232 \n",
      "Epoch: 1/10:  mini-batch 2981/4459:  Train loss: 3.3615145683288574  Test loss: 3.3924033641815186 \n",
      "Epoch: 1/10:  mini-batch 2982/4459:  Train loss: 3.789428949356079  Test loss: 3.3926944732666016 \n",
      "Epoch: 1/10:  mini-batch 2983/4459:  Train loss: 3.2956953048706055  Test loss: 3.3929405212402344 \n",
      "Epoch: 1/10:  mini-batch 2984/4459:  Train loss: 3.492149829864502  Test loss: 3.393422842025757 \n",
      "Epoch: 1/10:  mini-batch 2985/4459:  Train loss: 3.368638277053833  Test loss: 3.3938887119293213 \n",
      "Epoch: 1/10:  mini-batch 2986/4459:  Train loss: 3.2259130477905273  Test loss: 3.3925740718841553 \n",
      "Epoch: 1/10:  mini-batch 2987/4459:  Train loss: 3.2352190017700195  Test loss: 3.391180992126465 \n",
      "Epoch: 1/10:  mini-batch 2988/4459:  Train loss: 3.0650129318237305  Test loss: 3.389585494995117 \n",
      "Epoch: 1/10:  mini-batch 2989/4459:  Train loss: 3.532630681991577  Test loss: 3.387890577316284 \n",
      "Epoch: 1/10:  mini-batch 2990/4459:  Train loss: 3.4066123962402344  Test loss: 3.3859639167785645 \n",
      "Epoch: 1/10:  mini-batch 2991/4459:  Train loss: 3.3454322814941406  Test loss: 3.3840928077697754 \n",
      "Epoch: 1/10:  mini-batch 2992/4459:  Train loss: 3.7724757194519043  Test loss: 3.3833959102630615 \n",
      "Epoch: 1/10:  mini-batch 2993/4459:  Train loss: 3.4410088062286377  Test loss: 3.382706880569458 \n",
      "Epoch: 1/10:  mini-batch 2994/4459:  Train loss: 3.395317316055298  Test loss: 3.383180618286133 \n",
      "Epoch: 1/10:  mini-batch 2995/4459:  Train loss: 3.512360095977783  Test loss: 3.383492946624756 \n",
      "Epoch: 1/10:  mini-batch 2996/4459:  Train loss: 3.4035823345184326  Test loss: 3.3836846351623535 \n",
      "Epoch: 1/10:  mini-batch 2997/4459:  Train loss: 3.428534984588623  Test loss: 3.384134531021118 \n",
      "Epoch: 1/10:  mini-batch 2998/4459:  Train loss: 3.2319366931915283  Test loss: 3.384218692779541 \n",
      "Epoch: 1/10:  mini-batch 2999/4459:  Train loss: 3.466212749481201  Test loss: 3.3847155570983887 \n",
      "Epoch: 1/10:  mini-batch 3000/4459:  Train loss: 3.4798316955566406  Test loss: 3.386084794998169 \n",
      "Epoch: 1/10:  mini-batch 3001/4459:  Train loss: 3.043705701828003  Test loss: 3.38688063621521 \n",
      "Epoch: 1/10:  mini-batch 3002/4459:  Train loss: 3.290534019470215  Test loss: 3.3872013092041016 \n",
      "Epoch: 1/10:  mini-batch 3003/4459:  Train loss: 3.4076809883117676  Test loss: 3.3875997066497803 \n",
      "Epoch: 1/10:  mini-batch 3004/4459:  Train loss: 3.3413801193237305  Test loss: 3.3883438110351562 \n",
      "Epoch: 1/10:  mini-batch 3005/4459:  Train loss: 3.3160500526428223  Test loss: 3.3884878158569336 \n",
      "Epoch: 1/10:  mini-batch 3006/4459:  Train loss: 3.61053204536438  Test loss: 3.3896539211273193 \n",
      "Epoch: 1/10:  mini-batch 3007/4459:  Train loss: 3.5414767265319824  Test loss: 3.391397714614868 \n",
      "Epoch: 1/10:  mini-batch 3008/4459:  Train loss: 3.346465587615967  Test loss: 3.391529083251953 \n",
      "Epoch: 1/10:  mini-batch 3009/4459:  Train loss: 3.583740472793579  Test loss: 3.3906240463256836 \n",
      "Epoch: 1/10:  mini-batch 3010/4459:  Train loss: 3.3808529376983643  Test loss: 3.388551950454712 \n",
      "Epoch: 1/10:  mini-batch 3011/4459:  Train loss: 3.53352427482605  Test loss: 3.3860578536987305 \n",
      "Epoch: 1/10:  mini-batch 3012/4459:  Train loss: 3.7227258682250977  Test loss: 3.3833084106445312 \n",
      "Epoch: 1/10:  mini-batch 3013/4459:  Train loss: 3.3267760276794434  Test loss: 3.381317138671875 \n",
      "Epoch: 1/10:  mini-batch 3014/4459:  Train loss: 3.621717929840088  Test loss: 3.3797318935394287 \n",
      "Epoch: 1/10:  mini-batch 3015/4459:  Train loss: 3.71937894821167  Test loss: 3.37853741645813 \n",
      "Epoch: 1/10:  mini-batch 3016/4459:  Train loss: 3.5184073448181152  Test loss: 3.377429485321045 \n",
      "Epoch: 1/10:  mini-batch 3017/4459:  Train loss: 3.6352639198303223  Test loss: 3.3759260177612305 \n",
      "Epoch: 1/10:  mini-batch 3018/4459:  Train loss: 3.381406307220459  Test loss: 3.373873710632324 \n",
      "Epoch: 1/10:  mini-batch 3019/4459:  Train loss: 3.45719313621521  Test loss: 3.3720908164978027 \n",
      "Epoch: 1/10:  mini-batch 3020/4459:  Train loss: 3.6369221210479736  Test loss: 3.371037006378174 \n",
      "Epoch: 1/10:  mini-batch 3021/4459:  Train loss: 3.6150269508361816  Test loss: 3.3699934482574463 \n",
      "Epoch: 1/10:  mini-batch 3022/4459:  Train loss: 3.5204951763153076  Test loss: 3.3704538345336914 \n",
      "Epoch: 1/10:  mini-batch 3023/4459:  Train loss: 3.3371493816375732  Test loss: 3.3704442977905273 \n",
      "Epoch: 1/10:  mini-batch 3024/4459:  Train loss: 3.4383885860443115  Test loss: 3.37003231048584 \n",
      "Epoch: 1/10:  mini-batch 3025/4459:  Train loss: 3.4482429027557373  Test loss: 3.3692586421966553 \n",
      "Epoch: 1/10:  mini-batch 3026/4459:  Train loss: 3.437664270401001  Test loss: 3.3682494163513184 \n",
      "Epoch: 1/10:  mini-batch 3027/4459:  Train loss: 3.381416082382202  Test loss: 3.3669464588165283 \n",
      "Epoch: 1/10:  mini-batch 3028/4459:  Train loss: 3.4777705669403076  Test loss: 3.3669424057006836 \n",
      "Epoch: 1/10:  mini-batch 3029/4459:  Train loss: 3.3461034297943115  Test loss: 3.3662641048431396 \n",
      "Epoch: 1/10:  mini-batch 3030/4459:  Train loss: 3.4896445274353027  Test loss: 3.3654394149780273 \n",
      "Epoch: 1/10:  mini-batch 3031/4459:  Train loss: 3.471153736114502  Test loss: 3.36554217338562 \n",
      "Epoch: 1/10:  mini-batch 3032/4459:  Train loss: 3.2688653469085693  Test loss: 3.365063428878784 \n",
      "Epoch: 1/10:  mini-batch 3033/4459:  Train loss: 3.403627395629883  Test loss: 3.3642072677612305 \n",
      "Epoch: 1/10:  mini-batch 3034/4459:  Train loss: 3.4708166122436523  Test loss: 3.362396240234375 \n",
      "Epoch: 1/10:  mini-batch 3035/4459:  Train loss: 3.3826863765716553  Test loss: 3.3599555492401123 \n",
      "Epoch: 1/10:  mini-batch 3036/4459:  Train loss: 3.194328784942627  Test loss: 3.357203483581543 \n",
      "Epoch: 1/10:  mini-batch 3037/4459:  Train loss: 3.380133867263794  Test loss: 3.355282783508301 \n",
      "Epoch: 1/10:  mini-batch 3038/4459:  Train loss: 3.2747814655303955  Test loss: 3.3528199195861816 \n",
      "Epoch: 1/10:  mini-batch 3039/4459:  Train loss: 3.523752212524414  Test loss: 3.3508846759796143 \n",
      "Epoch: 1/10:  mini-batch 3040/4459:  Train loss: 3.221343517303467  Test loss: 3.3508107662200928 \n",
      "Epoch: 1/10:  mini-batch 3041/4459:  Train loss: 3.1156208515167236  Test loss: 3.351604461669922 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3042/4459:  Train loss: 3.570435047149658  Test loss: 3.350904703140259 \n",
      "Epoch: 1/10:  mini-batch 3043/4459:  Train loss: 3.113698959350586  Test loss: 3.35160493850708 \n",
      "Epoch: 1/10:  mini-batch 3044/4459:  Train loss: 3.2451839447021484  Test loss: 3.354249954223633 \n",
      "Epoch: 1/10:  mini-batch 3045/4459:  Train loss: 3.483576536178589  Test loss: 3.356321334838867 \n",
      "Epoch: 1/10:  mini-batch 3046/4459:  Train loss: 3.641780376434326  Test loss: 3.3579905033111572 \n",
      "Epoch: 1/10:  mini-batch 3047/4459:  Train loss: 3.4696738719940186  Test loss: 3.3608558177948 \n",
      "Epoch: 1/10:  mini-batch 3048/4459:  Train loss: 3.695659637451172  Test loss: 3.3626155853271484 \n",
      "Epoch: 1/10:  mini-batch 3049/4459:  Train loss: 3.57875919342041  Test loss: 3.3626832962036133 \n",
      "Epoch: 1/10:  mini-batch 3050/4459:  Train loss: 3.432690143585205  Test loss: 3.3611741065979004 \n",
      "Epoch: 1/10:  mini-batch 3051/4459:  Train loss: 3.4211909770965576  Test loss: 3.361011028289795 \n",
      "Epoch: 1/10:  mini-batch 3052/4459:  Train loss: 3.44795560836792  Test loss: 3.3616209030151367 \n",
      "Epoch: 1/10:  mini-batch 3053/4459:  Train loss: 3.011287212371826  Test loss: 3.3609306812286377 \n",
      "Epoch: 1/10:  mini-batch 3054/4459:  Train loss: 3.0267226696014404  Test loss: 3.360288143157959 \n",
      "Epoch: 1/10:  mini-batch 3055/4459:  Train loss: 3.1180410385131836  Test loss: 3.359938144683838 \n",
      "Epoch: 1/10:  mini-batch 3056/4459:  Train loss: 3.100205659866333  Test loss: 3.3600125312805176 \n",
      "Epoch: 1/10:  mini-batch 3057/4459:  Train loss: 3.161215305328369  Test loss: 3.359710693359375 \n",
      "Epoch: 1/10:  mini-batch 3058/4459:  Train loss: 3.275815486907959  Test loss: 3.360388994216919 \n",
      "Epoch: 1/10:  mini-batch 3059/4459:  Train loss: 3.112971782684326  Test loss: 3.3618810176849365 \n",
      "Epoch: 1/10:  mini-batch 3060/4459:  Train loss: 3.197545289993286  Test loss: 3.364180088043213 \n",
      "Epoch: 1/10:  mini-batch 3061/4459:  Train loss: 3.771728515625  Test loss: 3.366401433944702 \n",
      "Epoch: 1/10:  mini-batch 3062/4459:  Train loss: 3.496286392211914  Test loss: 3.368321180343628 \n",
      "Epoch: 1/10:  mini-batch 3063/4459:  Train loss: 3.174999952316284  Test loss: 3.3722071647644043 \n",
      "Epoch: 1/10:  mini-batch 3064/4459:  Train loss: 3.1089260578155518  Test loss: 3.3774967193603516 \n",
      "Epoch: 1/10:  mini-batch 3065/4459:  Train loss: 2.8297343254089355  Test loss: 3.384119987487793 \n",
      "Epoch: 1/10:  mini-batch 3066/4459:  Train loss: 2.990278720855713  Test loss: 3.391028881072998 \n",
      "Epoch: 1/10:  mini-batch 3067/4459:  Train loss: 3.0774569511413574  Test loss: 3.39749813079834 \n",
      "Epoch: 1/10:  mini-batch 3068/4459:  Train loss: 3.513352870941162  Test loss: 3.4020347595214844 \n",
      "Epoch: 1/10:  mini-batch 3069/4459:  Train loss: 3.206817626953125  Test loss: 3.4052395820617676 \n",
      "Epoch: 1/10:  mini-batch 3070/4459:  Train loss: 2.711107015609741  Test loss: 3.4120302200317383 \n",
      "Epoch: 1/10:  mini-batch 3071/4459:  Train loss: 3.1886463165283203  Test loss: 3.418783187866211 \n",
      "Epoch: 1/10:  mini-batch 3072/4459:  Train loss: 3.087355136871338  Test loss: 3.4265451431274414 \n",
      "Epoch: 1/10:  mini-batch 3073/4459:  Train loss: 3.1028406620025635  Test loss: 3.4364001750946045 \n",
      "Epoch: 1/10:  mini-batch 3074/4459:  Train loss: 3.360013008117676  Test loss: 3.442532539367676 \n",
      "Epoch: 1/10:  mini-batch 3075/4459:  Train loss: 3.49212646484375  Test loss: 3.4449236392974854 \n",
      "Epoch: 1/10:  mini-batch 3076/4459:  Train loss: 2.905329704284668  Test loss: 3.4483838081359863 \n",
      "Epoch: 1/10:  mini-batch 3077/4459:  Train loss: 3.157316207885742  Test loss: 3.451268196105957 \n",
      "Epoch: 1/10:  mini-batch 3078/4459:  Train loss: 3.145031690597534  Test loss: 3.451610803604126 \n",
      "Epoch: 1/10:  mini-batch 3079/4459:  Train loss: 3.936680316925049  Test loss: 3.444206714630127 \n",
      "Epoch: 1/10:  mini-batch 3080/4459:  Train loss: 3.916175603866577  Test loss: 3.433384895324707 \n",
      "Epoch: 1/10:  mini-batch 3081/4459:  Train loss: 2.74912691116333  Test loss: 3.427643060684204 \n",
      "Epoch: 1/10:  mini-batch 3082/4459:  Train loss: 3.0758004188537598  Test loss: 3.423419952392578 \n",
      "Epoch: 1/10:  mini-batch 3083/4459:  Train loss: 3.599365711212158  Test loss: 3.416745662689209 \n",
      "Epoch: 1/10:  mini-batch 3084/4459:  Train loss: 3.0783843994140625  Test loss: 3.412728786468506 \n",
      "Epoch: 1/10:  mini-batch 3085/4459:  Train loss: 3.211737632751465  Test loss: 3.4096884727478027 \n",
      "Epoch: 1/10:  mini-batch 3086/4459:  Train loss: 3.6912100315093994  Test loss: 3.407289505004883 \n",
      "Epoch: 1/10:  mini-batch 3087/4459:  Train loss: 3.7463018894195557  Test loss: 3.4048123359680176 \n",
      "Epoch: 1/10:  mini-batch 3088/4459:  Train loss: 3.648655414581299  Test loss: 3.4028306007385254 \n",
      "Epoch: 1/10:  mini-batch 3089/4459:  Train loss: 3.6470842361450195  Test loss: 3.40175199508667 \n",
      "Epoch: 1/10:  mini-batch 3090/4459:  Train loss: 3.941653251647949  Test loss: 3.4013240337371826 \n",
      "Epoch: 1/10:  mini-batch 3091/4459:  Train loss: 3.159600257873535  Test loss: 3.402510643005371 \n",
      "Epoch: 1/10:  mini-batch 3092/4459:  Train loss: 3.453817129135132  Test loss: 3.403566360473633 \n",
      "Epoch: 1/10:  mini-batch 3093/4459:  Train loss: 3.199084758758545  Test loss: 3.4055142402648926 \n",
      "Epoch: 1/10:  mini-batch 3094/4459:  Train loss: 3.1180450916290283  Test loss: 3.4080071449279785 \n",
      "Epoch: 1/10:  mini-batch 3095/4459:  Train loss: 3.66436505317688  Test loss: 3.4096381664276123 \n",
      "Epoch: 1/10:  mini-batch 3096/4459:  Train loss: 3.251286029815674  Test loss: 3.4099197387695312 \n",
      "Epoch: 1/10:  mini-batch 3097/4459:  Train loss: 3.4522407054901123  Test loss: 3.409668207168579 \n",
      "Epoch: 1/10:  mini-batch 3098/4459:  Train loss: 3.407052516937256  Test loss: 3.409151077270508 \n",
      "Epoch: 1/10:  mini-batch 3099/4459:  Train loss: 3.5790834426879883  Test loss: 3.408311367034912 \n",
      "Epoch: 1/10:  mini-batch 3100/4459:  Train loss: 3.3328795433044434  Test loss: 3.4074690341949463 \n",
      "Epoch: 1/10:  mini-batch 3101/4459:  Train loss: 3.3845763206481934  Test loss: 3.4068446159362793 \n",
      "Epoch: 1/10:  mini-batch 3102/4459:  Train loss: 3.3295090198516846  Test loss: 3.406135320663452 \n",
      "Epoch: 1/10:  mini-batch 3103/4459:  Train loss: 3.494351863861084  Test loss: 3.4055233001708984 \n",
      "Epoch: 1/10:  mini-batch 3104/4459:  Train loss: 3.642259120941162  Test loss: 3.4050259590148926 \n",
      "Epoch: 1/10:  mini-batch 3105/4459:  Train loss: 3.177215337753296  Test loss: 3.4048099517822266 \n",
      "Epoch: 1/10:  mini-batch 3106/4459:  Train loss: 3.292856216430664  Test loss: 3.40446138381958 \n",
      "Epoch: 1/10:  mini-batch 3107/4459:  Train loss: 3.612649917602539  Test loss: 3.404407501220703 \n",
      "Epoch: 1/10:  mini-batch 3108/4459:  Train loss: 3.200608968734741  Test loss: 3.404723644256592 \n",
      "Epoch: 1/10:  mini-batch 3109/4459:  Train loss: 3.3862948417663574  Test loss: 3.405155897140503 \n",
      "Epoch: 1/10:  mini-batch 3110/4459:  Train loss: 3.5559589862823486  Test loss: 3.4058423042297363 \n",
      "Epoch: 1/10:  mini-batch 3111/4459:  Train loss: 3.434757709503174  Test loss: 3.406527280807495 \n",
      "Epoch: 1/10:  mini-batch 3112/4459:  Train loss: 3.326420307159424  Test loss: 3.407212972640991 \n",
      "Epoch: 1/10:  mini-batch 3113/4459:  Train loss: 3.3795254230499268  Test loss: 3.4078259468078613 \n",
      "Epoch: 1/10:  mini-batch 3114/4459:  Train loss: 3.252251148223877  Test loss: 3.4082627296447754 \n",
      "Epoch: 1/10:  mini-batch 3115/4459:  Train loss: 3.5007262229919434  Test loss: 3.4091672897338867 \n",
      "Epoch: 1/10:  mini-batch 3116/4459:  Train loss: 3.4340100288391113  Test loss: 3.4099578857421875 \n",
      "Epoch: 1/10:  mini-batch 3117/4459:  Train loss: 3.1885740756988525  Test loss: 3.410311222076416 \n",
      "Epoch: 1/10:  mini-batch 3118/4459:  Train loss: 3.178112268447876  Test loss: 3.4105944633483887 \n",
      "Epoch: 1/10:  mini-batch 3119/4459:  Train loss: 3.512479543685913  Test loss: 3.4110841751098633 \n",
      "Epoch: 1/10:  mini-batch 3120/4459:  Train loss: 3.5006260871887207  Test loss: 3.411924362182617 \n",
      "Epoch: 1/10:  mini-batch 3121/4459:  Train loss: 3.328571319580078  Test loss: 3.413033962249756 \n",
      "Epoch: 1/10:  mini-batch 3122/4459:  Train loss: 3.3787760734558105  Test loss: 3.4148075580596924 \n",
      "Epoch: 1/10:  mini-batch 3123/4459:  Train loss: 3.2224206924438477  Test loss: 3.4165985584259033 \n",
      "Epoch: 1/10:  mini-batch 3124/4459:  Train loss: 3.1574883460998535  Test loss: 3.4184987545013428 \n",
      "Epoch: 1/10:  mini-batch 3125/4459:  Train loss: 3.4441585540771484  Test loss: 3.4198408126831055 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3126/4459:  Train loss: 3.5204811096191406  Test loss: 3.421473503112793 \n",
      "Epoch: 1/10:  mini-batch 3127/4459:  Train loss: 3.4918322563171387  Test loss: 3.423494338989258 \n",
      "Epoch: 1/10:  mini-batch 3128/4459:  Train loss: 3.6436755657196045  Test loss: 3.426109790802002 \n",
      "Epoch: 1/10:  mini-batch 3129/4459:  Train loss: 3.5519790649414062  Test loss: 3.427860736846924 \n",
      "Epoch: 1/10:  mini-batch 3130/4459:  Train loss: 3.3781914710998535  Test loss: 3.4298901557922363 \n",
      "Epoch: 1/10:  mini-batch 3131/4459:  Train loss: 3.4911935329437256  Test loss: 3.4318630695343018 \n",
      "Epoch: 1/10:  mini-batch 3132/4459:  Train loss: 3.775609016418457  Test loss: 3.4339137077331543 \n",
      "Epoch: 1/10:  mini-batch 3133/4459:  Train loss: 3.156791925430298  Test loss: 3.435516595840454 \n",
      "Epoch: 1/10:  mini-batch 3134/4459:  Train loss: 3.634448528289795  Test loss: 3.435715913772583 \n",
      "Epoch: 1/10:  mini-batch 3135/4459:  Train loss: 3.157498359680176  Test loss: 3.4362454414367676 \n",
      "Epoch: 1/10:  mini-batch 3136/4459:  Train loss: 3.4231395721435547  Test loss: 3.4374279975891113 \n",
      "Epoch: 1/10:  mini-batch 3137/4459:  Train loss: 3.477207660675049  Test loss: 3.4401257038116455 \n",
      "Epoch: 1/10:  mini-batch 3138/4459:  Train loss: 3.5230371952056885  Test loss: 3.4437079429626465 \n",
      "Epoch: 1/10:  mini-batch 3139/4459:  Train loss: 3.55436110496521  Test loss: 3.447969675064087 \n",
      "Epoch: 1/10:  mini-batch 3140/4459:  Train loss: 3.1122448444366455  Test loss: 3.4505510330200195 \n",
      "Epoch: 1/10:  mini-batch 3141/4459:  Train loss: 3.4415416717529297  Test loss: 3.4528517723083496 \n",
      "Epoch: 1/10:  mini-batch 3142/4459:  Train loss: 3.619631767272949  Test loss: 3.4554975032806396 \n",
      "Epoch: 1/10:  mini-batch 3143/4459:  Train loss: 3.606829881668091  Test loss: 3.458129644393921 \n",
      "Epoch: 1/10:  mini-batch 3144/4459:  Train loss: 3.556222438812256  Test loss: 3.460994243621826 \n",
      "Epoch: 1/10:  mini-batch 3145/4459:  Train loss: 3.335178852081299  Test loss: 3.4637527465820312 \n",
      "Epoch: 1/10:  mini-batch 3146/4459:  Train loss: 3.395451068878174  Test loss: 3.466179847717285 \n",
      "Epoch: 1/10:  mini-batch 3147/4459:  Train loss: 3.5073437690734863  Test loss: 3.468423843383789 \n",
      "Epoch: 1/10:  mini-batch 3148/4459:  Train loss: 3.5694777965545654  Test loss: 3.4698715209960938 \n",
      "Epoch: 1/10:  mini-batch 3149/4459:  Train loss: 3.6795785427093506  Test loss: 3.4706292152404785 \n",
      "Epoch: 1/10:  mini-batch 3150/4459:  Train loss: 3.4170591831207275  Test loss: 3.471043109893799 \n",
      "Epoch: 1/10:  mini-batch 3151/4459:  Train loss: 3.644636392593384  Test loss: 3.471153736114502 \n",
      "Epoch: 1/10:  mini-batch 3152/4459:  Train loss: 3.662123680114746  Test loss: 3.4702577590942383 \n",
      "Epoch: 1/10:  mini-batch 3153/4459:  Train loss: 3.6389360427856445  Test loss: 3.468392848968506 \n",
      "Epoch: 1/10:  mini-batch 3154/4459:  Train loss: 3.4866957664489746  Test loss: 3.466672897338867 \n",
      "Epoch: 1/10:  mini-batch 3155/4459:  Train loss: 3.3264379501342773  Test loss: 3.4646949768066406 \n",
      "Epoch: 1/10:  mini-batch 3156/4459:  Train loss: 3.8861639499664307  Test loss: 3.463069438934326 \n",
      "Epoch: 1/10:  mini-batch 3157/4459:  Train loss: 3.595828056335449  Test loss: 3.4621822834014893 \n",
      "Epoch: 1/10:  mini-batch 3158/4459:  Train loss: 3.6217617988586426  Test loss: 3.4620614051818848 \n",
      "Epoch: 1/10:  mini-batch 3159/4459:  Train loss: 3.2684245109558105  Test loss: 3.462155818939209 \n",
      "Epoch: 1/10:  mini-batch 3160/4459:  Train loss: 3.5950918197631836  Test loss: 3.4625720977783203 \n",
      "Epoch: 1/10:  mini-batch 3161/4459:  Train loss: 3.4860849380493164  Test loss: 3.4631757736206055 \n",
      "Epoch: 1/10:  mini-batch 3162/4459:  Train loss: 3.253668785095215  Test loss: 3.4633755683898926 \n",
      "Epoch: 1/10:  mini-batch 3163/4459:  Train loss: 3.111482620239258  Test loss: 3.463693618774414 \n",
      "Epoch: 1/10:  mini-batch 3164/4459:  Train loss: 3.3044114112854004  Test loss: 3.46360182762146 \n",
      "Epoch: 1/10:  mini-batch 3165/4459:  Train loss: 3.443624496459961  Test loss: 3.46344256401062 \n",
      "Epoch: 1/10:  mini-batch 3166/4459:  Train loss: 3.5341477394104004  Test loss: 3.463832378387451 \n",
      "Epoch: 1/10:  mini-batch 3167/4459:  Train loss: 3.0963661670684814  Test loss: 3.4638938903808594 \n",
      "Epoch: 1/10:  mini-batch 3168/4459:  Train loss: 3.5257606506347656  Test loss: 3.4636921882629395 \n",
      "Epoch: 1/10:  mini-batch 3169/4459:  Train loss: 3.2884674072265625  Test loss: 3.4633469581604004 \n",
      "Epoch: 1/10:  mini-batch 3170/4459:  Train loss: 3.416735887527466  Test loss: 3.4627609252929688 \n",
      "Epoch: 1/10:  mini-batch 3171/4459:  Train loss: 3.750162124633789  Test loss: 3.4618899822235107 \n",
      "Epoch: 1/10:  mini-batch 3172/4459:  Train loss: 3.6126625537872314  Test loss: 3.460747718811035 \n",
      "Epoch: 1/10:  mini-batch 3173/4459:  Train loss: 3.494676351547241  Test loss: 3.459744930267334 \n",
      "Epoch: 1/10:  mini-batch 3174/4459:  Train loss: 3.5777878761291504  Test loss: 3.4587221145629883 \n",
      "Epoch: 1/10:  mini-batch 3175/4459:  Train loss: 3.364535093307495  Test loss: 3.457317352294922 \n",
      "Epoch: 1/10:  mini-batch 3176/4459:  Train loss: 3.2501606941223145  Test loss: 3.4555845260620117 \n",
      "Epoch: 1/10:  mini-batch 3177/4459:  Train loss: 3.3809549808502197  Test loss: 3.4542341232299805 \n",
      "Epoch: 1/10:  mini-batch 3178/4459:  Train loss: 3.515512466430664  Test loss: 3.4523587226867676 \n",
      "Epoch: 1/10:  mini-batch 3179/4459:  Train loss: 3.238429546356201  Test loss: 3.451005697250366 \n",
      "Epoch: 1/10:  mini-batch 3180/4459:  Train loss: 3.3890085220336914  Test loss: 3.4501945972442627 \n",
      "Epoch: 1/10:  mini-batch 3181/4459:  Train loss: 3.303253412246704  Test loss: 3.449643135070801 \n",
      "Epoch: 1/10:  mini-batch 3182/4459:  Train loss: 3.5008797645568848  Test loss: 3.4480504989624023 \n",
      "Epoch: 1/10:  mini-batch 3183/4459:  Train loss: 3.331061840057373  Test loss: 3.4473533630371094 \n",
      "Epoch: 1/10:  mini-batch 3184/4459:  Train loss: 3.4209704399108887  Test loss: 3.447357177734375 \n",
      "Epoch: 1/10:  mini-batch 3185/4459:  Train loss: 3.2500476837158203  Test loss: 3.448176860809326 \n",
      "Epoch: 1/10:  mini-batch 3186/4459:  Train loss: 3.319016218185425  Test loss: 3.449763298034668 \n",
      "Epoch: 1/10:  mini-batch 3187/4459:  Train loss: 3.4681310653686523  Test loss: 3.4513514041900635 \n",
      "Epoch: 1/10:  mini-batch 3188/4459:  Train loss: 3.403857946395874  Test loss: 3.4528698921203613 \n",
      "Epoch: 1/10:  mini-batch 3189/4459:  Train loss: 3.353543519973755  Test loss: 3.454923629760742 \n",
      "Epoch: 1/10:  mini-batch 3190/4459:  Train loss: 3.1497788429260254  Test loss: 3.457336664199829 \n",
      "Epoch: 1/10:  mini-batch 3191/4459:  Train loss: 3.260664939880371  Test loss: 3.4601221084594727 \n",
      "Epoch: 1/10:  mini-batch 3192/4459:  Train loss: 3.282722234725952  Test loss: 3.4633984565734863 \n",
      "Epoch: 1/10:  mini-batch 3193/4459:  Train loss: 3.1381335258483887  Test loss: 3.467463493347168 \n",
      "Epoch: 1/10:  mini-batch 3194/4459:  Train loss: 3.1121811866760254  Test loss: 3.472598075866699 \n",
      "Epoch: 1/10:  mini-batch 3195/4459:  Train loss: 3.1279263496398926  Test loss: 3.4788594245910645 \n",
      "Epoch: 1/10:  mini-batch 3196/4459:  Train loss: 3.189940929412842  Test loss: 3.485797882080078 \n",
      "Epoch: 1/10:  mini-batch 3197/4459:  Train loss: 3.0414347648620605  Test loss: 3.493696689605713 \n",
      "Epoch: 1/10:  mini-batch 3198/4459:  Train loss: 3.1204423904418945  Test loss: 3.502985715866089 \n",
      "Epoch: 1/10:  mini-batch 3199/4459:  Train loss: 2.897430419921875  Test loss: 3.5136525630950928 \n",
      "Epoch: 1/10:  mini-batch 3200/4459:  Train loss: 2.722064971923828  Test loss: 3.526988983154297 \n",
      "Epoch: 1/10:  mini-batch 3201/4459:  Train loss: 2.768984794616699  Test loss: 3.544354200363159 \n",
      "Epoch: 1/10:  mini-batch 3202/4459:  Train loss: 3.431297779083252  Test loss: 3.562023162841797 \n",
      "Epoch: 1/10:  mini-batch 3203/4459:  Train loss: 2.8658504486083984  Test loss: 3.5831689834594727 \n",
      "Epoch: 1/10:  mini-batch 3204/4459:  Train loss: 2.808370590209961  Test loss: 3.607314109802246 \n",
      "Epoch: 1/10:  mini-batch 3205/4459:  Train loss: 2.9882068634033203  Test loss: 3.634878158569336 \n",
      "Epoch: 1/10:  mini-batch 3206/4459:  Train loss: 2.770681858062744  Test loss: 3.667414903640747 \n",
      "Epoch: 1/10:  mini-batch 3207/4459:  Train loss: 2.768366813659668  Test loss: 3.706331729888916 \n",
      "Epoch: 1/10:  mini-batch 3208/4459:  Train loss: 3.1266746520996094  Test loss: 3.7498779296875 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3209/4459:  Train loss: 2.6880106925964355  Test loss: 3.8015129566192627 \n",
      "Epoch: 1/10:  mini-batch 3210/4459:  Train loss: 2.828291654586792  Test loss: 3.854825019836426 \n",
      "Epoch: 1/10:  mini-batch 3211/4459:  Train loss: 2.54758358001709  Test loss: 3.9256467819213867 \n",
      "Epoch: 1/10:  mini-batch 3212/4459:  Train loss: 2.7624311447143555  Test loss: 4.0084428787231445 \n",
      "Epoch: 1/10:  mini-batch 3213/4459:  Train loss: 2.535524845123291  Test loss: 4.111372470855713 \n",
      "Epoch: 1/10:  mini-batch 3214/4459:  Train loss: 2.9789748191833496  Test loss: 4.162928581237793 \n",
      "Epoch: 1/10:  mini-batch 3215/4459:  Train loss: 3.8724074363708496  Test loss: 4.123591423034668 \n",
      "Epoch: 1/10:  mini-batch 3216/4459:  Train loss: 2.2830300331115723  Test loss: 4.110903739929199 \n",
      "Epoch: 1/10:  mini-batch 3217/4459:  Train loss: 3.3690829277038574  Test loss: 4.0739545822143555 \n",
      "Epoch: 1/10:  mini-batch 3218/4459:  Train loss: 3.2233359813690186  Test loss: 4.030014991760254 \n",
      "Epoch: 1/10:  mini-batch 3219/4459:  Train loss: 2.987320899963379  Test loss: 3.990166425704956 \n",
      "Epoch: 1/10:  mini-batch 3220/4459:  Train loss: 2.758664608001709  Test loss: 3.959812879562378 \n",
      "Epoch: 1/10:  mini-batch 3221/4459:  Train loss: 2.6938636302948  Test loss: 3.939331531524658 \n",
      "Epoch: 1/10:  mini-batch 3222/4459:  Train loss: 2.6670422554016113  Test loss: 3.9313230514526367 \n",
      "Epoch: 1/10:  mini-batch 3223/4459:  Train loss: 2.2453033924102783  Test loss: 3.9373536109924316 \n",
      "Epoch: 1/10:  mini-batch 3224/4459:  Train loss: 2.4183554649353027  Test loss: 3.949568271636963 \n",
      "Epoch: 1/10:  mini-batch 3225/4459:  Train loss: 2.7787184715270996  Test loss: 3.96175479888916 \n",
      "Epoch: 1/10:  mini-batch 3226/4459:  Train loss: 2.5439224243164062  Test loss: 3.974004030227661 \n",
      "Epoch: 1/10:  mini-batch 3227/4459:  Train loss: 2.4729840755462646  Test loss: 3.990035057067871 \n",
      "Epoch: 1/10:  mini-batch 3228/4459:  Train loss: 2.91874098777771  Test loss: 4.0033087730407715 \n",
      "Epoch: 1/10:  mini-batch 3229/4459:  Train loss: 3.061163902282715  Test loss: 4.011384010314941 \n",
      "Epoch: 1/10:  mini-batch 3230/4459:  Train loss: 2.6251683235168457  Test loss: 4.022383689880371 \n",
      "Epoch: 1/10:  mini-batch 3231/4459:  Train loss: 2.366425037384033  Test loss: 4.040643215179443 \n",
      "Epoch: 1/10:  mini-batch 3232/4459:  Train loss: 2.711918830871582  Test loss: 4.059364318847656 \n",
      "Epoch: 1/10:  mini-batch 3233/4459:  Train loss: 3.277813196182251  Test loss: 4.065280914306641 \n",
      "Epoch: 1/10:  mini-batch 3234/4459:  Train loss: 2.78867244720459  Test loss: 4.069851398468018 \n",
      "Epoch: 1/10:  mini-batch 3235/4459:  Train loss: 2.6215696334838867  Test loss: 4.075973987579346 \n",
      "Epoch: 1/10:  mini-batch 3236/4459:  Train loss: 2.550595760345459  Test loss: 4.0866594314575195 \n",
      "Epoch: 1/10:  mini-batch 3237/4459:  Train loss: 2.5381269454956055  Test loss: 4.1016716957092285 \n",
      "Epoch: 1/10:  mini-batch 3238/4459:  Train loss: 2.304044723510742  Test loss: 4.126585006713867 \n",
      "Epoch: 1/10:  mini-batch 3239/4459:  Train loss: 2.832692861557007  Test loss: 4.146085262298584 \n",
      "Epoch: 1/10:  mini-batch 3240/4459:  Train loss: 2.004425525665283  Test loss: 4.180734157562256 \n",
      "Epoch: 1/10:  mini-batch 3241/4459:  Train loss: 2.346158742904663  Test loss: 4.222556114196777 \n",
      "Epoch: 1/10:  mini-batch 3242/4459:  Train loss: 2.65762996673584  Test loss: 4.261869430541992 \n",
      "Epoch: 1/10:  mini-batch 3243/4459:  Train loss: 2.7008492946624756  Test loss: 4.295192241668701 \n",
      "Epoch: 1/10:  mini-batch 3244/4459:  Train loss: 2.7204272747039795  Test loss: 4.3195319175720215 \n",
      "Epoch: 1/10:  mini-batch 3245/4459:  Train loss: 2.4851903915405273  Test loss: 4.344522953033447 \n",
      "Epoch: 1/10:  mini-batch 3246/4459:  Train loss: 2.1902270317077637  Test loss: 4.380247116088867 \n",
      "Epoch: 1/10:  mini-batch 3247/4459:  Train loss: 3.409990072250366  Test loss: 4.3824462890625 \n",
      "Epoch: 1/10:  mini-batch 3248/4459:  Train loss: 2.2089576721191406  Test loss: 4.393763542175293 \n",
      "Epoch: 1/10:  mini-batch 3249/4459:  Train loss: 1.8955497741699219  Test loss: 4.424847602844238 \n",
      "Epoch: 1/10:  mini-batch 3250/4459:  Train loss: 2.691403388977051  Test loss: 4.4491376876831055 \n",
      "Epoch: 1/10:  mini-batch 3251/4459:  Train loss: 2.8899223804473877  Test loss: 4.457531929016113 \n",
      "Epoch: 1/10:  mini-batch 3252/4459:  Train loss: 1.9910521507263184  Test loss: 4.481119155883789 \n",
      "Epoch: 1/10:  mini-batch 3253/4459:  Train loss: 2.1281356811523438  Test loss: 4.514371871948242 \n",
      "Epoch: 1/10:  mini-batch 3254/4459:  Train loss: 2.716034412384033  Test loss: 4.535725116729736 \n",
      "Epoch: 1/10:  mini-batch 3255/4459:  Train loss: 2.613706111907959  Test loss: 4.550311088562012 \n",
      "Epoch: 1/10:  mini-batch 3256/4459:  Train loss: 3.066093921661377  Test loss: 4.5407843589782715 \n",
      "Epoch: 1/10:  mini-batch 3257/4459:  Train loss: 2.238612413406372  Test loss: 4.540580749511719 \n",
      "Epoch: 1/10:  mini-batch 3258/4459:  Train loss: 2.8051652908325195  Test loss: 4.528182506561279 \n",
      "Epoch: 1/10:  mini-batch 3259/4459:  Train loss: 2.5521621704101562  Test loss: 4.512678146362305 \n",
      "Epoch: 1/10:  mini-batch 3260/4459:  Train loss: 3.1715798377990723  Test loss: 4.476809024810791 \n",
      "Epoch: 1/10:  mini-batch 3261/4459:  Train loss: 2.380784034729004  Test loss: 4.4500837326049805 \n",
      "Epoch: 1/10:  mini-batch 3262/4459:  Train loss: 2.0122077465057373  Test loss: 4.4409589767456055 \n",
      "Epoch: 1/10:  mini-batch 3263/4459:  Train loss: 1.8044202327728271  Test loss: 4.453942775726318 \n",
      "Epoch: 1/10:  mini-batch 3264/4459:  Train loss: 2.321868658065796  Test loss: 4.470687389373779 \n",
      "Epoch: 1/10:  mini-batch 3265/4459:  Train loss: 2.487565517425537  Test loss: 4.487770080566406 \n",
      "Epoch: 1/10:  mini-batch 3266/4459:  Train loss: 2.7227585315704346  Test loss: 4.498829364776611 \n",
      "Epoch: 1/10:  mini-batch 3267/4459:  Train loss: 2.3437445163726807  Test loss: 4.514909267425537 \n",
      "Epoch: 1/10:  mini-batch 3268/4459:  Train loss: 2.3305532932281494  Test loss: 4.535431861877441 \n",
      "Epoch: 1/10:  mini-batch 3269/4459:  Train loss: 2.5429887771606445  Test loss: 4.554713726043701 \n",
      "Epoch: 1/10:  mini-batch 3270/4459:  Train loss: 2.551995277404785  Test loss: 4.570544719696045 \n",
      "Epoch: 1/10:  mini-batch 3271/4459:  Train loss: 2.49190092086792  Test loss: 4.584911823272705 \n",
      "Epoch: 1/10:  mini-batch 3272/4459:  Train loss: 4.076570987701416  Test loss: 4.537528038024902 \n",
      "Epoch: 1/10:  mini-batch 3273/4459:  Train loss: 2.1165528297424316  Test loss: 4.509441375732422 \n",
      "Epoch: 1/10:  mini-batch 3274/4459:  Train loss: 2.4299519062042236  Test loss: 4.488445281982422 \n",
      "Epoch: 1/10:  mini-batch 3275/4459:  Train loss: 2.8229262828826904  Test loss: 4.463248252868652 \n",
      "Epoch: 1/10:  mini-batch 3276/4459:  Train loss: 2.504143714904785  Test loss: 4.4423723220825195 \n",
      "Epoch: 1/10:  mini-batch 3277/4459:  Train loss: 2.7999672889709473  Test loss: 4.419554233551025 \n",
      "Epoch: 1/10:  mini-batch 3278/4459:  Train loss: 2.668290138244629  Test loss: 4.398196220397949 \n",
      "Epoch: 1/10:  mini-batch 3279/4459:  Train loss: 3.071831226348877  Test loss: 4.3678297996521 \n",
      "Epoch: 1/10:  mini-batch 3280/4459:  Train loss: 2.7223548889160156  Test loss: 4.341766834259033 \n",
      "Epoch: 1/10:  mini-batch 3281/4459:  Train loss: 3.167083978652954  Test loss: 4.308935165405273 \n",
      "Epoch: 1/10:  mini-batch 3282/4459:  Train loss: 2.7229063510894775  Test loss: 4.281395435333252 \n",
      "Epoch: 1/10:  mini-batch 3283/4459:  Train loss: 2.408860206604004  Test loss: 4.264588356018066 \n",
      "Epoch: 1/10:  mini-batch 3284/4459:  Train loss: 2.8146963119506836  Test loss: 4.249085426330566 \n",
      "Epoch: 1/10:  mini-batch 3285/4459:  Train loss: 2.6687564849853516  Test loss: 4.237766265869141 \n",
      "Epoch: 1/10:  mini-batch 3286/4459:  Train loss: 2.325620651245117  Test loss: 4.234712600708008 \n",
      "Epoch: 1/10:  mini-batch 3287/4459:  Train loss: 2.7930407524108887  Test loss: 4.231949329376221 \n",
      "Epoch: 1/10:  mini-batch 3288/4459:  Train loss: 2.700778007507324  Test loss: 4.232364177703857 \n",
      "Epoch: 1/10:  mini-batch 3289/4459:  Train loss: 2.770554542541504  Test loss: 4.233219623565674 \n",
      "Epoch: 1/10:  mini-batch 3290/4459:  Train loss: 2.6877291202545166  Test loss: 4.236695289611816 \n",
      "Epoch: 1/10:  mini-batch 3291/4459:  Train loss: 2.5767269134521484  Test loss: 4.2451934814453125 \n",
      "Epoch: 1/10:  mini-batch 3292/4459:  Train loss: 2.3031740188598633  Test loss: 4.259316444396973 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3293/4459:  Train loss: 2.68445086479187  Test loss: 4.2748332023620605 \n",
      "Epoch: 1/10:  mini-batch 3294/4459:  Train loss: 2.399362087249756  Test loss: 4.2945756912231445 \n",
      "Epoch: 1/10:  mini-batch 3295/4459:  Train loss: 2.210603952407837  Test loss: 4.319697856903076 \n",
      "Epoch: 1/10:  mini-batch 3296/4459:  Train loss: 2.463331699371338  Test loss: 4.348150253295898 \n",
      "Epoch: 1/10:  mini-batch 3297/4459:  Train loss: 2.290092945098877  Test loss: 4.380221366882324 \n",
      "Epoch: 1/10:  mini-batch 3298/4459:  Train loss: 2.3233704566955566  Test loss: 4.415998458862305 \n",
      "Epoch: 1/10:  mini-batch 3299/4459:  Train loss: 2.284127712249756  Test loss: 4.4543328285217285 \n",
      "Epoch: 1/10:  mini-batch 3300/4459:  Train loss: 2.850778341293335  Test loss: 4.48522424697876 \n",
      "Epoch: 1/10:  mini-batch 3301/4459:  Train loss: 2.525217294692993  Test loss: 4.515847206115723 \n",
      "Epoch: 1/10:  mini-batch 3302/4459:  Train loss: 2.6490931510925293  Test loss: 4.542630195617676 \n",
      "Epoch: 1/10:  mini-batch 3303/4459:  Train loss: 2.7391860485076904  Test loss: 4.563744068145752 \n",
      "Epoch: 1/10:  mini-batch 3304/4459:  Train loss: 3.483781576156616  Test loss: 4.56008243560791 \n",
      "Epoch: 1/10:  mini-batch 3305/4459:  Train loss: 3.4568376541137695  Test loss: 4.537747383117676 \n",
      "Epoch: 1/10:  mini-batch 3306/4459:  Train loss: 3.3050971031188965  Test loss: 4.504439353942871 \n",
      "Epoch: 1/10:  mini-batch 3307/4459:  Train loss: 2.8805341720581055  Test loss: 4.4705095291137695 \n",
      "Epoch: 1/10:  mini-batch 3308/4459:  Train loss: 2.776682138442993  Test loss: 4.438419818878174 \n",
      "Epoch: 1/10:  mini-batch 3309/4459:  Train loss: 2.6469733715057373  Test loss: 4.412489414215088 \n",
      "Epoch: 1/10:  mini-batch 3310/4459:  Train loss: 2.697925090789795  Test loss: 4.3924784660339355 \n",
      "Epoch: 1/10:  mini-batch 3311/4459:  Train loss: 2.5069994926452637  Test loss: 4.379447937011719 \n",
      "Epoch: 1/10:  mini-batch 3312/4459:  Train loss: 3.051666021347046  Test loss: 4.363545894622803 \n",
      "Epoch: 1/10:  mini-batch 3313/4459:  Train loss: 3.6504149436950684  Test loss: 4.333707332611084 \n",
      "Epoch: 1/10:  mini-batch 3314/4459:  Train loss: 2.4081921577453613  Test loss: 4.313913345336914 \n",
      "Epoch: 1/10:  mini-batch 3315/4459:  Train loss: 2.5272135734558105  Test loss: 4.299973964691162 \n",
      "Epoch: 1/10:  mini-batch 3316/4459:  Train loss: 2.4243133068084717  Test loss: 4.2928080558776855 \n",
      "Epoch: 1/10:  mini-batch 3317/4459:  Train loss: 2.8391146659851074  Test loss: 4.286674976348877 \n",
      "Epoch: 1/10:  mini-batch 3318/4459:  Train loss: 2.4834187030792236  Test loss: 4.286180019378662 \n",
      "Epoch: 1/10:  mini-batch 3319/4459:  Train loss: 2.7263193130493164  Test loss: 4.2863311767578125 \n",
      "Epoch: 1/10:  mini-batch 3320/4459:  Train loss: 2.3858866691589355  Test loss: 4.294132232666016 \n",
      "Epoch: 1/10:  mini-batch 3321/4459:  Train loss: 2.604544162750244  Test loss: 4.304617881774902 \n",
      "Epoch: 1/10:  mini-batch 3322/4459:  Train loss: 2.4212136268615723  Test loss: 4.317734718322754 \n",
      "Epoch: 1/10:  mini-batch 3323/4459:  Train loss: 2.5079212188720703  Test loss: 4.335094451904297 \n",
      "Epoch: 1/10:  mini-batch 3324/4459:  Train loss: 2.681421995162964  Test loss: 4.353577136993408 \n",
      "Epoch: 1/10:  mini-batch 3325/4459:  Train loss: 2.5517637729644775  Test loss: 4.374252796173096 \n",
      "Epoch: 1/10:  mini-batch 3326/4459:  Train loss: 2.4763870239257812  Test loss: 4.399044513702393 \n",
      "Epoch: 1/10:  mini-batch 3327/4459:  Train loss: 2.502821445465088  Test loss: 4.4254608154296875 \n",
      "Epoch: 1/10:  mini-batch 3328/4459:  Train loss: 2.526766777038574  Test loss: 4.453910827636719 \n",
      "Epoch: 1/10:  mini-batch 3329/4459:  Train loss: 2.6250100135803223  Test loss: 4.480694770812988 \n",
      "Epoch: 1/10:  mini-batch 3330/4459:  Train loss: 2.430534601211548  Test loss: 4.510384559631348 \n",
      "Epoch: 1/10:  mini-batch 3331/4459:  Train loss: 2.65907883644104  Test loss: 4.53657341003418 \n",
      "Epoch: 1/10:  mini-batch 3332/4459:  Train loss: 2.449800968170166  Test loss: 4.564476490020752 \n",
      "Epoch: 1/10:  mini-batch 3333/4459:  Train loss: 2.493149518966675  Test loss: 4.593699932098389 \n",
      "Epoch: 1/10:  mini-batch 3334/4459:  Train loss: 2.678044319152832  Test loss: 4.619497299194336 \n",
      "Epoch: 1/10:  mini-batch 3335/4459:  Train loss: 2.32338547706604  Test loss: 4.647545337677002 \n",
      "Epoch: 1/10:  mini-batch 3336/4459:  Train loss: 2.5921952724456787  Test loss: 4.673372745513916 \n",
      "Epoch: 1/10:  mini-batch 3337/4459:  Train loss: 2.432955503463745  Test loss: 4.699883937835693 \n",
      "Epoch: 1/10:  mini-batch 3338/4459:  Train loss: 2.6466126441955566  Test loss: 4.722213268280029 \n",
      "Epoch: 1/10:  mini-batch 3339/4459:  Train loss: 2.573637008666992  Test loss: 4.74215030670166 \n",
      "Epoch: 1/10:  mini-batch 3340/4459:  Train loss: 3.376217842102051  Test loss: 4.741518020629883 \n",
      "Epoch: 1/10:  mini-batch 3341/4459:  Train loss: 3.074746608734131  Test loss: 4.725625991821289 \n",
      "Epoch: 1/10:  mini-batch 3342/4459:  Train loss: 2.5955049991607666  Test loss: 4.708175182342529 \n",
      "Epoch: 1/10:  mini-batch 3343/4459:  Train loss: 2.8944716453552246  Test loss: 4.684822082519531 \n",
      "Epoch: 1/10:  mini-batch 3344/4459:  Train loss: 2.5406076908111572  Test loss: 4.664494514465332 \n",
      "Epoch: 1/10:  mini-batch 3345/4459:  Train loss: 3.8932862281799316  Test loss: 4.618386268615723 \n",
      "Epoch: 1/10:  mini-batch 3346/4459:  Train loss: 3.299757242202759  Test loss: 4.563910484313965 \n",
      "Epoch: 1/10:  mini-batch 3347/4459:  Train loss: 2.498246908187866  Test loss: 4.521005630493164 \n",
      "Epoch: 1/10:  mini-batch 3348/4459:  Train loss: 2.767848014831543  Test loss: 4.481050491333008 \n",
      "Epoch: 1/10:  mini-batch 3349/4459:  Train loss: 4.332657814025879  Test loss: 4.4204254150390625 \n",
      "Epoch: 1/10:  mini-batch 3350/4459:  Train loss: 3.067594528198242  Test loss: 4.361883640289307 \n",
      "Epoch: 1/10:  mini-batch 3351/4459:  Train loss: 4.191138744354248  Test loss: 4.291775703430176 \n",
      "Epoch: 1/10:  mini-batch 3352/4459:  Train loss: 3.669339179992676  Test loss: 4.219850063323975 \n",
      "Epoch: 1/10:  mini-batch 3353/4459:  Train loss: 2.6301815509796143  Test loss: 4.162775993347168 \n",
      "Epoch: 1/10:  mini-batch 3354/4459:  Train loss: 2.647209644317627  Test loss: 4.119333267211914 \n",
      "Epoch: 1/10:  mini-batch 3355/4459:  Train loss: 2.6289806365966797  Test loss: 4.089844703674316 \n",
      "Epoch: 1/10:  mini-batch 3356/4459:  Train loss: 2.7270517349243164  Test loss: 4.06992244720459 \n",
      "Epoch: 1/10:  mini-batch 3357/4459:  Train loss: 2.612114906311035  Test loss: 4.059501647949219 \n",
      "Epoch: 1/10:  mini-batch 3358/4459:  Train loss: 2.8530712127685547  Test loss: 4.054698944091797 \n",
      "Epoch: 1/10:  mini-batch 3359/4459:  Train loss: 3.894407272338867  Test loss: 4.03434419631958 \n",
      "Epoch: 1/10:  mini-batch 3360/4459:  Train loss: 3.9061074256896973  Test loss: 4.004375457763672 \n",
      "Epoch: 1/10:  mini-batch 3361/4459:  Train loss: 3.8951432704925537  Test loss: 3.9689576625823975 \n",
      "Epoch: 1/10:  mini-batch 3362/4459:  Train loss: 3.840522050857544  Test loss: 3.9304580688476562 \n",
      "Epoch: 1/10:  mini-batch 3363/4459:  Train loss: 3.259223461151123  Test loss: 3.897294044494629 \n",
      "Epoch: 1/10:  mini-batch 3364/4459:  Train loss: 3.1596014499664307  Test loss: 3.8692688941955566 \n",
      "Epoch: 1/10:  mini-batch 3365/4459:  Train loss: 3.4369020462036133  Test loss: 3.8429226875305176 \n",
      "Epoch: 1/10:  mini-batch 3366/4459:  Train loss: 3.5949671268463135  Test loss: 3.8166723251342773 \n",
      "Epoch: 1/10:  mini-batch 3367/4459:  Train loss: 3.365882158279419  Test loss: 3.793964385986328 \n",
      "Epoch: 1/10:  mini-batch 3368/4459:  Train loss: 3.38032865524292  Test loss: 3.773054599761963 \n",
      "Epoch: 1/10:  mini-batch 3369/4459:  Train loss: 3.4424519538879395  Test loss: 3.751940965652466 \n",
      "Epoch: 1/10:  mini-batch 3370/4459:  Train loss: 3.280411720275879  Test loss: 3.7364587783813477 \n",
      "Epoch: 1/10:  mini-batch 3371/4459:  Train loss: 3.0382885932922363  Test loss: 3.7328670024871826 \n",
      "Epoch: 1/10:  mini-batch 3372/4459:  Train loss: 3.1755385398864746  Test loss: 3.73330020904541 \n",
      "Epoch: 1/10:  mini-batch 3373/4459:  Train loss: 3.374117851257324  Test loss: 3.7364659309387207 \n",
      "Epoch: 1/10:  mini-batch 3374/4459:  Train loss: 3.30210018157959  Test loss: 3.743675470352173 \n",
      "Epoch: 1/10:  mini-batch 3375/4459:  Train loss: 3.5076968669891357  Test loss: 3.7517762184143066 \n",
      "Epoch: 1/10:  mini-batch 3376/4459:  Train loss: 3.262202739715576  Test loss: 3.761531352996826 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3377/4459:  Train loss: 2.8611366748809814  Test loss: 3.7796034812927246 \n",
      "Epoch: 1/10:  mini-batch 3378/4459:  Train loss: 3.1158194541931152  Test loss: 3.7965030670166016 \n",
      "Epoch: 1/10:  mini-batch 3379/4459:  Train loss: 3.1219563484191895  Test loss: 3.8126606941223145 \n",
      "Epoch: 1/10:  mini-batch 3380/4459:  Train loss: 2.8176634311676025  Test loss: 3.8355414867401123 \n",
      "Epoch: 1/10:  mini-batch 3381/4459:  Train loss: 2.8605103492736816  Test loss: 3.8627800941467285 \n",
      "Epoch: 1/10:  mini-batch 3382/4459:  Train loss: 2.9962708950042725  Test loss: 3.8887791633605957 \n",
      "Epoch: 1/10:  mini-batch 3383/4459:  Train loss: 2.9091053009033203  Test loss: 3.9161219596862793 \n",
      "Epoch: 1/10:  mini-batch 3384/4459:  Train loss: 2.7729296684265137  Test loss: 3.9474499225616455 \n",
      "Epoch: 1/10:  mini-batch 3385/4459:  Train loss: 2.817233085632324  Test loss: 3.980377197265625 \n",
      "Epoch: 1/10:  mini-batch 3386/4459:  Train loss: 2.8464081287384033  Test loss: 4.014188766479492 \n",
      "Epoch: 1/10:  mini-batch 3387/4459:  Train loss: 3.4747989177703857  Test loss: 4.041213035583496 \n",
      "Epoch: 1/10:  mini-batch 3388/4459:  Train loss: 3.4832115173339844  Test loss: 4.0612921714782715 \n",
      "Epoch: 1/10:  mini-batch 3389/4459:  Train loss: 3.6770496368408203  Test loss: 4.065661430358887 \n",
      "Epoch: 1/10:  mini-batch 3390/4459:  Train loss: 4.0287370681762695  Test loss: 4.0518927574157715 \n",
      "Epoch: 1/10:  mini-batch 3391/4459:  Train loss: 3.8389205932617188  Test loss: 4.026497840881348 \n",
      "Epoch: 1/10:  mini-batch 3392/4459:  Train loss: 4.065038681030273  Test loss: 3.991652250289917 \n",
      "Epoch: 1/10:  mini-batch 3393/4459:  Train loss: 4.04563045501709  Test loss: 3.9547083377838135 \n",
      "Epoch: 1/10:  mini-batch 3394/4459:  Train loss: 3.69453763961792  Test loss: 3.9212727546691895 \n",
      "Epoch: 1/10:  mini-batch 3395/4459:  Train loss: 3.882124185562134  Test loss: 3.88734769821167 \n",
      "Epoch: 1/10:  mini-batch 3396/4459:  Train loss: 4.115324974060059  Test loss: 3.8506548404693604 \n",
      "Epoch: 1/10:  mini-batch 3397/4459:  Train loss: 3.748927116394043  Test loss: 3.816291570663452 \n",
      "Epoch: 1/10:  mini-batch 3398/4459:  Train loss: 3.9645402431488037  Test loss: 3.7830190658569336 \n",
      "Epoch: 1/10:  mini-batch 3399/4459:  Train loss: 3.732684373855591  Test loss: 3.753237724304199 \n",
      "Epoch: 1/10:  mini-batch 3400/4459:  Train loss: 3.585031509399414  Test loss: 3.7269206047058105 \n",
      "Epoch: 1/10:  mini-batch 3401/4459:  Train loss: 3.6957616806030273  Test loss: 3.702241897583008 \n",
      "Epoch: 1/10:  mini-batch 3402/4459:  Train loss: 3.4390711784362793  Test loss: 3.6803948879241943 \n",
      "Epoch: 1/10:  mini-batch 3403/4459:  Train loss: 3.5164101123809814  Test loss: 3.6615045070648193 \n",
      "Epoch: 1/10:  mini-batch 3404/4459:  Train loss: 3.568850040435791  Test loss: 3.645509719848633 \n",
      "Epoch: 1/10:  mini-batch 3405/4459:  Train loss: 3.4499237537384033  Test loss: 3.6314337253570557 \n",
      "Epoch: 1/10:  mini-batch 3406/4459:  Train loss: 3.7702674865722656  Test loss: 3.6183671951293945 \n",
      "Epoch: 1/10:  mini-batch 3407/4459:  Train loss: 3.749673843383789  Test loss: 3.6067817211151123 \n",
      "Epoch: 1/10:  mini-batch 3408/4459:  Train loss: 3.3659167289733887  Test loss: 3.5977401733398438 \n",
      "Epoch: 1/10:  mini-batch 3409/4459:  Train loss: 3.3308606147766113  Test loss: 3.5902602672576904 \n",
      "Epoch: 1/10:  mini-batch 3410/4459:  Train loss: 3.683194160461426  Test loss: 3.583219051361084 \n",
      "Epoch: 1/10:  mini-batch 3411/4459:  Train loss: 3.7002086639404297  Test loss: 3.5765786170959473 \n",
      "Epoch: 1/10:  mini-batch 3412/4459:  Train loss: 3.510127067565918  Test loss: 3.5704827308654785 \n",
      "Epoch: 1/10:  mini-batch 3413/4459:  Train loss: 3.4678409099578857  Test loss: 3.565955877304077 \n",
      "Epoch: 1/10:  mini-batch 3414/4459:  Train loss: 3.3981878757476807  Test loss: 3.562591791152954 \n",
      "Epoch: 1/10:  mini-batch 3415/4459:  Train loss: 3.5536937713623047  Test loss: 3.558868169784546 \n",
      "Epoch: 1/10:  mini-batch 3416/4459:  Train loss: 3.5767104625701904  Test loss: 3.5559334754943848 \n",
      "Epoch: 1/10:  mini-batch 3417/4459:  Train loss: 3.577507972717285  Test loss: 3.5530009269714355 \n",
      "Epoch: 1/10:  mini-batch 3418/4459:  Train loss: 3.5036683082580566  Test loss: 3.5507962703704834 \n",
      "Epoch: 1/10:  mini-batch 3419/4459:  Train loss: 3.5044193267822266  Test loss: 3.5484886169433594 \n",
      "Epoch: 1/10:  mini-batch 3420/4459:  Train loss: 3.3398454189300537  Test loss: 3.54646897315979 \n",
      "Epoch: 1/10:  mini-batch 3421/4459:  Train loss: 3.6051084995269775  Test loss: 3.544198513031006 \n",
      "Epoch: 1/10:  mini-batch 3422/4459:  Train loss: 3.523376226425171  Test loss: 3.5418500900268555 \n",
      "Epoch: 1/10:  mini-batch 3423/4459:  Train loss: 3.5193886756896973  Test loss: 3.5398852825164795 \n",
      "Epoch: 1/10:  mini-batch 3424/4459:  Train loss: 3.492034912109375  Test loss: 3.53835391998291 \n",
      "Epoch: 1/10:  mini-batch 3425/4459:  Train loss: 3.49338960647583  Test loss: 3.536428689956665 \n",
      "Epoch: 1/10:  mini-batch 3426/4459:  Train loss: 3.5214221477508545  Test loss: 3.5343949794769287 \n",
      "Epoch: 1/10:  mini-batch 3427/4459:  Train loss: 3.31938099861145  Test loss: 3.5325376987457275 \n",
      "Epoch: 1/10:  mini-batch 3428/4459:  Train loss: 3.5027074813842773  Test loss: 3.5298900604248047 \n",
      "Epoch: 1/10:  mini-batch 3429/4459:  Train loss: 3.5326528549194336  Test loss: 3.527086019515991 \n",
      "Epoch: 1/10:  mini-batch 3430/4459:  Train loss: 3.516073226928711  Test loss: 3.523764133453369 \n",
      "Epoch: 1/10:  mini-batch 3431/4459:  Train loss: 3.4765517711639404  Test loss: 3.5206305980682373 \n",
      "Epoch: 1/10:  mini-batch 3432/4459:  Train loss: 3.5214972496032715  Test loss: 3.517165184020996 \n",
      "Epoch: 1/10:  mini-batch 3433/4459:  Train loss: 3.4605047702789307  Test loss: 3.5131895542144775 \n",
      "Epoch: 1/10:  mini-batch 3434/4459:  Train loss: 3.4620561599731445  Test loss: 3.5086488723754883 \n",
      "Epoch: 1/10:  mini-batch 3435/4459:  Train loss: 3.579091787338257  Test loss: 3.5040793418884277 \n",
      "Epoch: 1/10:  mini-batch 3436/4459:  Train loss: 3.441138982772827  Test loss: 3.4989914894104004 \n",
      "Epoch: 1/10:  mini-batch 3437/4459:  Train loss: 3.4602389335632324  Test loss: 3.4937384128570557 \n",
      "Epoch: 1/10:  mini-batch 3438/4459:  Train loss: 3.442276954650879  Test loss: 3.488456964492798 \n",
      "Epoch: 1/10:  mini-batch 3439/4459:  Train loss: 3.5931355953216553  Test loss: 3.48337459564209 \n",
      "Epoch: 1/10:  mini-batch 3440/4459:  Train loss: 3.3909997940063477  Test loss: 3.478875160217285 \n",
      "Epoch: 1/10:  mini-batch 3441/4459:  Train loss: 3.501195192337036  Test loss: 3.4750499725341797 \n",
      "Epoch: 1/10:  mini-batch 3442/4459:  Train loss: 3.4711077213287354  Test loss: 3.472121000289917 \n",
      "Epoch: 1/10:  mini-batch 3443/4459:  Train loss: 3.299762487411499  Test loss: 3.4687881469726562 \n",
      "Epoch: 1/10:  mini-batch 3444/4459:  Train loss: 3.3338701725006104  Test loss: 3.4650282859802246 \n",
      "Epoch: 1/10:  mini-batch 3445/4459:  Train loss: 3.404170513153076  Test loss: 3.4620773792266846 \n",
      "Epoch: 1/10:  mini-batch 3446/4459:  Train loss: 3.449303150177002  Test loss: 3.4591073989868164 \n",
      "Epoch: 1/10:  mini-batch 3447/4459:  Train loss: 3.39815092086792  Test loss: 3.4556949138641357 \n",
      "Epoch: 1/10:  mini-batch 3448/4459:  Train loss: 3.525806427001953  Test loss: 3.4533638954162598 \n",
      "Epoch: 1/10:  mini-batch 3449/4459:  Train loss: 3.300351142883301  Test loss: 3.450101852416992 \n",
      "Epoch: 1/10:  mini-batch 3450/4459:  Train loss: 3.4413349628448486  Test loss: 3.447084426879883 \n",
      "Epoch: 1/10:  mini-batch 3451/4459:  Train loss: 3.403585910797119  Test loss: 3.4439144134521484 \n",
      "Epoch: 1/10:  mini-batch 3452/4459:  Train loss: 3.28334379196167  Test loss: 3.4406046867370605 \n",
      "Epoch: 1/10:  mini-batch 3453/4459:  Train loss: 3.5756278038024902  Test loss: 3.438236713409424 \n",
      "Epoch: 1/10:  mini-batch 3454/4459:  Train loss: 3.279439926147461  Test loss: 3.43560791015625 \n",
      "Epoch: 1/10:  mini-batch 3455/4459:  Train loss: 3.4349582195281982  Test loss: 3.434116840362549 \n",
      "Epoch: 1/10:  mini-batch 3456/4459:  Train loss: 3.355862617492676  Test loss: 3.432103157043457 \n",
      "Epoch: 1/10:  mini-batch 3457/4459:  Train loss: 3.2560877799987793  Test loss: 3.430013418197632 \n",
      "Epoch: 1/10:  mini-batch 3458/4459:  Train loss: 3.320065975189209  Test loss: 3.4276788234710693 \n",
      "Epoch: 1/10:  mini-batch 3459/4459:  Train loss: 3.4715561866760254  Test loss: 3.425945520401001 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3460/4459:  Train loss: 3.717836380004883  Test loss: 3.4246022701263428 \n",
      "Epoch: 1/10:  mini-batch 3461/4459:  Train loss: 3.45119571685791  Test loss: 3.424105644226074 \n",
      "Epoch: 1/10:  mini-batch 3462/4459:  Train loss: 3.486605167388916  Test loss: 3.4239284992218018 \n",
      "Epoch: 1/10:  mini-batch 3463/4459:  Train loss: 3.2062883377075195  Test loss: 3.424211025238037 \n",
      "Epoch: 1/10:  mini-batch 3464/4459:  Train loss: 3.641876697540283  Test loss: 3.4250710010528564 \n",
      "Epoch: 1/10:  mini-batch 3465/4459:  Train loss: 3.350090503692627  Test loss: 3.4261410236358643 \n",
      "Epoch: 1/10:  mini-batch 3466/4459:  Train loss: 3.261699914932251  Test loss: 3.4276819229125977 \n",
      "Epoch: 1/10:  mini-batch 3467/4459:  Train loss: 3.4701130390167236  Test loss: 3.4289684295654297 \n",
      "Epoch: 1/10:  mini-batch 3468/4459:  Train loss: 3.5389554500579834  Test loss: 3.4304165840148926 \n",
      "Epoch: 1/10:  mini-batch 3469/4459:  Train loss: 3.4808568954467773  Test loss: 3.4311952590942383 \n",
      "Epoch: 1/10:  mini-batch 3470/4459:  Train loss: 3.3462629318237305  Test loss: 3.431729316711426 \n",
      "Epoch: 1/10:  mini-batch 3471/4459:  Train loss: 3.5944342613220215  Test loss: 3.432288646697998 \n",
      "Epoch: 1/10:  mini-batch 3472/4459:  Train loss: 3.458439826965332  Test loss: 3.432516574859619 \n",
      "Epoch: 1/10:  mini-batch 3473/4459:  Train loss: 3.299017906188965  Test loss: 3.4327549934387207 \n",
      "Epoch: 1/10:  mini-batch 3474/4459:  Train loss: 3.3276381492614746  Test loss: 3.4331438541412354 \n",
      "Epoch: 1/10:  mini-batch 3475/4459:  Train loss: 3.293978452682495  Test loss: 3.4344520568847656 \n",
      "Epoch: 1/10:  mini-batch 3476/4459:  Train loss: 3.4510090351104736  Test loss: 3.4350316524505615 \n",
      "Epoch: 1/10:  mini-batch 3477/4459:  Train loss: 3.33480167388916  Test loss: 3.4354000091552734 \n",
      "Epoch: 1/10:  mini-batch 3478/4459:  Train loss: 3.2297987937927246  Test loss: 3.434954881668091 \n",
      "Epoch: 1/10:  mini-batch 3479/4459:  Train loss: 3.384565830230713  Test loss: 3.434640884399414 \n",
      "Epoch: 1/10:  mini-batch 3480/4459:  Train loss: 3.4015071392059326  Test loss: 3.4335553646087646 \n",
      "Epoch: 1/10:  mini-batch 3481/4459:  Train loss: 3.232548713684082  Test loss: 3.431881904602051 \n",
      "Epoch: 1/10:  mini-batch 3482/4459:  Train loss: 3.163388729095459  Test loss: 3.4303736686706543 \n",
      "Epoch: 1/10:  mini-batch 3483/4459:  Train loss: 3.3134117126464844  Test loss: 3.4289798736572266 \n",
      "Epoch: 1/10:  mini-batch 3484/4459:  Train loss: 3.241595506668091  Test loss: 3.4275755882263184 \n",
      "Epoch: 1/10:  mini-batch 3485/4459:  Train loss: 2.9532434940338135  Test loss: 3.426668405532837 \n",
      "Epoch: 1/10:  mini-batch 3486/4459:  Train loss: 3.085707664489746  Test loss: 3.4260265827178955 \n",
      "Epoch: 1/10:  mini-batch 3487/4459:  Train loss: 2.985985040664673  Test loss: 3.4258673191070557 \n",
      "Epoch: 1/10:  mini-batch 3488/4459:  Train loss: 3.4247498512268066  Test loss: 3.4249160289764404 \n",
      "Epoch: 1/10:  mini-batch 3489/4459:  Train loss: 3.4255590438842773  Test loss: 3.4244604110717773 \n",
      "Epoch: 1/10:  mini-batch 3490/4459:  Train loss: 3.2604939937591553  Test loss: 3.4232873916625977 \n",
      "Epoch: 1/10:  mini-batch 3491/4459:  Train loss: 3.2827725410461426  Test loss: 3.421220302581787 \n",
      "Epoch: 1/10:  mini-batch 3492/4459:  Train loss: 3.062598943710327  Test loss: 3.4209766387939453 \n",
      "Epoch: 1/10:  mini-batch 3493/4459:  Train loss: 3.6592531204223633  Test loss: 3.4190986156463623 \n",
      "Epoch: 1/10:  mini-batch 3494/4459:  Train loss: 3.0463156700134277  Test loss: 3.4173622131347656 \n",
      "Epoch: 1/10:  mini-batch 3495/4459:  Train loss: 3.4837779998779297  Test loss: 3.414829969406128 \n",
      "Epoch: 1/10:  mini-batch 3496/4459:  Train loss: 3.2009997367858887  Test loss: 3.413353204727173 \n",
      "Epoch: 1/10:  mini-batch 3497/4459:  Train loss: 3.873708724975586  Test loss: 3.411372184753418 \n",
      "Epoch: 1/10:  mini-batch 3498/4459:  Train loss: 3.084162950515747  Test loss: 3.410567045211792 \n",
      "Epoch: 1/10:  mini-batch 3499/4459:  Train loss: 2.8080742359161377  Test loss: 3.4116578102111816 \n",
      "Epoch: 1/10:  mini-batch 3500/4459:  Train loss: 3.2619245052337646  Test loss: 3.4111435413360596 \n",
      "Epoch: 1/10:  mini-batch 3501/4459:  Train loss: 3.3892555236816406  Test loss: 3.410247325897217 \n",
      "Epoch: 1/10:  mini-batch 3502/4459:  Train loss: 3.4505679607391357  Test loss: 3.4083762168884277 \n",
      "Epoch: 1/10:  mini-batch 3523/4459:  Train loss: 3.402797222137451  Test loss: 3.347917079925537 \n",
      "Epoch: 1/10:  mini-batch 3524/4459:  Train loss: 3.4145989418029785  Test loss: 3.348935127258301 \n",
      "Epoch: 1/10:  mini-batch 3525/4459:  Train loss: 3.401456832885742  Test loss: 3.349360942840576 \n",
      "Epoch: 1/10:  mini-batch 3526/4459:  Train loss: 3.2812137603759766  Test loss: 3.3492965698242188 \n",
      "Epoch: 1/10:  mini-batch 3527/4459:  Train loss: 3.3737220764160156  Test loss: 3.34918212890625 \n",
      "Epoch: 1/10:  mini-batch 3528/4459:  Train loss: 3.110698699951172  Test loss: 3.3492910861968994 \n",
      "Epoch: 1/10:  mini-batch 3529/4459:  Train loss: 3.2205536365509033  Test loss: 3.3491430282592773 \n",
      "Epoch: 1/10:  mini-batch 3530/4459:  Train loss: 3.184157133102417  Test loss: 3.3485641479492188 \n",
      "Epoch: 1/10:  mini-batch 3531/4459:  Train loss: 3.0255582332611084  Test loss: 3.3481638431549072 \n",
      "Epoch: 1/10:  mini-batch 3532/4459:  Train loss: 3.2339906692504883  Test loss: 3.3480288982391357 \n",
      "Epoch: 1/10:  mini-batch 3533/4459:  Train loss: 3.2782018184661865  Test loss: 3.3482632637023926 \n",
      "Epoch: 1/10:  mini-batch 3534/4459:  Train loss: 3.3888726234436035  Test loss: 3.348421812057495 \n",
      "Epoch: 1/10:  mini-batch 3535/4459:  Train loss: 3.2896790504455566  Test loss: 3.3489372730255127 \n",
      "Epoch: 1/10:  mini-batch 3536/4459:  Train loss: 3.3879854679107666  Test loss: 3.348681926727295 \n",
      "Epoch: 1/10:  mini-batch 3537/4459:  Train loss: 3.7955029010772705  Test loss: 3.348602771759033 \n",
      "Epoch: 1/10:  mini-batch 3538/4459:  Train loss: 3.403975248336792  Test loss: 3.348559856414795 \n",
      "Epoch: 1/10:  mini-batch 3539/4459:  Train loss: 3.389051675796509  Test loss: 3.348282814025879 \n",
      "Epoch: 1/10:  mini-batch 3540/4459:  Train loss: 3.6605358123779297  Test loss: 3.3483171463012695 \n",
      "Epoch: 1/10:  mini-batch 3541/4459:  Train loss: 3.0475075244903564  Test loss: 3.3485896587371826 \n",
      "Epoch: 1/10:  mini-batch 3542/4459:  Train loss: 3.3772382736206055  Test loss: 3.3495230674743652 \n",
      "Epoch: 1/10:  mini-batch 3543/4459:  Train loss: 3.6253538131713867  Test loss: 3.35056209564209 \n",
      "Epoch: 1/10:  mini-batch 3544/4459:  Train loss: 3.311936855316162  Test loss: 3.3513593673706055 \n",
      "Epoch: 1/10:  mini-batch 3545/4459:  Train loss: 3.0218119621276855  Test loss: 3.352369785308838 \n",
      "Epoch: 1/10:  mini-batch 3546/4459:  Train loss: 3.256269693374634  Test loss: 3.353412628173828 \n",
      "Epoch: 1/10:  mini-batch 3547/4459:  Train loss: 3.083002805709839  Test loss: 3.3546206951141357 \n",
      "Epoch: 1/10:  mini-batch 3548/4459:  Train loss: 3.075925350189209  Test loss: 3.355736017227173 \n",
      "Epoch: 1/10:  mini-batch 3549/4459:  Train loss: 3.2820820808410645  Test loss: 3.3568975925445557 \n",
      "Epoch: 1/10:  mini-batch 3550/4459:  Train loss: 3.3864643573760986  Test loss: 3.3582472801208496 \n",
      "Epoch: 1/10:  mini-batch 3551/4459:  Train loss: 3.3617725372314453  Test loss: 3.3595290184020996 \n",
      "Epoch: 1/10:  mini-batch 3552/4459:  Train loss: 3.794445514678955  Test loss: 3.3616890907287598 \n",
      "Epoch: 1/10:  mini-batch 3553/4459:  Train loss: 3.324265241622925  Test loss: 3.3629283905029297 \n",
      "Epoch: 1/10:  mini-batch 3554/4459:  Train loss: 3.4790070056915283  Test loss: 3.3643932342529297 \n",
      "Epoch: 1/10:  mini-batch 3555/4459:  Train loss: 3.3330116271972656  Test loss: 3.365443706512451 \n",
      "Epoch: 1/10:  mini-batch 3556/4459:  Train loss: 3.2740752696990967  Test loss: 3.3656861782073975 \n",
      "Epoch: 1/10:  mini-batch 3557/4459:  Train loss: 3.2466344833374023  Test loss: 3.365945339202881 \n",
      "Epoch: 1/10:  mini-batch 3558/4459:  Train loss: 3.103125810623169  Test loss: 3.3663973808288574 \n",
      "Epoch: 1/10:  mini-batch 3559/4459:  Train loss: 3.4838593006134033  Test loss: 3.3668441772460938 \n",
      "Epoch: 1/10:  mini-batch 3560/4459:  Train loss: 3.443821907043457  Test loss: 3.3666625022888184 \n",
      "Epoch: 1/10:  mini-batch 3561/4459:  Train loss: 3.3206534385681152  Test loss: 3.3659870624542236 \n",
      "Epoch: 1/10:  mini-batch 3562/4459:  Train loss: 3.1206278800964355  Test loss: 3.3656866550445557 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3563/4459:  Train loss: 3.1044511795043945  Test loss: 3.365931510925293 \n",
      "Epoch: 1/10:  mini-batch 3564/4459:  Train loss: 2.8734893798828125  Test loss: 3.366652250289917 \n",
      "Epoch: 1/10:  mini-batch 3565/4459:  Train loss: 3.100965976715088  Test loss: 3.3675804138183594 \n",
      "Epoch: 1/10:  mini-batch 3566/4459:  Train loss: 3.633141040802002  Test loss: 3.369018077850342 \n",
      "Epoch: 1/10:  mini-batch 3567/4459:  Train loss: 3.7860500812530518  Test loss: 3.370663642883301 \n",
      "Epoch: 1/10:  mini-batch 3568/4459:  Train loss: 3.371947765350342  Test loss: 3.371964454650879 \n",
      "Epoch: 1/10:  mini-batch 3569/4459:  Train loss: 3.8232946395874023  Test loss: 3.3735432624816895 \n",
      "Epoch: 1/10:  mini-batch 3570/4459:  Train loss: 3.5245509147644043  Test loss: 3.3740639686584473 \n",
      "Epoch: 1/10:  mini-batch 3571/4459:  Train loss: 3.2081189155578613  Test loss: 3.374788284301758 \n",
      "Epoch: 1/10:  mini-batch 3572/4459:  Train loss: 3.0864944458007812  Test loss: 3.3754148483276367 \n",
      "Epoch: 1/10:  mini-batch 3573/4459:  Train loss: 3.18131160736084  Test loss: 3.3755693435668945 \n",
      "Epoch: 1/10:  mini-batch 3574/4459:  Train loss: 3.137953758239746  Test loss: 3.3755054473876953 \n",
      "Epoch: 1/10:  mini-batch 3575/4459:  Train loss: 3.3464725017547607  Test loss: 3.3754000663757324 \n",
      "Epoch: 1/10:  mini-batch 3576/4459:  Train loss: 3.1959238052368164  Test loss: 3.375305414199829 \n",
      "Epoch: 1/10:  mini-batch 3577/4459:  Train loss: 3.288917064666748  Test loss: 3.3750219345092773 \n",
      "Epoch: 1/10:  mini-batch 3578/4459:  Train loss: 3.398219585418701  Test loss: 3.3750343322753906 \n",
      "Epoch: 1/10:  mini-batch 3579/4459:  Train loss: 3.5978331565856934  Test loss: 3.3749516010284424 \n",
      "Epoch: 1/10:  mini-batch 3580/4459:  Train loss: 3.4297127723693848  Test loss: 3.3750479221343994 \n",
      "Epoch: 1/10:  mini-batch 3581/4459:  Train loss: 3.2956066131591797  Test loss: 3.375399112701416 \n",
      "Epoch: 1/10:  mini-batch 3582/4459:  Train loss: 3.4078142642974854  Test loss: 3.3747167587280273 \n",
      "Epoch: 1/10:  mini-batch 3583/4459:  Train loss: 3.0386404991149902  Test loss: 3.3743479251861572 \n",
      "Epoch: 1/10:  mini-batch 3584/4459:  Train loss: 3.567811965942383  Test loss: 3.37371826171875 \n",
      "Epoch: 1/10:  mini-batch 3585/4459:  Train loss: 3.491577386856079  Test loss: 3.37361741065979 \n",
      "Epoch: 1/10:  mini-batch 3586/4459:  Train loss: 3.546809673309326  Test loss: 3.374392032623291 \n",
      "Epoch: 1/10:  mini-batch 3587/4459:  Train loss: 3.5316171646118164  Test loss: 3.3754796981811523 \n",
      "Epoch: 1/10:  mini-batch 3588/4459:  Train loss: 3.4021811485290527  Test loss: 3.3761191368103027 \n",
      "Epoch: 1/10:  mini-batch 3589/4459:  Train loss: 3.3133797645568848  Test loss: 3.3766732215881348 \n",
      "Epoch: 1/10:  mini-batch 3590/4459:  Train loss: 3.462031841278076  Test loss: 3.376776933670044 \n",
      "Epoch: 1/10:  mini-batch 3591/4459:  Train loss: 3.4392786026000977  Test loss: 3.376701831817627 \n",
      "Epoch: 1/10:  mini-batch 3592/4459:  Train loss: 3.3654656410217285  Test loss: 3.376411199569702 \n",
      "Epoch: 1/10:  mini-batch 3593/4459:  Train loss: 3.4229326248168945  Test loss: 3.376325845718384 \n",
      "Epoch: 1/10:  mini-batch 3594/4459:  Train loss: 3.4196832180023193  Test loss: 3.377004384994507 \n",
      "Epoch: 1/10:  mini-batch 3595/4459:  Train loss: 3.46478009223938  Test loss: 3.3775272369384766 \n",
      "Epoch: 1/10:  mini-batch 3596/4459:  Train loss: 3.26005220413208  Test loss: 3.3780856132507324 \n",
      "Epoch: 1/10:  mini-batch 3597/4459:  Train loss: 3.0639381408691406  Test loss: 3.378824472427368 \n",
      "Epoch: 1/10:  mini-batch 3598/4459:  Train loss: 3.1104085445404053  Test loss: 3.3791275024414062 \n",
      "Epoch: 1/10:  mini-batch 3599/4459:  Train loss: 3.2973413467407227  Test loss: 3.3791286945343018 \n",
      "Epoch: 1/10:  mini-batch 3600/4459:  Train loss: 3.4542880058288574  Test loss: 3.3786895275115967 \n",
      "Epoch: 1/10:  mini-batch 3601/4459:  Train loss: 3.2576422691345215  Test loss: 3.378558397293091 \n",
      "Epoch: 1/10:  mini-batch 3602/4459:  Train loss: 3.1076252460479736  Test loss: 3.378525972366333 \n",
      "Epoch: 1/10:  mini-batch 3603/4459:  Train loss: 3.3301591873168945  Test loss: 3.3780627250671387 \n",
      "Epoch: 1/10:  mini-batch 3604/4459:  Train loss: 2.864534854888916  Test loss: 3.3779163360595703 \n",
      "Epoch: 1/10:  mini-batch 3605/4459:  Train loss: 3.5586109161376953  Test loss: 3.3780899047851562 \n",
      "Epoch: 1/10:  mini-batch 3606/4459:  Train loss: 3.301102876663208  Test loss: 3.378288984298706 \n",
      "Epoch: 1/10:  mini-batch 3607/4459:  Train loss: 3.0148792266845703  Test loss: 3.378697633743286 \n",
      "Epoch: 1/10:  mini-batch 3608/4459:  Train loss: 3.1002039909362793  Test loss: 3.379432201385498 \n",
      "Epoch: 1/10:  mini-batch 3609/4459:  Train loss: 3.331973075866699  Test loss: 3.3799569606781006 \n",
      "Epoch: 1/10:  mini-batch 3610/4459:  Train loss: 3.313363552093506  Test loss: 3.38081693649292 \n",
      "Epoch: 1/10:  mini-batch 3611/4459:  Train loss: 3.5905933380126953  Test loss: 3.3814244270324707 \n",
      "Epoch: 1/10:  mini-batch 3612/4459:  Train loss: 2.9805498123168945  Test loss: 3.382509231567383 \n",
      "Epoch: 1/10:  mini-batch 3613/4459:  Train loss: 3.412585735321045  Test loss: 3.3840792179107666 \n",
      "Epoch: 1/10:  mini-batch 3614/4459:  Train loss: 3.369356155395508  Test loss: 3.385751724243164 \n",
      "Epoch: 1/10:  mini-batch 3615/4459:  Train loss: 3.6075563430786133  Test loss: 3.386972427368164 \n",
      "Epoch: 1/10:  mini-batch 3616/4459:  Train loss: 3.01979398727417  Test loss: 3.388160228729248 \n",
      "Epoch: 1/10:  mini-batch 3617/4459:  Train loss: 3.070645332336426  Test loss: 3.3889968395233154 \n",
      "Epoch: 1/10:  mini-batch 3618/4459:  Train loss: 3.262587547302246  Test loss: 3.3894293308258057 \n",
      "Epoch: 1/10:  mini-batch 3619/4459:  Train loss: 3.344511032104492  Test loss: 3.390028953552246 \n",
      "Epoch: 1/10:  mini-batch 3620/4459:  Train loss: 3.593883514404297  Test loss: 3.390465259552002 \n",
      "Epoch: 1/10:  mini-batch 3621/4459:  Train loss: 3.195838212966919  Test loss: 3.3905060291290283 \n",
      "Epoch: 1/10:  mini-batch 3622/4459:  Train loss: 3.1837804317474365  Test loss: 3.390514850616455 \n",
      "Epoch: 1/10:  mini-batch 3623/4459:  Train loss: 3.490346670150757  Test loss: 3.3901898860931396 \n",
      "Epoch: 1/10:  mini-batch 3624/4459:  Train loss: 3.2472715377807617  Test loss: 3.3902783393859863 \n",
      "Epoch: 1/10:  mini-batch 3625/4459:  Train loss: 3.298647880554199  Test loss: 3.3901419639587402 \n",
      "Epoch: 1/10:  mini-batch 3626/4459:  Train loss: 3.2767274379730225  Test loss: 3.3901753425598145 \n",
      "Epoch: 1/10:  mini-batch 3627/4459:  Train loss: 2.8125174045562744  Test loss: 3.390347719192505 \n",
      "Epoch: 1/10:  mini-batch 3628/4459:  Train loss: 2.849431037902832  Test loss: 3.3913516998291016 \n",
      "Epoch: 1/10:  mini-batch 3629/4459:  Train loss: 3.1255671977996826  Test loss: 3.3925623893737793 \n",
      "Epoch: 1/10:  mini-batch 3630/4459:  Train loss: 2.9289283752441406  Test loss: 3.3944051265716553 \n",
      "Epoch: 1/10:  mini-batch 3631/4459:  Train loss: 2.9378442764282227  Test loss: 3.3968024253845215 \n",
      "Epoch: 1/10:  mini-batch 3632/4459:  Train loss: 3.005168914794922  Test loss: 3.3993008136749268 \n",
      "Epoch: 1/10:  mini-batch 3633/4459:  Train loss: 3.5247535705566406  Test loss: 3.4014806747436523 \n",
      "Epoch: 1/10:  mini-batch 3634/4459:  Train loss: 3.1194639205932617  Test loss: 3.40377140045166 \n",
      "Epoch: 1/10:  mini-batch 3635/4459:  Train loss: 3.0704641342163086  Test loss: 3.4065611362457275 \n",
      "Epoch: 1/10:  mini-batch 3636/4459:  Train loss: 3.6107282638549805  Test loss: 3.4080376625061035 \n",
      "Epoch: 1/10:  mini-batch 3637/4459:  Train loss: 2.8748526573181152  Test loss: 3.4104678630828857 \n",
      "Epoch: 1/10:  mini-batch 3638/4459:  Train loss: 3.718740224838257  Test loss: 3.41048526763916 \n",
      "Epoch: 1/10:  mini-batch 3639/4459:  Train loss: 3.7059831619262695  Test loss: 3.4095916748046875 \n",
      "Epoch: 1/10:  mini-batch 3640/4459:  Train loss: 3.1635537147521973  Test loss: 3.408834934234619 \n",
      "Epoch: 1/10:  mini-batch 3641/4459:  Train loss: 3.2466745376586914  Test loss: 3.407233238220215 \n",
      "Epoch: 1/10:  mini-batch 3642/4459:  Train loss: 3.375485897064209  Test loss: 3.4055728912353516 \n",
      "Epoch: 1/10:  mini-batch 3643/4459:  Train loss: 3.545490264892578  Test loss: 3.403743267059326 \n",
      "Epoch: 1/10:  mini-batch 3644/4459:  Train loss: 3.3943800926208496  Test loss: 3.4007675647735596 \n",
      "Epoch: 1/10:  mini-batch 3645/4459:  Train loss: 3.517643690109253  Test loss: 3.398350238800049 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3646/4459:  Train loss: 3.44114351272583  Test loss: 3.395453929901123 \n",
      "Epoch: 1/10:  mini-batch 3647/4459:  Train loss: 3.586117744445801  Test loss: 3.392385959625244 \n",
      "Epoch: 1/10:  mini-batch 3648/4459:  Train loss: 3.464106798171997  Test loss: 3.3891868591308594 \n",
      "Epoch: 1/10:  mini-batch 3649/4459:  Train loss: 3.067755699157715  Test loss: 3.3866491317749023 \n",
      "Epoch: 1/10:  mini-batch 3650/4459:  Train loss: 3.878249168395996  Test loss: 3.383690357208252 \n",
      "Epoch: 1/10:  mini-batch 3651/4459:  Train loss: 3.1223301887512207  Test loss: 3.3812665939331055 \n",
      "Epoch: 1/10:  mini-batch 3652/4459:  Train loss: 3.1212804317474365  Test loss: 3.379770278930664 \n",
      "Epoch: 1/10:  mini-batch 3653/4459:  Train loss: 3.2339749336242676  Test loss: 3.3791234493255615 \n",
      "Epoch: 1/10:  mini-batch 3654/4459:  Train loss: 3.7045722007751465  Test loss: 3.377894639968872 \n",
      "Epoch: 1/10:  mini-batch 3655/4459:  Train loss: 3.088019609451294  Test loss: 3.3767590522766113 \n",
      "Epoch: 1/10:  mini-batch 3656/4459:  Train loss: 3.4797966480255127  Test loss: 3.37538480758667 \n",
      "Epoch: 1/10:  mini-batch 3657/4459:  Train loss: 3.371148109436035  Test loss: 3.374373435974121 \n",
      "Epoch: 1/10:  mini-batch 3658/4459:  Train loss: 4.0062255859375  Test loss: 3.3732409477233887 \n",
      "Epoch: 1/10:  mini-batch 3659/4459:  Train loss: 3.121366024017334  Test loss: 3.372732162475586 \n",
      "Epoch: 1/10:  mini-batch 3660/4459:  Train loss: 3.2241008281707764  Test loss: 3.372312545776367 \n",
      "Epoch: 1/10:  mini-batch 3661/4459:  Train loss: 3.4939332008361816  Test loss: 3.3718647956848145 \n",
      "Epoch: 1/10:  mini-batch 3662/4459:  Train loss: 3.2895889282226562  Test loss: 3.371509552001953 \n",
      "Epoch: 1/10:  mini-batch 3663/4459:  Train loss: 3.39376163482666  Test loss: 3.3713600635528564 \n",
      "Epoch: 1/10:  mini-batch 3664/4459:  Train loss: 3.3723249435424805  Test loss: 3.370840072631836 \n",
      "Epoch: 1/10:  mini-batch 3665/4459:  Train loss: 3.1272506713867188  Test loss: 3.370299816131592 \n",
      "Epoch: 1/10:  mini-batch 3666/4459:  Train loss: 3.229145050048828  Test loss: 3.3695805072784424 \n",
      "Epoch: 1/10:  mini-batch 3667/4459:  Train loss: 3.354290008544922  Test loss: 3.3687713146209717 \n",
      "Epoch: 1/10:  mini-batch 3668/4459:  Train loss: 2.8964149951934814  Test loss: 3.3680784702301025 \n",
      "Epoch: 1/10:  mini-batch 3669/4459:  Train loss: 3.397073745727539  Test loss: 3.367769718170166 \n",
      "Epoch: 1/10:  mini-batch 3670/4459:  Train loss: 3.164785623550415  Test loss: 3.367560863494873 \n",
      "Epoch: 1/10:  mini-batch 3671/4459:  Train loss: 3.2570180892944336  Test loss: 3.3670949935913086 \n",
      "Epoch: 1/10:  mini-batch 3672/4459:  Train loss: 3.368579864501953  Test loss: 3.3670244216918945 \n",
      "Epoch: 1/10:  mini-batch 3673/4459:  Train loss: 3.0958361625671387  Test loss: 3.367011547088623 \n",
      "Epoch: 1/10:  mini-batch 3674/4459:  Train loss: 3.1564011573791504  Test loss: 3.3669586181640625 \n",
      "Epoch: 1/10:  mini-batch 3675/4459:  Train loss: 3.5105295181274414  Test loss: 3.3674073219299316 \n",
      "Epoch: 1/10:  mini-batch 3676/4459:  Train loss: 3.2391042709350586  Test loss: 3.3680500984191895 \n",
      "Epoch: 1/10:  mini-batch 3677/4459:  Train loss: 3.276423931121826  Test loss: 3.3682022094726562 \n",
      "Epoch: 1/10:  mini-batch 3678/4459:  Train loss: 3.5571324825286865  Test loss: 3.36826753616333 \n",
      "Epoch: 1/10:  mini-batch 3679/4459:  Train loss: 3.1572458744049072  Test loss: 3.368482828140259 \n",
      "Epoch: 1/10:  mini-batch 3680/4459:  Train loss: 3.321514844894409  Test loss: 3.368720054626465 \n",
      "Epoch: 1/10:  mini-batch 3681/4459:  Train loss: 3.4297125339508057  Test loss: 3.3688831329345703 \n",
      "Epoch: 1/10:  mini-batch 3682/4459:  Train loss: 3.35722017288208  Test loss: 3.3691515922546387 \n",
      "Epoch: 1/10:  mini-batch 3683/4459:  Train loss: 3.2971909046173096  Test loss: 3.369358777999878 \n",
      "Epoch: 1/10:  mini-batch 3684/4459:  Train loss: 2.9635159969329834  Test loss: 3.3694591522216797 \n",
      "Epoch: 1/10:  mini-batch 3685/4459:  Train loss: 3.1429710388183594  Test loss: 3.3690173625946045 \n",
      "Epoch: 1/10:  mini-batch 3686/4459:  Train loss: 3.195631980895996  Test loss: 3.368335247039795 \n",
      "Epoch: 1/10:  mini-batch 3687/4459:  Train loss: 3.0506017208099365  Test loss: 3.367882251739502 \n",
      "Epoch: 1/10:  mini-batch 3688/4459:  Train loss: 3.12453031539917  Test loss: 3.367532253265381 \n",
      "Epoch: 1/10:  mini-batch 3689/4459:  Train loss: 3.625415563583374  Test loss: 3.366920232772827 \n",
      "Epoch: 1/10:  mini-batch 3690/4459:  Train loss: 3.5562524795532227  Test loss: 3.36629056930542 \n",
      "Epoch: 1/10:  mini-batch 3691/4459:  Train loss: 3.219353199005127  Test loss: 3.365797996520996 \n",
      "Epoch: 1/10:  mini-batch 3692/4459:  Train loss: 3.124025344848633  Test loss: 3.365340232849121 \n",
      "Epoch: 1/10:  mini-batch 3693/4459:  Train loss: 3.1562438011169434  Test loss: 3.3647780418395996 \n",
      "Epoch: 1/10:  mini-batch 3694/4459:  Train loss: 3.307749032974243  Test loss: 3.3643975257873535 \n",
      "Epoch: 1/10:  mini-batch 3695/4459:  Train loss: 3.1177308559417725  Test loss: 3.3645596504211426 \n",
      "Epoch: 1/10:  mini-batch 3696/4459:  Train loss: 3.2843356132507324  Test loss: 3.3644776344299316 \n",
      "Epoch: 1/10:  mini-batch 3697/4459:  Train loss: 3.2261698246002197  Test loss: 3.3645901679992676 \n",
      "Epoch: 1/10:  mini-batch 3698/4459:  Train loss: 3.0720603466033936  Test loss: 3.365388870239258 \n",
      "Epoch: 1/10:  mini-batch 3699/4459:  Train loss: 3.2678275108337402  Test loss: 3.366015672683716 \n",
      "Epoch: 1/10:  mini-batch 3700/4459:  Train loss: 2.945680618286133  Test loss: 3.3669328689575195 \n",
      "Epoch: 1/10:  mini-batch 3701/4459:  Train loss: 3.357637405395508  Test loss: 3.367155075073242 \n",
      "Epoch: 1/10:  mini-batch 3702/4459:  Train loss: 3.163857936859131  Test loss: 3.3673648834228516 \n",
      "Epoch: 1/10:  mini-batch 3703/4459:  Train loss: 4.029985427856445  Test loss: 3.366398334503174 \n",
      "Epoch: 1/10:  mini-batch 3704/4459:  Train loss: 2.8662219047546387  Test loss: 3.3660268783569336 \n",
      "Epoch: 1/10:  mini-batch 3705/4459:  Train loss: 3.5164847373962402  Test loss: 3.365772247314453 \n",
      "Epoch: 1/10:  mini-batch 3706/4459:  Train loss: 3.702890396118164  Test loss: 3.365374803543091 \n",
      "Epoch: 1/10:  mini-batch 3707/4459:  Train loss: 3.330793857574463  Test loss: 3.3648908138275146 \n",
      "Epoch: 1/10:  mini-batch 3708/4459:  Train loss: 3.7336719036102295  Test loss: 3.3648548126220703 \n",
      "Epoch: 1/10:  mini-batch 3709/4459:  Train loss: 3.3733887672424316  Test loss: 3.364956855773926 \n",
      "Epoch: 1/10:  mini-batch 3710/4459:  Train loss: 3.37485671043396  Test loss: 3.365213632583618 \n",
      "Epoch: 1/10:  mini-batch 3711/4459:  Train loss: 3.4009599685668945  Test loss: 3.3653955459594727 \n",
      "Epoch: 1/10:  mini-batch 3712/4459:  Train loss: 3.8392281532287598  Test loss: 3.3655662536621094 \n",
      "Epoch: 1/10:  mini-batch 3713/4459:  Train loss: 3.46022629737854  Test loss: 3.366206645965576 \n",
      "Epoch: 1/10:  mini-batch 3714/4459:  Train loss: 3.4950385093688965  Test loss: 3.3667654991149902 \n",
      "Epoch: 1/10:  mini-batch 3715/4459:  Train loss: 3.65360164642334  Test loss: 3.3673202991485596 \n",
      "Epoch: 1/10:  mini-batch 3716/4459:  Train loss: 3.4272825717926025  Test loss: 3.3678131103515625 \n",
      "Epoch: 1/10:  mini-batch 3717/4459:  Train loss: 3.3682663440704346  Test loss: 3.3683929443359375 \n",
      "Epoch: 1/10:  mini-batch 3718/4459:  Train loss: 3.375166893005371  Test loss: 3.369107723236084 \n",
      "Epoch: 1/10:  mini-batch 3719/4459:  Train loss: 2.9916741847991943  Test loss: 3.3698220252990723 \n",
      "Epoch: 1/10:  mini-batch 3720/4459:  Train loss: 2.949108123779297  Test loss: 3.3704450130462646 \n",
      "Epoch: 1/10:  mini-batch 3721/4459:  Train loss: 3.1147022247314453  Test loss: 3.370328426361084 \n",
      "Epoch: 1/10:  mini-batch 3722/4459:  Train loss: 3.299203395843506  Test loss: 3.370551586151123 \n",
      "Epoch: 1/10:  mini-batch 3723/4459:  Train loss: 3.3464150428771973  Test loss: 3.370685577392578 \n",
      "Epoch: 1/10:  mini-batch 3724/4459:  Train loss: 3.258410692214966  Test loss: 3.3711743354797363 \n",
      "Epoch: 1/10:  mini-batch 3725/4459:  Train loss: 3.524548292160034  Test loss: 3.3721556663513184 \n",
      "Epoch: 1/10:  mini-batch 3726/4459:  Train loss: 3.4678332805633545  Test loss: 3.373063802719116 \n",
      "Epoch: 1/10:  mini-batch 3727/4459:  Train loss: 3.422247886657715  Test loss: 3.373941421508789 \n",
      "Epoch: 1/10:  mini-batch 3728/4459:  Train loss: 3.2949466705322266  Test loss: 3.37465238571167 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3729/4459:  Train loss: 3.3395628929138184  Test loss: 3.3751349449157715 \n",
      "Epoch: 1/10:  mini-batch 3730/4459:  Train loss: 3.174386501312256  Test loss: 3.3755340576171875 \n",
      "Epoch: 1/10:  mini-batch 3731/4459:  Train loss: 3.5631356239318848  Test loss: 3.3759708404541016 \n",
      "Epoch: 1/10:  mini-batch 3732/4459:  Train loss: 3.350766181945801  Test loss: 3.3764052391052246 \n",
      "Epoch: 1/10:  mini-batch 3733/4459:  Train loss: 3.154721736907959  Test loss: 3.376924753189087 \n",
      "Epoch: 1/10:  mini-batch 3734/4459:  Train loss: 3.242438793182373  Test loss: 3.377535104751587 \n",
      "Epoch: 1/10:  mini-batch 3735/4459:  Train loss: 3.270637035369873  Test loss: 3.3783082962036133 \n",
      "Epoch: 1/10:  mini-batch 3736/4459:  Train loss: 3.464625597000122  Test loss: 3.3792591094970703 \n",
      "Epoch: 1/10:  mini-batch 3737/4459:  Train loss: 3.823636293411255  Test loss: 3.3802859783172607 \n",
      "Epoch: 1/10:  mini-batch 3738/4459:  Train loss: 3.334959030151367  Test loss: 3.381364107131958 \n",
      "Epoch: 1/10:  mini-batch 3739/4459:  Train loss: 3.2468490600585938  Test loss: 3.381925582885742 \n",
      "Epoch: 1/10:  mini-batch 3740/4459:  Train loss: 3.2107276916503906  Test loss: 3.3822805881500244 \n",
      "Epoch: 1/10:  mini-batch 3741/4459:  Train loss: 2.996915817260742  Test loss: 3.3824150562286377 \n",
      "Epoch: 1/10:  mini-batch 3742/4459:  Train loss: 3.417926788330078  Test loss: 3.3824126720428467 \n",
      "Epoch: 1/10:  mini-batch 3743/4459:  Train loss: 3.583594560623169  Test loss: 3.382504940032959 \n",
      "Epoch: 1/10:  mini-batch 3744/4459:  Train loss: 3.340386390686035  Test loss: 3.382495880126953 \n",
      "Epoch: 1/10:  mini-batch 3745/4459:  Train loss: 3.783525228500366  Test loss: 3.3826260566711426 \n",
      "Epoch: 1/10:  mini-batch 3746/4459:  Train loss: 2.969120979309082  Test loss: 3.38281512260437 \n",
      "Epoch: 1/10:  mini-batch 3747/4459:  Train loss: 3.234776496887207  Test loss: 3.3831303119659424 \n",
      "Epoch: 1/10:  mini-batch 3748/4459:  Train loss: 3.4838504791259766  Test loss: 3.3831262588500977 \n",
      "Epoch: 1/10:  mini-batch 3749/4459:  Train loss: 3.414886951446533  Test loss: 3.3831677436828613 \n",
      "Epoch: 1/10:  mini-batch 3750/4459:  Train loss: 3.4912009239196777  Test loss: 3.382983684539795 \n",
      "Epoch: 1/10:  mini-batch 3751/4459:  Train loss: 3.629720687866211  Test loss: 3.382779359817505 \n",
      "Epoch: 1/10:  mini-batch 3752/4459:  Train loss: 3.3847544193267822  Test loss: 3.3828814029693604 \n",
      "Epoch: 1/10:  mini-batch 3753/4459:  Train loss: 3.037886142730713  Test loss: 3.383090019226074 \n",
      "Epoch: 1/10:  mini-batch 3754/4459:  Train loss: 3.089301586151123  Test loss: 3.3828883171081543 \n",
      "Epoch: 1/10:  mini-batch 3755/4459:  Train loss: 3.5830180644989014  Test loss: 3.382659912109375 \n",
      "Epoch: 1/10:  mini-batch 3756/4459:  Train loss: 3.320256233215332  Test loss: 3.382554054260254 \n",
      "Epoch: 1/10:  mini-batch 3757/4459:  Train loss: 3.5221385955810547  Test loss: 3.3826651573181152 \n",
      "Epoch: 1/10:  mini-batch 3758/4459:  Train loss: 2.981398582458496  Test loss: 3.382880687713623 \n",
      "Epoch: 1/10:  mini-batch 3759/4459:  Train loss: 3.43499493598938  Test loss: 3.3831489086151123 \n",
      "Epoch: 1/10:  mini-batch 3760/4459:  Train loss: 3.245838165283203  Test loss: 3.3835442066192627 \n",
      "Epoch: 1/10:  mini-batch 3761/4459:  Train loss: 2.9020822048187256  Test loss: 3.3843417167663574 \n",
      "Epoch: 1/10:  mini-batch 3762/4459:  Train loss: 3.2976465225219727  Test loss: 3.3850159645080566 \n",
      "Epoch: 1/10:  mini-batch 3763/4459:  Train loss: 3.2264249324798584  Test loss: 3.385788917541504 \n",
      "Epoch: 1/10:  mini-batch 3764/4459:  Train loss: 3.7128701210021973  Test loss: 3.3864943981170654 \n",
      "Epoch: 1/10:  mini-batch 3765/4459:  Train loss: 3.6608633995056152  Test loss: 3.387230157852173 \n",
      "Epoch: 1/10:  mini-batch 3766/4459:  Train loss: 3.448554754257202  Test loss: 3.387765645980835 \n",
      "Epoch: 1/10:  mini-batch 3767/4459:  Train loss: 3.1957314014434814  Test loss: 3.388303756713867 \n",
      "Epoch: 1/10:  mini-batch 3768/4459:  Train loss: 3.242872714996338  Test loss: 3.3886613845825195 \n",
      "Epoch: 1/10:  mini-batch 3769/4459:  Train loss: 3.1765823364257812  Test loss: 3.3891358375549316 \n",
      "Epoch: 1/10:  mini-batch 3770/4459:  Train loss: 3.2575466632843018  Test loss: 3.389397621154785 \n",
      "Epoch: 1/10:  mini-batch 3771/4459:  Train loss: 3.3232998847961426  Test loss: 3.3891730308532715 \n",
      "Epoch: 1/10:  mini-batch 3772/4459:  Train loss: 3.518357753753662  Test loss: 3.3886725902557373 \n",
      "Epoch: 1/10:  mini-batch 3773/4459:  Train loss: 3.7539782524108887  Test loss: 3.3884072303771973 \n",
      "Epoch: 1/10:  mini-batch 3774/4459:  Train loss: 3.581174373626709  Test loss: 3.3880958557128906 \n",
      "Epoch: 1/10:  mini-batch 3775/4459:  Train loss: 3.16058349609375  Test loss: 3.3880414962768555 \n",
      "Epoch: 1/10:  mini-batch 3776/4459:  Train loss: 3.123725652694702  Test loss: 3.3876726627349854 \n",
      "Epoch: 1/10:  mini-batch 3777/4459:  Train loss: 3.2617075443267822  Test loss: 3.387138605117798 \n",
      "Epoch: 1/10:  mini-batch 3778/4459:  Train loss: 3.453404188156128  Test loss: 3.3866257667541504 \n",
      "Epoch: 1/10:  mini-batch 3779/4459:  Train loss: 3.315493106842041  Test loss: 3.3862969875335693 \n",
      "Epoch: 1/10:  mini-batch 3780/4459:  Train loss: 3.335346221923828  Test loss: 3.3863558769226074 \n",
      "Epoch: 1/10:  mini-batch 3781/4459:  Train loss: 3.4115219116210938  Test loss: 3.386518955230713 \n",
      "Epoch: 1/10:  mini-batch 3782/4459:  Train loss: 2.9218740463256836  Test loss: 3.3869218826293945 \n",
      "Epoch: 1/10:  mini-batch 3783/4459:  Train loss: 2.9638123512268066  Test loss: 3.3872485160827637 \n",
      "Epoch: 1/10:  mini-batch 3784/4459:  Train loss: 3.2108283042907715  Test loss: 3.3877549171447754 \n",
      "Epoch: 1/10:  mini-batch 3785/4459:  Train loss: 3.175954580307007  Test loss: 3.388286590576172 \n",
      "Epoch: 1/10:  mini-batch 3786/4459:  Train loss: 2.971613883972168  Test loss: 3.3888275623321533 \n",
      "Epoch: 1/10:  mini-batch 3787/4459:  Train loss: 3.7906365394592285  Test loss: 3.3887217044830322 \n",
      "Epoch: 1/10:  mini-batch 3788/4459:  Train loss: 3.782733917236328  Test loss: 3.3884549140930176 \n",
      "Epoch: 1/10:  mini-batch 3789/4459:  Train loss: 3.4333455562591553  Test loss: 3.388113021850586 \n",
      "Epoch: 1/10:  mini-batch 3790/4459:  Train loss: 3.5451819896698  Test loss: 3.387666702270508 \n",
      "Epoch: 1/10:  mini-batch 3791/4459:  Train loss: 3.447039842605591  Test loss: 3.387312173843384 \n",
      "Epoch: 1/10:  mini-batch 3792/4459:  Train loss: 3.1775636672973633  Test loss: 3.38694429397583 \n",
      "Epoch: 1/10:  mini-batch 3793/4459:  Train loss: 3.2184252738952637  Test loss: 3.3864266872406006 \n",
      "Epoch: 1/10:  mini-batch 3794/4459:  Train loss: 3.510075569152832  Test loss: 3.3854689598083496 \n",
      "Epoch: 1/10:  mini-batch 3795/4459:  Train loss: 3.231973171234131  Test loss: 3.384766101837158 \n",
      "Epoch: 1/10:  mini-batch 3796/4459:  Train loss: 3.2791404724121094  Test loss: 3.3842272758483887 \n",
      "Epoch: 1/10:  mini-batch 3797/4459:  Train loss: 3.363711357116699  Test loss: 3.383995532989502 \n",
      "Epoch: 1/10:  mini-batch 3798/4459:  Train loss: 3.169374465942383  Test loss: 3.3840999603271484 \n",
      "Epoch: 1/10:  mini-batch 3799/4459:  Train loss: 3.6024832725524902  Test loss: 3.3842501640319824 \n",
      "Epoch: 1/10:  mini-batch 3800/4459:  Train loss: 3.3881802558898926  Test loss: 3.384491443634033 \n",
      "Epoch: 1/10:  mini-batch 3801/4459:  Train loss: 3.4331820011138916  Test loss: 3.384026288986206 \n",
      "Epoch: 1/10:  mini-batch 3802/4459:  Train loss: 3.288747787475586  Test loss: 3.383763551712036 \n",
      "Epoch: 1/10:  mini-batch 3803/4459:  Train loss: 3.2133069038391113  Test loss: 3.3835625648498535 \n",
      "Epoch: 1/10:  mini-batch 3804/4459:  Train loss: 3.460209369659424  Test loss: 3.383733034133911 \n",
      "Epoch: 1/10:  mini-batch 3805/4459:  Train loss: 3.302578926086426  Test loss: 3.384122371673584 \n",
      "Epoch: 1/10:  mini-batch 3806/4459:  Train loss: 2.8606245517730713  Test loss: 3.384777069091797 \n",
      "Epoch: 1/10:  mini-batch 3807/4459:  Train loss: 3.3406214714050293  Test loss: 3.3851571083068848 \n",
      "Epoch: 1/10:  mini-batch 3808/4459:  Train loss: 3.3299899101257324  Test loss: 3.385822296142578 \n",
      "Epoch: 1/10:  mini-batch 3809/4459:  Train loss: 3.3308842182159424  Test loss: 3.386593818664551 \n",
      "Epoch: 1/10:  mini-batch 3810/4459:  Train loss: 3.410372495651245  Test loss: 3.3875975608825684 \n",
      "Epoch: 1/10:  mini-batch 3811/4459:  Train loss: 2.9888758659362793  Test loss: 3.3887720108032227 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3812/4459:  Train loss: 3.336592674255371  Test loss: 3.3896119594573975 \n",
      "Epoch: 1/10:  mini-batch 3813/4459:  Train loss: 3.231344699859619  Test loss: 3.3904008865356445 \n",
      "Epoch: 1/10:  mini-batch 3814/4459:  Train loss: 3.333712577819824  Test loss: 3.3911242485046387 \n",
      "Epoch: 1/10:  mini-batch 3815/4459:  Train loss: 3.4100182056427  Test loss: 3.3920092582702637 \n",
      "Epoch: 1/10:  mini-batch 3816/4459:  Train loss: 3.2196946144104004  Test loss: 3.392824172973633 \n",
      "Epoch: 1/10:  mini-batch 3817/4459:  Train loss: 3.1889615058898926  Test loss: 3.392895221710205 \n",
      "Epoch: 1/10:  mini-batch 3818/4459:  Train loss: 3.941871166229248  Test loss: 3.3927440643310547 \n",
      "Epoch: 1/10:  mini-batch 3819/4459:  Train loss: 3.79602313041687  Test loss: 3.3924636840820312 \n",
      "Epoch: 1/10:  mini-batch 3820/4459:  Train loss: 3.0644121170043945  Test loss: 3.3923609256744385 \n",
      "Epoch: 1/10:  mini-batch 3821/4459:  Train loss: 3.1527962684631348  Test loss: 3.392573595046997 \n",
      "Epoch: 1/10:  mini-batch 3822/4459:  Train loss: 2.898624897003174  Test loss: 3.3927948474884033 \n",
      "Epoch: 1/10:  mini-batch 3823/4459:  Train loss: 3.3125901222229004  Test loss: 3.3931968212127686 \n",
      "Epoch: 1/10:  mini-batch 3824/4459:  Train loss: 2.9754202365875244  Test loss: 3.393937587738037 \n",
      "Epoch: 1/10:  mini-batch 3825/4459:  Train loss: 3.075413465499878  Test loss: 3.394557476043701 \n",
      "Epoch: 1/10:  mini-batch 3826/4459:  Train loss: 3.3142993450164795  Test loss: 3.394935131072998 \n",
      "Epoch: 1/10:  mini-batch 3827/4459:  Train loss: 3.2897963523864746  Test loss: 3.395313024520874 \n",
      "Epoch: 1/10:  mini-batch 3828/4459:  Train loss: 3.555342674255371  Test loss: 3.3948893547058105 \n",
      "Epoch: 1/10:  mini-batch 3829/4459:  Train loss: 3.4223453998565674  Test loss: 3.394265651702881 \n",
      "Epoch: 1/10:  mini-batch 3830/4459:  Train loss: 3.958526611328125  Test loss: 3.393436908721924 \n",
      "Epoch: 1/10:  mini-batch 3831/4459:  Train loss: 3.4500088691711426  Test loss: 3.392674446105957 \n",
      "Epoch: 1/10:  mini-batch 3832/4459:  Train loss: 3.005885601043701  Test loss: 3.3921825885772705 \n",
      "Epoch: 1/10:  mini-batch 3833/4459:  Train loss: 3.2238569259643555  Test loss: 3.3911380767822266 \n",
      "Epoch: 1/10:  mini-batch 3834/4459:  Train loss: 3.319674015045166  Test loss: 3.3895068168640137 \n",
      "Epoch: 1/10:  mini-batch 3835/4459:  Train loss: 3.6742942333221436  Test loss: 3.3877649307250977 \n",
      "Epoch: 1/10:  mini-batch 3836/4459:  Train loss: 3.306213140487671  Test loss: 3.386251449584961 \n",
      "Epoch: 1/10:  mini-batch 3837/4459:  Train loss: 3.0953495502471924  Test loss: 3.3849544525146484 \n",
      "Epoch: 1/10:  mini-batch 3838/4459:  Train loss: 3.5178422927856445  Test loss: 3.3836112022399902 \n",
      "Epoch: 1/10:  mini-batch 3839/4459:  Train loss: 3.370011806488037  Test loss: 3.3821170330047607 \n",
      "Epoch: 1/10:  mini-batch 3840/4459:  Train loss: 2.977052688598633  Test loss: 3.381051540374756 \n",
      "Epoch: 1/10:  mini-batch 3841/4459:  Train loss: 3.7179014682769775  Test loss: 3.379713296890259 \n",
      "Epoch: 1/10:  mini-batch 3842/4459:  Train loss: 3.2182364463806152  Test loss: 3.377957344055176 \n",
      "Epoch: 1/10:  mini-batch 3843/4459:  Train loss: 3.6780200004577637  Test loss: 3.376215696334839 \n",
      "Epoch: 1/10:  mini-batch 3844/4459:  Train loss: 3.6602416038513184  Test loss: 3.3744986057281494 \n",
      "Epoch: 1/10:  mini-batch 3845/4459:  Train loss: 3.3554704189300537  Test loss: 3.3726613521575928 \n",
      "Epoch: 1/10:  mini-batch 3846/4459:  Train loss: 2.8880488872528076  Test loss: 3.3712830543518066 \n",
      "Epoch: 1/10:  mini-batch 3847/4459:  Train loss: 3.6241681575775146  Test loss: 3.370238780975342 \n",
      "Epoch: 1/10:  mini-batch 3848/4459:  Train loss: 2.584745168685913  Test loss: 3.3698558807373047 \n",
      "Epoch: 1/10:  mini-batch 3849/4459:  Train loss: 3.2785024642944336  Test loss: 3.3693864345550537 \n",
      "Epoch: 1/10:  mini-batch 3850/4459:  Train loss: 3.0823068618774414  Test loss: 3.3690125942230225 \n",
      "Epoch: 1/10:  mini-batch 3851/4459:  Train loss: 3.2278363704681396  Test loss: 3.3689324855804443 \n",
      "Epoch: 1/10:  mini-batch 3852/4459:  Train loss: 2.7765274047851562  Test loss: 3.369201421737671 \n",
      "Epoch: 1/10:  mini-batch 3853/4459:  Train loss: 2.924440860748291  Test loss: 3.369783401489258 \n",
      "Epoch: 1/10:  mini-batch 3854/4459:  Train loss: 3.1656007766723633  Test loss: 3.3701016902923584 \n",
      "Epoch: 1/10:  mini-batch 3855/4459:  Train loss: 2.8947441577911377  Test loss: 3.370814085006714 \n",
      "Epoch: 1/10:  mini-batch 3856/4459:  Train loss: 3.7306876182556152  Test loss: 3.371399402618408 \n",
      "Epoch: 1/10:  mini-batch 3857/4459:  Train loss: 3.692193031311035  Test loss: 3.37186336517334 \n",
      "Epoch: 1/10:  mini-batch 3858/4459:  Train loss: 3.1174542903900146  Test loss: 3.372682809829712 \n",
      "Epoch: 1/10:  mini-batch 3859/4459:  Train loss: 3.506784439086914  Test loss: 3.373203754425049 \n",
      "Epoch: 1/10:  mini-batch 3860/4459:  Train loss: 3.013176679611206  Test loss: 3.374154806137085 \n",
      "Epoch: 1/10:  mini-batch 3861/4459:  Train loss: 3.144224166870117  Test loss: 3.3754091262817383 \n",
      "Epoch: 1/10:  mini-batch 3862/4459:  Train loss: 3.0841064453125  Test loss: 3.3770227432250977 \n",
      "Epoch: 1/10:  mini-batch 3863/4459:  Train loss: 3.275300979614258  Test loss: 3.3786067962646484 \n",
      "Epoch: 1/10:  mini-batch 3864/4459:  Train loss: 3.178980827331543  Test loss: 3.380114793777466 \n",
      "Epoch: 1/10:  mini-batch 3865/4459:  Train loss: 3.2168164253234863  Test loss: 3.3817458152770996 \n",
      "Epoch: 1/10:  mini-batch 3866/4459:  Train loss: 3.3397374153137207  Test loss: 3.3829169273376465 \n",
      "Epoch: 1/10:  mini-batch 3867/4459:  Train loss: 3.263841152191162  Test loss: 3.383114814758301 \n",
      "Epoch: 1/10:  mini-batch 3868/4459:  Train loss: 3.181086540222168  Test loss: 3.3831119537353516 \n",
      "Epoch: 1/10:  mini-batch 3869/4459:  Train loss: 3.354114532470703  Test loss: 3.3828463554382324 \n",
      "Epoch: 1/10:  mini-batch 3870/4459:  Train loss: 3.7910609245300293  Test loss: 3.3819775581359863 \n",
      "Epoch: 1/10:  mini-batch 3871/4459:  Train loss: 3.1536192893981934  Test loss: 3.3811535835266113 \n",
      "Epoch: 1/10:  mini-batch 3872/4459:  Train loss: 2.9530954360961914  Test loss: 3.380836248397827 \n",
      "Epoch: 1/10:  mini-batch 3873/4459:  Train loss: 2.996286392211914  Test loss: 3.380687952041626 \n",
      "Epoch: 1/10:  mini-batch 3874/4459:  Train loss: 3.159648895263672  Test loss: 3.3809385299682617 \n",
      "Epoch: 1/10:  mini-batch 3875/4459:  Train loss: 3.1729841232299805  Test loss: 3.3810300827026367 \n",
      "Epoch: 1/10:  mini-batch 3876/4459:  Train loss: 3.000685691833496  Test loss: 3.381192207336426 \n",
      "Epoch: 1/10:  mini-batch 3877/4459:  Train loss: 3.2450344562530518  Test loss: 3.380922317504883 \n",
      "Epoch: 1/10:  mini-batch 3878/4459:  Train loss: 3.2965176105499268  Test loss: 3.380519390106201 \n",
      "Epoch: 1/10:  mini-batch 3879/4459:  Train loss: 3.7245492935180664  Test loss: 3.3797523975372314 \n",
      "Epoch: 1/10:  mini-batch 3880/4459:  Train loss: 3.1526665687561035  Test loss: 3.37923264503479 \n",
      "Epoch: 1/10:  mini-batch 3881/4459:  Train loss: 3.2891855239868164  Test loss: 3.3784940242767334 \n",
      "Epoch: 1/10:  mini-batch 3882/4459:  Train loss: 2.99493408203125  Test loss: 3.3781681060791016 \n",
      "Epoch: 1/10:  mini-batch 3883/4459:  Train loss: 3.792565107345581  Test loss: 3.3771870136260986 \n",
      "Epoch: 1/10:  mini-batch 3884/4459:  Train loss: 3.2681844234466553  Test loss: 3.376124620437622 \n",
      "Epoch: 1/10:  mini-batch 3885/4459:  Train loss: 3.079948663711548  Test loss: 3.3748247623443604 \n",
      "Epoch: 1/10:  mini-batch 3886/4459:  Train loss: 3.2210092544555664  Test loss: 3.37347149848938 \n",
      "Epoch: 1/10:  mini-batch 3887/4459:  Train loss: 3.453716278076172  Test loss: 3.3717193603515625 \n",
      "Epoch: 1/10:  mini-batch 3888/4459:  Train loss: 3.355780601501465  Test loss: 3.3699045181274414 \n",
      "Epoch: 1/10:  mini-batch 3889/4459:  Train loss: 2.8956308364868164  Test loss: 3.3685474395751953 \n",
      "Epoch: 1/10:  mini-batch 3890/4459:  Train loss: 2.957164764404297  Test loss: 3.367457628250122 \n",
      "Epoch: 1/10:  mini-batch 3891/4459:  Train loss: 3.203188419342041  Test loss: 3.36674165725708 \n",
      "Epoch: 1/10:  mini-batch 3892/4459:  Train loss: 3.339601993560791  Test loss: 3.365570068359375 \n",
      "Epoch: 1/10:  mini-batch 3893/4459:  Train loss: 3.394667863845825  Test loss: 3.364344596862793 \n",
      "Epoch: 1/10:  mini-batch 3894/4459:  Train loss: 3.3927645683288574  Test loss: 3.3628804683685303 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3895/4459:  Train loss: 3.2297637462615967  Test loss: 3.361445426940918 \n",
      "Epoch: 1/10:  mini-batch 3896/4459:  Train loss: 3.3227992057800293  Test loss: 3.3603878021240234 \n",
      "Epoch: 1/10:  mini-batch 3897/4459:  Train loss: 3.285468816757202  Test loss: 3.359527587890625 \n",
      "Epoch: 1/10:  mini-batch 3898/4459:  Train loss: 3.7719740867614746  Test loss: 3.3586668968200684 \n",
      "Epoch: 1/10:  mini-batch 3899/4459:  Train loss: 3.0798768997192383  Test loss: 3.3580708503723145 \n",
      "Epoch: 1/10:  mini-batch 3900/4459:  Train loss: 2.647683620452881  Test loss: 3.35823392868042 \n",
      "Epoch: 1/10:  mini-batch 3901/4459:  Train loss: 2.9549646377563477  Test loss: 3.3579907417297363 \n",
      "Epoch: 1/10:  mini-batch 3902/4459:  Train loss: 3.376991033554077  Test loss: 3.35756254196167 \n",
      "Epoch: 1/10:  mini-batch 3903/4459:  Train loss: 3.620673656463623  Test loss: 3.356942892074585 \n",
      "Epoch: 1/10:  mini-batch 3904/4459:  Train loss: 3.4628043174743652  Test loss: 3.3562726974487305 \n",
      "Epoch: 1/10:  mini-batch 3905/4459:  Train loss: 3.4932897090911865  Test loss: 3.355386734008789 \n",
      "Epoch: 1/10:  mini-batch 3906/4459:  Train loss: 3.413022041320801  Test loss: 3.3544301986694336 \n",
      "Epoch: 1/10:  mini-batch 3907/4459:  Train loss: 3.315192461013794  Test loss: 3.3540475368499756 \n",
      "Epoch: 1/10:  mini-batch 3908/4459:  Train loss: 3.112668514251709  Test loss: 3.353451728820801 \n",
      "Epoch: 1/10:  mini-batch 3909/4459:  Train loss: 3.175290584564209  Test loss: 3.3526864051818848 \n",
      "Epoch: 1/10:  mini-batch 3910/4459:  Train loss: 3.0490047931671143  Test loss: 3.3523364067077637 \n",
      "Epoch: 1/10:  mini-batch 3911/4459:  Train loss: 2.9280834197998047  Test loss: 3.3524203300476074 \n",
      "Epoch: 1/10:  mini-batch 3912/4459:  Train loss: 3.252946615219116  Test loss: 3.352372407913208 \n",
      "Epoch: 1/10:  mini-batch 3913/4459:  Train loss: 3.373459815979004  Test loss: 3.3520843982696533 \n",
      "Epoch: 1/10:  mini-batch 3914/4459:  Train loss: 2.983344554901123  Test loss: 3.3519725799560547 \n",
      "Epoch: 1/10:  mini-batch 3915/4459:  Train loss: 3.647768497467041  Test loss: 3.351670503616333 \n",
      "Epoch: 1/10:  mini-batch 3916/4459:  Train loss: 3.5426859855651855  Test loss: 3.351717472076416 \n",
      "Epoch: 1/10:  mini-batch 3917/4459:  Train loss: 2.9620778560638428  Test loss: 3.3519585132598877 \n",
      "Epoch: 1/10:  mini-batch 3918/4459:  Train loss: 3.434077024459839  Test loss: 3.3518893718719482 \n",
      "Epoch: 1/10:  mini-batch 3919/4459:  Train loss: 3.9005045890808105  Test loss: 3.351292610168457 \n",
      "Epoch: 1/10:  mini-batch 3920/4459:  Train loss: 3.522900104522705  Test loss: 3.35056209564209 \n",
      "Epoch: 1/10:  mini-batch 3921/4459:  Train loss: 3.1271109580993652  Test loss: 3.3500850200653076 \n",
      "Epoch: 1/10:  mini-batch 3922/4459:  Train loss: 3.136221408843994  Test loss: 3.3492684364318848 \n",
      "Epoch: 1/10:  mini-batch 3923/4459:  Train loss: 3.2749269008636475  Test loss: 3.348398447036743 \n",
      "Epoch: 1/10:  mini-batch 3924/4459:  Train loss: 3.135258674621582  Test loss: 3.3477344512939453 \n",
      "Epoch: 1/10:  mini-batch 3925/4459:  Train loss: 3.5135159492492676  Test loss: 3.3471603393554688 \n",
      "Epoch: 1/10:  mini-batch 3926/4459:  Train loss: 3.169060230255127  Test loss: 3.346684455871582 \n",
      "Epoch: 1/10:  mini-batch 3927/4459:  Train loss: 3.1733555793762207  Test loss: 3.346259117126465 \n",
      "Epoch: 1/10:  mini-batch 3928/4459:  Train loss: 3.6522107124328613  Test loss: 3.3460581302642822 \n",
      "Epoch: 1/10:  mini-batch 3929/4459:  Train loss: 2.9312195777893066  Test loss: 3.3460450172424316 \n",
      "Epoch: 1/10:  mini-batch 3930/4459:  Train loss: 2.95371413230896  Test loss: 3.3461451530456543 \n",
      "Epoch: 1/10:  mini-batch 3931/4459:  Train loss: 3.6159276962280273  Test loss: 3.346081256866455 \n",
      "Epoch: 1/10:  mini-batch 3932/4459:  Train loss: 2.9099104404449463  Test loss: 3.3461949825286865 \n",
      "Epoch: 1/10:  mini-batch 3933/4459:  Train loss: 2.950291633605957  Test loss: 3.3463807106018066 \n",
      "Epoch: 1/10:  mini-batch 3934/4459:  Train loss: 3.482724189758301  Test loss: 3.3464648723602295 \n",
      "Epoch: 1/10:  mini-batch 3935/4459:  Train loss: 3.4258627891540527  Test loss: 3.3468000888824463 \n",
      "Epoch: 1/10:  mini-batch 3936/4459:  Train loss: 3.2476375102996826  Test loss: 3.3463573455810547 \n",
      "Epoch: 1/10:  mini-batch 3937/4459:  Train loss: 3.312101364135742  Test loss: 3.3459832668304443 \n",
      "Epoch: 1/10:  mini-batch 3938/4459:  Train loss: 3.3717429637908936  Test loss: 3.345689296722412 \n",
      "Epoch: 1/10:  mini-batch 3939/4459:  Train loss: 3.5973260402679443  Test loss: 3.345078229904175 \n",
      "Epoch: 1/10:  mini-batch 3940/4459:  Train loss: 3.2890610694885254  Test loss: 3.3447415828704834 \n",
      "Epoch: 1/10:  mini-batch 3941/4459:  Train loss: 3.362138271331787  Test loss: 3.3444416522979736 \n",
      "Epoch: 1/10:  mini-batch 3942/4459:  Train loss: 3.8734939098358154  Test loss: 3.3446671962738037 \n",
      "Epoch: 1/10:  mini-batch 3943/4459:  Train loss: 3.572845935821533  Test loss: 3.345123529434204 \n",
      "Epoch: 1/10:  mini-batch 3944/4459:  Train loss: 3.933210849761963  Test loss: 3.3457095623016357 \n",
      "Epoch: 1/10:  mini-batch 3945/4459:  Train loss: 2.9171934127807617  Test loss: 3.3464884757995605 \n",
      "Epoch: 1/10:  mini-batch 3946/4459:  Train loss: 3.2967886924743652  Test loss: 3.347240447998047 \n",
      "Epoch: 1/10:  mini-batch 3947/4459:  Train loss: 3.1751465797424316  Test loss: 3.3480968475341797 \n",
      "Epoch: 1/10:  mini-batch 3948/4459:  Train loss: 3.244788646697998  Test loss: 3.348773717880249 \n",
      "Epoch: 1/10:  mini-batch 3949/4459:  Train loss: 3.040027618408203  Test loss: 3.348690986633301 \n",
      "Epoch: 1/10:  mini-batch 3950/4459:  Train loss: 3.1273863315582275  Test loss: 3.3487040996551514 \n",
      "Epoch: 1/10:  mini-batch 3951/4459:  Train loss: 3.7415101528167725  Test loss: 3.3489181995391846 \n",
      "Epoch: 1/10:  mini-batch 3952/4459:  Train loss: 3.03535795211792  Test loss: 3.34895658493042 \n",
      "Epoch: 1/10:  mini-batch 3953/4459:  Train loss: 2.7862300872802734  Test loss: 3.3492937088012695 \n",
      "Epoch: 1/10:  mini-batch 3954/4459:  Train loss: 3.498094081878662  Test loss: 3.3494162559509277 \n",
      "Epoch: 1/10:  mini-batch 3955/4459:  Train loss: 3.1248362064361572  Test loss: 3.3497848510742188 \n",
      "Epoch: 1/10:  mini-batch 3956/4459:  Train loss: 3.206186294555664  Test loss: 3.3499324321746826 \n",
      "Epoch: 1/10:  mini-batch 3957/4459:  Train loss: 3.1467223167419434  Test loss: 3.349775791168213 \n",
      "Epoch: 1/10:  mini-batch 3958/4459:  Train loss: 3.819265365600586  Test loss: 3.34934401512146 \n",
      "Epoch: 1/10:  mini-batch 3959/4459:  Train loss: 3.400712490081787  Test loss: 3.348689317703247 \n",
      "Epoch: 1/10:  mini-batch 3960/4459:  Train loss: 3.323256731033325  Test loss: 3.347914695739746 \n",
      "Epoch: 1/10:  mini-batch 3961/4459:  Train loss: 3.3310976028442383  Test loss: 3.3466806411743164 \n",
      "Epoch: 1/10:  mini-batch 3962/4459:  Train loss: 3.051724433898926  Test loss: 3.345665454864502 \n",
      "Epoch: 1/10:  mini-batch 3963/4459:  Train loss: 2.934812068939209  Test loss: 3.345094919204712 \n",
      "Epoch: 1/10:  mini-batch 3964/4459:  Train loss: 2.9546449184417725  Test loss: 3.3447253704071045 \n",
      "Epoch: 1/10:  mini-batch 3965/4459:  Train loss: 2.8998780250549316  Test loss: 3.3445303440093994 \n",
      "Epoch: 1/10:  mini-batch 3966/4459:  Train loss: 3.19400691986084  Test loss: 3.344005584716797 \n",
      "Epoch: 1/10:  mini-batch 3967/4459:  Train loss: 3.181558609008789  Test loss: 3.343597412109375 \n",
      "Epoch: 1/10:  mini-batch 3968/4459:  Train loss: 2.984276533126831  Test loss: 3.3436293601989746 \n",
      "Epoch: 1/10:  mini-batch 3969/4459:  Train loss: 3.752746343612671  Test loss: 3.3436551094055176 \n",
      "Epoch: 1/10:  mini-batch 3970/4459:  Train loss: 3.3215713500976562  Test loss: 3.343830108642578 \n",
      "Epoch: 1/10:  mini-batch 3971/4459:  Train loss: 3.6148529052734375  Test loss: 3.3440279960632324 \n",
      "Epoch: 1/10:  mini-batch 3972/4459:  Train loss: 3.240495443344116  Test loss: 3.3444130420684814 \n",
      "Epoch: 1/10:  mini-batch 3973/4459:  Train loss: 3.286109447479248  Test loss: 3.344822406768799 \n",
      "Epoch: 1/10:  mini-batch 3974/4459:  Train loss: 3.1532559394836426  Test loss: 3.3454086780548096 \n",
      "Epoch: 1/10:  mini-batch 3975/4459:  Train loss: 3.3145217895507812  Test loss: 3.3453378677368164 \n",
      "Epoch: 1/10:  mini-batch 3976/4459:  Train loss: 3.2387232780456543  Test loss: 3.3452706336975098 \n",
      "Epoch: 1/10:  mini-batch 3977/4459:  Train loss: 3.017803192138672  Test loss: 3.345724582672119 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 3978/4459:  Train loss: 3.3036251068115234  Test loss: 3.3462347984313965 \n",
      "Epoch: 1/10:  mini-batch 3979/4459:  Train loss: 3.4325919151306152  Test loss: 3.346619129180908 \n",
      "Epoch: 1/10:  mini-batch 3980/4459:  Train loss: 3.26646089553833  Test loss: 3.3464155197143555 \n",
      "Epoch: 1/10:  mini-batch 3981/4459:  Train loss: 3.1012511253356934  Test loss: 3.345798969268799 \n",
      "Epoch: 1/10:  mini-batch 3982/4459:  Train loss: 3.230802297592163  Test loss: 3.3452646732330322 \n",
      "Epoch: 1/10:  mini-batch 3983/4459:  Train loss: 2.8605875968933105  Test loss: 3.345060110092163 \n",
      "Epoch: 1/10:  mini-batch 3984/4459:  Train loss: 2.859649181365967  Test loss: 3.3453025817871094 \n",
      "Epoch: 1/10:  mini-batch 3985/4459:  Train loss: 3.395066499710083  Test loss: 3.345499038696289 \n",
      "Epoch: 1/10:  mini-batch 3986/4459:  Train loss: 3.7981672286987305  Test loss: 3.3451638221740723 \n",
      "Epoch: 1/10:  mini-batch 3987/4459:  Train loss: 3.35467791557312  Test loss: 3.3447799682617188 \n",
      "Epoch: 1/10:  mini-batch 3988/4459:  Train loss: 3.5468015670776367  Test loss: 3.3442635536193848 \n",
      "Epoch: 1/10:  mini-batch 3989/4459:  Train loss: 3.711540699005127  Test loss: 3.3434431552886963 \n",
      "Epoch: 1/10:  mini-batch 3990/4459:  Train loss: 3.187448263168335  Test loss: 3.342525005340576 \n",
      "Epoch: 1/10:  mini-batch 3991/4459:  Train loss: 2.596750259399414  Test loss: 3.3425612449645996 \n",
      "Epoch: 1/10:  mini-batch 3992/4459:  Train loss: 3.487865924835205  Test loss: 3.3423690795898438 \n",
      "Epoch: 1/10:  mini-batch 3993/4459:  Train loss: 3.733875036239624  Test loss: 3.3417482376098633 \n",
      "Epoch: 1/10:  mini-batch 3994/4459:  Train loss: 3.744424819946289  Test loss: 3.34093976020813 \n",
      "Epoch: 1/10:  mini-batch 3995/4459:  Train loss: 3.3587191104888916  Test loss: 3.340491533279419 \n",
      "Epoch: 1/10:  mini-batch 3996/4459:  Train loss: 3.513338327407837  Test loss: 3.3398547172546387 \n",
      "Epoch: 1/10:  mini-batch 3997/4459:  Train loss: 3.165783643722534  Test loss: 3.339449405670166 \n",
      "Epoch: 1/10:  mini-batch 3998/4459:  Train loss: 3.0957651138305664  Test loss: 3.3392865657806396 \n",
      "Epoch: 1/10:  mini-batch 3999/4459:  Train loss: 3.2082595825195312  Test loss: 3.3389086723327637 \n",
      "Epoch: 1/10:  mini-batch 4000/4459:  Train loss: 3.2254061698913574  Test loss: 3.3386425971984863 \n",
      "Epoch: 1/10:  mini-batch 4001/4459:  Train loss: 3.417203426361084  Test loss: 3.338488817214966 \n",
      "Epoch: 1/10:  mini-batch 4002/4459:  Train loss: 3.597132682800293  Test loss: 3.338261604309082 \n",
      "Epoch: 1/10:  mini-batch 4003/4459:  Train loss: 3.614680051803589  Test loss: 3.33813214302063 \n",
      "Epoch: 1/10:  mini-batch 4004/4459:  Train loss: 3.5869574546813965  Test loss: 3.338150978088379 \n",
      "Epoch: 1/10:  mini-batch 4005/4459:  Train loss: 2.7748236656188965  Test loss: 3.338473320007324 \n",
      "Epoch: 1/10:  mini-batch 4006/4459:  Train loss: 3.6160824298858643  Test loss: 3.3384485244750977 \n",
      "Epoch: 1/10:  mini-batch 4007/4459:  Train loss: 3.491065740585327  Test loss: 3.3385043144226074 \n",
      "Epoch: 1/10:  mini-batch 4008/4459:  Train loss: 3.85038161277771  Test loss: 3.3382561206817627 \n",
      "Epoch: 1/10:  mini-batch 4009/4459:  Train loss: 2.8952198028564453  Test loss: 3.3382327556610107 \n",
      "Epoch: 1/10:  mini-batch 4010/4459:  Train loss: 3.1141717433929443  Test loss: 3.338324785232544 \n",
      "Epoch: 1/10:  mini-batch 4011/4459:  Train loss: 3.186046838760376  Test loss: 3.3384618759155273 \n",
      "Epoch: 1/10:  mini-batch 4012/4459:  Train loss: 3.1705708503723145  Test loss: 3.338559150695801 \n",
      "Epoch: 1/10:  mini-batch 4013/4459:  Train loss: 3.3553876876831055  Test loss: 3.338804244995117 \n",
      "Epoch: 1/10:  mini-batch 4014/4459:  Train loss: 3.295827627182007  Test loss: 3.3391788005828857 \n",
      "Epoch: 1/10:  mini-batch 4015/4459:  Train loss: 3.4548420906066895  Test loss: 3.339172840118408 \n",
      "Epoch: 1/10:  mini-batch 4016/4459:  Train loss: 3.4153478145599365  Test loss: 3.3394718170166016 \n",
      "Epoch: 1/10:  mini-batch 4017/4459:  Train loss: 3.3689136505126953  Test loss: 3.339689254760742 \n",
      "Epoch: 1/10:  mini-batch 4018/4459:  Train loss: 3.5249013900756836  Test loss: 3.339967727661133 \n",
      "Epoch: 1/10:  mini-batch 4019/4459:  Train loss: 3.7598495483398438  Test loss: 3.3403658866882324 \n",
      "Epoch: 1/10:  mini-batch 4020/4459:  Train loss: 3.225933074951172  Test loss: 3.3408854007720947 \n",
      "Epoch: 1/10:  mini-batch 4021/4459:  Train loss: 3.3789374828338623  Test loss: 3.341484546661377 \n",
      "Epoch: 1/10:  mini-batch 4022/4459:  Train loss: 3.3957197666168213  Test loss: 3.3421998023986816 \n",
      "Epoch: 1/10:  mini-batch 4023/4459:  Train loss: 3.2165400981903076  Test loss: 3.342956781387329 \n",
      "Epoch: 1/10:  mini-batch 4024/4459:  Train loss: 3.6658763885498047  Test loss: 3.343780040740967 \n",
      "Epoch: 1/10:  mini-batch 4025/4459:  Train loss: 3.05405330657959  Test loss: 3.3446078300476074 \n",
      "Epoch: 1/10:  mini-batch 4026/4459:  Train loss: 3.2904930114746094  Test loss: 3.345414638519287 \n",
      "Epoch: 1/10:  mini-batch 4027/4459:  Train loss: 3.244196891784668  Test loss: 3.346040725708008 \n",
      "Epoch: 1/10:  mini-batch 4028/4459:  Train loss: 2.9705662727355957  Test loss: 3.346717357635498 \n",
      "Epoch: 1/10:  mini-batch 4029/4459:  Train loss: 3.0509462356567383  Test loss: 3.347273111343384 \n",
      "Epoch: 1/10:  mini-batch 4030/4459:  Train loss: 3.0345253944396973  Test loss: 3.347888946533203 \n",
      "Epoch: 1/10:  mini-batch 4031/4459:  Train loss: 3.2851083278656006  Test loss: 3.348583698272705 \n",
      "Epoch: 1/10:  mini-batch 4032/4459:  Train loss: 2.895878791809082  Test loss: 3.3493261337280273 \n",
      "Epoch: 1/10:  mini-batch 4033/4459:  Train loss: 4.0214433670043945  Test loss: 3.3504269123077393 \n",
      "Epoch: 1/10:  mini-batch 4034/4459:  Train loss: 3.536790609359741  Test loss: 3.351433515548706 \n",
      "Epoch: 1/10:  mini-batch 4035/4459:  Train loss: 3.203946113586426  Test loss: 3.3522653579711914 \n",
      "Epoch: 1/10:  mini-batch 4036/4459:  Train loss: 3.0882394313812256  Test loss: 3.3529915809631348 \n",
      "Epoch: 1/10:  mini-batch 4037/4459:  Train loss: 2.895843982696533  Test loss: 3.3535099029541016 \n",
      "Epoch: 1/10:  mini-batch 4038/4459:  Train loss: 3.2792084217071533  Test loss: 3.3539905548095703 \n",
      "Epoch: 1/10:  mini-batch 4039/4459:  Train loss: 3.0373923778533936  Test loss: 3.354715347290039 \n",
      "Epoch: 1/10:  mini-batch 4040/4459:  Train loss: 3.085555076599121  Test loss: 3.3556456565856934 \n",
      "Epoch: 1/10:  mini-batch 4041/4459:  Train loss: 3.4308032989501953  Test loss: 3.3563930988311768 \n",
      "Epoch: 1/10:  mini-batch 4042/4459:  Train loss: 3.390559196472168  Test loss: 3.356837511062622 \n",
      "Epoch: 1/10:  mini-batch 4043/4459:  Train loss: 3.107403516769409  Test loss: 3.35726261138916 \n",
      "Epoch: 1/10:  mini-batch 4044/4459:  Train loss: 3.0547637939453125  Test loss: 3.3576974868774414 \n",
      "Epoch: 1/10:  mini-batch 4045/4459:  Train loss: 2.813567876815796  Test loss: 3.3583223819732666 \n",
      "Epoch: 1/10:  mini-batch 4046/4459:  Train loss: 3.6604175567626953  Test loss: 3.3591456413269043 \n",
      "Epoch: 1/10:  mini-batch 4047/4459:  Train loss: 3.132143020629883  Test loss: 3.3599605560302734 \n",
      "Epoch: 1/10:  mini-batch 4048/4459:  Train loss: 3.5244710445404053  Test loss: 3.36049485206604 \n",
      "Epoch: 1/10:  mini-batch 4049/4459:  Train loss: 3.074552536010742  Test loss: 3.3609890937805176 \n",
      "Epoch: 1/10:  mini-batch 4050/4459:  Train loss: 3.375352621078491  Test loss: 3.3614611625671387 \n",
      "Epoch: 1/10:  mini-batch 4051/4459:  Train loss: 3.2334938049316406  Test loss: 3.361673593521118 \n",
      "Epoch: 1/10:  mini-batch 4052/4459:  Train loss: 3.6113243103027344  Test loss: 3.3618645668029785 \n",
      "Epoch: 1/10:  mini-batch 4053/4459:  Train loss: 2.9326558113098145  Test loss: 3.3623909950256348 \n",
      "Epoch: 1/10:  mini-batch 4054/4459:  Train loss: 3.1206963062286377  Test loss: 3.362652063369751 \n",
      "Epoch: 1/10:  mini-batch 4055/4459:  Train loss: 3.4484572410583496  Test loss: 3.362818717956543 \n",
      "Epoch: 1/10:  mini-batch 4056/4459:  Train loss: 3.1778745651245117  Test loss: 3.3630685806274414 \n",
      "Epoch: 1/10:  mini-batch 4057/4459:  Train loss: 2.948660373687744  Test loss: 3.3633785247802734 \n",
      "Epoch: 1/10:  mini-batch 4058/4459:  Train loss: 3.0336337089538574  Test loss: 3.3633618354797363 \n",
      "Epoch: 1/10:  mini-batch 4059/4459:  Train loss: 3.4736673831939697  Test loss: 3.363161325454712 \n",
      "Epoch: 1/10:  mini-batch 4060/4459:  Train loss: 3.1203718185424805  Test loss: 3.3632397651672363 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 4061/4459:  Train loss: 3.629091501235962  Test loss: 3.3631839752197266 \n",
      "Epoch: 1/10:  mini-batch 4062/4459:  Train loss: 3.6323227882385254  Test loss: 3.362696886062622 \n",
      "Epoch: 1/10:  mini-batch 4063/4459:  Train loss: 3.3317692279815674  Test loss: 3.362273931503296 \n",
      "Epoch: 1/10:  mini-batch 4064/4459:  Train loss: 3.311627149581909  Test loss: 3.361494541168213 \n",
      "Epoch: 1/10:  mini-batch 4065/4459:  Train loss: 3.796640157699585  Test loss: 3.3604907989501953 \n",
      "Epoch: 1/10:  mini-batch 4066/4459:  Train loss: 3.5603601932525635  Test loss: 3.3593788146972656 \n",
      "Epoch: 1/10:  mini-batch 4067/4459:  Train loss: 3.4326109886169434  Test loss: 3.3582255840301514 \n",
      "Epoch: 1/10:  mini-batch 4068/4459:  Train loss: 3.6145195960998535  Test loss: 3.3571581840515137 \n",
      "Epoch: 1/10:  mini-batch 4069/4459:  Train loss: 3.576342821121216  Test loss: 3.356318950653076 \n",
      "Epoch: 1/10:  mini-batch 4070/4459:  Train loss: 3.1176533699035645  Test loss: 3.3559656143188477 \n",
      "Epoch: 1/10:  mini-batch 4071/4459:  Train loss: 3.5464885234832764  Test loss: 3.355278968811035 \n",
      "Epoch: 1/10:  mini-batch 4072/4459:  Train loss: 3.653796434402466  Test loss: 3.3546142578125 \n",
      "Epoch: 1/10:  mini-batch 4073/4459:  Train loss: 3.1967318058013916  Test loss: 3.354330539703369 \n",
      "Epoch: 1/10:  mini-batch 4074/4459:  Train loss: 2.9548003673553467  Test loss: 3.3541765213012695 \n",
      "Epoch: 1/10:  mini-batch 4075/4459:  Train loss: 3.067211151123047  Test loss: 3.353846788406372 \n",
      "Epoch: 1/10:  mini-batch 4076/4459:  Train loss: 2.8389735221862793  Test loss: 3.3539419174194336 \n",
      "Epoch: 1/10:  mini-batch 4077/4459:  Train loss: 3.140531063079834  Test loss: 3.353989362716675 \n",
      "Epoch: 1/10:  mini-batch 4078/4459:  Train loss: 3.3332839012145996  Test loss: 3.3541042804718018 \n",
      "Epoch: 1/10:  mini-batch 4079/4459:  Train loss: 3.0697216987609863  Test loss: 3.354313611984253 \n",
      "Epoch: 1/10:  mini-batch 4080/4459:  Train loss: 3.6815543174743652  Test loss: 3.354588508605957 \n",
      "Epoch: 1/10:  mini-batch 4081/4459:  Train loss: 3.418461322784424  Test loss: 3.3548736572265625 \n",
      "Epoch: 1/10:  mini-batch 4082/4459:  Train loss: 3.2405881881713867  Test loss: 3.3549110889434814 \n",
      "Epoch: 1/10:  mini-batch 4083/4459:  Train loss: 3.290818929672241  Test loss: 3.354973793029785 \n",
      "Epoch: 1/10:  mini-batch 4084/4459:  Train loss: 3.4060535430908203  Test loss: 3.3547286987304688 \n",
      "Epoch: 1/10:  mini-batch 4085/4459:  Train loss: 3.201833724975586  Test loss: 3.3543028831481934 \n",
      "Epoch: 1/10:  mini-batch 4086/4459:  Train loss: 3.454610586166382  Test loss: 3.3540329933166504 \n",
      "Epoch: 1/10:  mini-batch 4087/4459:  Train loss: 3.508181095123291  Test loss: 3.353641986846924 \n",
      "Epoch: 1/10:  mini-batch 4088/4459:  Train loss: 3.108699321746826  Test loss: 3.3534488677978516 \n",
      "Epoch: 1/10:  mini-batch 4089/4459:  Train loss: 3.161674976348877  Test loss: 3.3534107208251953 \n",
      "Epoch: 1/10:  mini-batch 4090/4459:  Train loss: 3.012364625930786  Test loss: 3.353226661682129 \n",
      "Epoch: 1/10:  mini-batch 4091/4459:  Train loss: 3.1485490798950195  Test loss: 3.353302001953125 \n",
      "Epoch: 1/10:  mini-batch 4092/4459:  Train loss: 3.3749704360961914  Test loss: 3.353025197982788 \n",
      "Epoch: 1/10:  mini-batch 4093/4459:  Train loss: 2.9593868255615234  Test loss: 3.3530404567718506 \n",
      "Epoch: 1/10:  mini-batch 4094/4459:  Train loss: 3.42557430267334  Test loss: 3.3528294563293457 \n",
      "Epoch: 1/10:  mini-batch 4095/4459:  Train loss: 3.026974678039551  Test loss: 3.3528130054473877 \n",
      "Epoch: 1/10:  mini-batch 4096/4459:  Train loss: 3.241985321044922  Test loss: 3.352879047393799 \n",
      "Epoch: 1/10:  mini-batch 4097/4459:  Train loss: 2.936038017272949  Test loss: 3.3532865047454834 \n",
      "Epoch: 1/10:  mini-batch 4098/4459:  Train loss: 2.790590763092041  Test loss: 3.3541746139526367 \n",
      "Epoch: 1/10:  mini-batch 4099/4459:  Train loss: 3.2862114906311035  Test loss: 3.3550257682800293 \n",
      "Epoch: 1/10:  mini-batch 4100/4459:  Train loss: 3.4458277225494385  Test loss: 3.3560128211975098 \n",
      "Epoch: 1/10:  mini-batch 4101/4459:  Train loss: 2.9436678886413574  Test loss: 3.3575265407562256 \n",
      "Epoch: 1/10:  mini-batch 4102/4459:  Train loss: 3.1224656105041504  Test loss: 3.3587114810943604 \n",
      "Epoch: 1/10:  mini-batch 4103/4459:  Train loss: 3.2551393508911133  Test loss: 3.3597636222839355 \n",
      "Epoch: 1/10:  mini-batch 4104/4459:  Train loss: 3.348891019821167  Test loss: 3.360332489013672 \n",
      "Epoch: 1/10:  mini-batch 4105/4459:  Train loss: 3.1751439571380615  Test loss: 3.3611488342285156 \n",
      "Epoch: 1/10:  mini-batch 4106/4459:  Train loss: 2.945584297180176  Test loss: 3.3623757362365723 \n",
      "Epoch: 1/10:  mini-batch 4107/4459:  Train loss: 3.3048038482666016  Test loss: 3.363508939743042 \n",
      "Epoch: 1/10:  mini-batch 4108/4459:  Train loss: 3.3232316970825195  Test loss: 3.3645052909851074 \n",
      "Epoch: 1/10:  mini-batch 4109/4459:  Train loss: 3.5655603408813477  Test loss: 3.3646786212921143 \n",
      "Epoch: 1/10:  mini-batch 4110/4459:  Train loss: 3.0147616863250732  Test loss: 3.3652970790863037 \n",
      "Epoch: 1/10:  mini-batch 4111/4459:  Train loss: 3.1918818950653076  Test loss: 3.3662776947021484 \n",
      "Epoch: 1/10:  mini-batch 4112/4459:  Train loss: 3.2658162117004395  Test loss: 3.3669509887695312 \n",
      "Epoch: 1/10:  mini-batch 4113/4459:  Train loss: 3.056215286254883  Test loss: 3.3680717945098877 \n",
      "Epoch: 1/10:  mini-batch 4114/4459:  Train loss: 3.1964094638824463  Test loss: 3.3689005374908447 \n",
      "Epoch: 1/10:  mini-batch 4115/4459:  Train loss: 3.580386161804199  Test loss: 3.368882656097412 \n",
      "Epoch: 1/10:  mini-batch 4116/4459:  Train loss: 3.38486909866333  Test loss: 3.3689157962799072 \n",
      "Epoch: 1/10:  mini-batch 4117/4459:  Train loss: 3.515270709991455  Test loss: 3.3680334091186523 \n",
      "Epoch: 1/10:  mini-batch 4118/4459:  Train loss: 3.6086511611938477  Test loss: 3.3663034439086914 \n",
      "Epoch: 1/10:  mini-batch 4119/4459:  Train loss: 2.9614949226379395  Test loss: 3.365401029586792 \n",
      "Epoch: 1/10:  mini-batch 4120/4459:  Train loss: 3.2293310165405273  Test loss: 3.364398956298828 \n",
      "Epoch: 1/10:  mini-batch 4121/4459:  Train loss: 3.6571764945983887  Test loss: 3.362942695617676 \n",
      "Epoch: 1/10:  mini-batch 4122/4459:  Train loss: 3.7588422298431396  Test loss: 3.36100435256958 \n",
      "Epoch: 1/10:  mini-batch 4123/4459:  Train loss: 3.876208782196045  Test loss: 3.3586947917938232 \n",
      "Epoch: 1/10:  mini-batch 4124/4459:  Train loss: 3.7272186279296875  Test loss: 3.3568248748779297 \n",
      "Epoch: 1/10:  mini-batch 4125/4459:  Train loss: 3.3387179374694824  Test loss: 3.35532546043396 \n",
      "Epoch: 1/10:  mini-batch 4126/4459:  Train loss: 2.907226085662842  Test loss: 3.3545827865600586 \n",
      "Epoch: 1/10:  mini-batch 4127/4459:  Train loss: 3.5743393898010254  Test loss: 3.353753089904785 \n",
      "Epoch: 1/10:  mini-batch 4128/4459:  Train loss: 3.2145745754241943  Test loss: 3.3532466888427734 \n",
      "Epoch: 1/10:  mini-batch 4129/4459:  Train loss: 3.172482490539551  Test loss: 3.352592945098877 \n",
      "Epoch: 1/10:  mini-batch 4130/4459:  Train loss: 2.9963996410369873  Test loss: 3.3516831398010254 \n",
      "Epoch: 1/10:  mini-batch 4131/4459:  Train loss: 2.784956455230713  Test loss: 3.3510751724243164 \n",
      "Epoch: 1/10:  mini-batch 4132/4459:  Train loss: 3.396656036376953  Test loss: 3.3505680561065674 \n",
      "Epoch: 1/10:  mini-batch 4133/4459:  Train loss: 3.381687641143799  Test loss: 3.3502495288848877 \n",
      "Epoch: 1/10:  mini-batch 4134/4459:  Train loss: 3.018554449081421  Test loss: 3.3501999378204346 \n",
      "Epoch: 1/10:  mini-batch 4135/4459:  Train loss: 2.9980340003967285  Test loss: 3.3502302169799805 \n",
      "Epoch: 1/10:  mini-batch 4136/4459:  Train loss: 3.3115994930267334  Test loss: 3.3500728607177734 \n",
      "Epoch: 1/10:  mini-batch 4137/4459:  Train loss: 3.4226815700531006  Test loss: 3.3500356674194336 \n",
      "Epoch: 1/10:  mini-batch 4138/4459:  Train loss: 3.471616744995117  Test loss: 3.3498125076293945 \n",
      "Epoch: 1/10:  mini-batch 4139/4459:  Train loss: 3.2749457359313965  Test loss: 3.349493980407715 \n",
      "Epoch: 1/10:  mini-batch 4140/4459:  Train loss: 3.34749436378479  Test loss: 3.3490042686462402 \n",
      "Epoch: 1/10:  mini-batch 4141/4459:  Train loss: 3.5627005100250244  Test loss: 3.3485355377197266 \n",
      "Epoch: 1/10:  mini-batch 4142/4459:  Train loss: 3.5941977500915527  Test loss: 3.3476600646972656 \n",
      "Epoch: 1/10:  mini-batch 4143/4459:  Train loss: 3.0101356506347656  Test loss: 3.3468728065490723 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 4144/4459:  Train loss: 3.218857765197754  Test loss: 3.345982074737549 \n",
      "Epoch: 1/10:  mini-batch 4145/4459:  Train loss: 3.5310277938842773  Test loss: 3.345306396484375 \n",
      "Epoch: 1/10:  mini-batch 4146/4459:  Train loss: 2.8715243339538574  Test loss: 3.3450260162353516 \n",
      "Epoch: 1/10:  mini-batch 4147/4459:  Train loss: 3.326284646987915  Test loss: 3.3443143367767334 \n",
      "Epoch: 1/10:  mini-batch 4148/4459:  Train loss: 3.3159737586975098  Test loss: 3.3434391021728516 \n",
      "Epoch: 1/10:  mini-batch 4149/4459:  Train loss: 3.6930673122406006  Test loss: 3.3427436351776123 \n",
      "Epoch: 1/10:  mini-batch 4150/4459:  Train loss: 3.690493106842041  Test loss: 3.3422443866729736 \n",
      "Epoch: 1/10:  mini-batch 4151/4459:  Train loss: 3.349379301071167  Test loss: 3.3417015075683594 \n",
      "Epoch: 1/10:  mini-batch 4152/4459:  Train loss: 3.25996732711792  Test loss: 3.3413543701171875 \n",
      "Epoch: 1/10:  mini-batch 4153/4459:  Train loss: 3.181884288787842  Test loss: 3.3412764072418213 \n",
      "Epoch: 1/10:  mini-batch 4154/4459:  Train loss: 3.2030858993530273  Test loss: 3.3413164615631104 \n",
      "Epoch: 1/10:  mini-batch 4155/4459:  Train loss: 3.2018706798553467  Test loss: 3.3407325744628906 \n",
      "Epoch: 1/10:  mini-batch 4156/4459:  Train loss: 3.138734817504883  Test loss: 3.340369939804077 \n",
      "Epoch: 1/10:  mini-batch 4157/4459:  Train loss: 3.069489002227783  Test loss: 3.340226173400879 \n",
      "Epoch: 1/10:  mini-batch 4158/4459:  Train loss: 3.4615001678466797  Test loss: 3.3399901390075684 \n",
      "Epoch: 1/10:  mini-batch 4159/4459:  Train loss: 3.3953423500061035  Test loss: 3.3398451805114746 \n",
      "Epoch: 1/10:  mini-batch 4160/4459:  Train loss: 3.0337791442871094  Test loss: 3.3398938179016113 \n",
      "Epoch: 1/10:  mini-batch 4161/4459:  Train loss: 3.190081834793091  Test loss: 3.3396170139312744 \n",
      "Epoch: 1/10:  mini-batch 4162/4459:  Train loss: 3.045046806335449  Test loss: 3.3391854763031006 \n",
      "Epoch: 1/10:  mini-batch 4163/4459:  Train loss: 3.0417633056640625  Test loss: 3.3390145301818848 \n",
      "Epoch: 1/10:  mini-batch 4164/4459:  Train loss: 3.413037061691284  Test loss: 3.3388843536376953 \n",
      "Epoch: 1/10:  mini-batch 4165/4459:  Train loss: 3.22829008102417  Test loss: 3.3388941287994385 \n",
      "Epoch: 1/10:  mini-batch 4166/4459:  Train loss: 3.0021235942840576  Test loss: 3.3392534255981445 \n",
      "Epoch: 1/10:  mini-batch 4167/4459:  Train loss: 3.152352809906006  Test loss: 3.339904308319092 \n",
      "Epoch: 1/10:  mini-batch 4168/4459:  Train loss: 2.94691801071167  Test loss: 3.3408145904541016 \n",
      "Epoch: 1/10:  mini-batch 4169/4459:  Train loss: 3.2381086349487305  Test loss: 3.341681957244873 \n",
      "Epoch: 1/10:  mini-batch 4170/4459:  Train loss: 3.112657070159912  Test loss: 3.342705011367798 \n",
      "Epoch: 1/10:  mini-batch 4171/4459:  Train loss: 2.9388556480407715  Test loss: 3.344074249267578 \n",
      "Epoch: 1/10:  mini-batch 4172/4459:  Train loss: 2.9135565757751465  Test loss: 3.3453493118286133 \n",
      "Epoch: 1/10:  mini-batch 4173/4459:  Train loss: 3.429532527923584  Test loss: 3.3465428352355957 \n",
      "Epoch: 1/10:  mini-batch 4174/4459:  Train loss: 3.635317325592041  Test loss: 3.347418785095215 \n",
      "Epoch: 1/10:  mini-batch 4175/4459:  Train loss: 3.4254331588745117  Test loss: 3.348339796066284 \n",
      "Epoch: 1/10:  mini-batch 4176/4459:  Train loss: 3.4286718368530273  Test loss: 3.349092960357666 \n",
      "Epoch: 1/10:  mini-batch 4177/4459:  Train loss: 3.188194513320923  Test loss: 3.3494863510131836 \n",
      "Epoch: 1/10:  mini-batch 4178/4459:  Train loss: 3.515559196472168  Test loss: 3.349508285522461 \n",
      "Epoch: 1/10:  mini-batch 4179/4459:  Train loss: 3.089799642562866  Test loss: 3.3495969772338867 \n",
      "Epoch: 1/10:  mini-batch 4180/4459:  Train loss: 3.3576431274414062  Test loss: 3.3488826751708984 \n",
      "Epoch: 1/10:  mini-batch 4181/4459:  Train loss: 3.0251595973968506  Test loss: 3.34840989112854 \n",
      "Epoch: 1/10:  mini-batch 4182/4459:  Train loss: 3.6978414058685303  Test loss: 3.3473191261291504 \n",
      "Epoch: 1/10:  mini-batch 4183/4459:  Train loss: 3.535698890686035  Test loss: 3.3460323810577393 \n",
      "Epoch: 1/10:  mini-batch 4184/4459:  Train loss: 4.062183856964111  Test loss: 3.344789505004883 \n",
      "Epoch: 1/10:  mini-batch 4185/4459:  Train loss: 3.4945149421691895  Test loss: 3.343518018722534 \n",
      "Epoch: 1/10:  mini-batch 4186/4459:  Train loss: 2.808742046356201  Test loss: 3.3428773880004883 \n",
      "Epoch: 1/10:  mini-batch 4187/4459:  Train loss: 3.4953932762145996  Test loss: 3.342233896255493 \n",
      "Epoch: 1/10:  mini-batch 4188/4459:  Train loss: 3.2645883560180664  Test loss: 3.341397285461426 \n",
      "Epoch: 1/10:  mini-batch 4189/4459:  Train loss: 3.187516212463379  Test loss: 3.340590000152588 \n",
      "Epoch: 1/10:  mini-batch 4190/4459:  Train loss: 2.8849761486053467  Test loss: 3.340102434158325 \n",
      "Epoch: 1/10:  mini-batch 4191/4459:  Train loss: 3.2545740604400635  Test loss: 3.3396811485290527 \n",
      "Epoch: 1/10:  mini-batch 4192/4459:  Train loss: 3.0044665336608887  Test loss: 3.3389499187469482 \n",
      "Epoch: 1/10:  mini-batch 4193/4459:  Train loss: 3.724348545074463  Test loss: 3.3380351066589355 \n",
      "Epoch: 1/10:  mini-batch 4194/4459:  Train loss: 3.0982136726379395  Test loss: 3.3372273445129395 \n",
      "Epoch: 1/10:  mini-batch 4195/4459:  Train loss: 3.7130634784698486  Test loss: 3.3365774154663086 \n",
      "Epoch: 1/10:  mini-batch 4196/4459:  Train loss: 3.440685987472534  Test loss: 3.336017370223999 \n",
      "Epoch: 1/10:  mini-batch 4197/4459:  Train loss: 4.070906639099121  Test loss: 3.335561513900757 \n",
      "Epoch: 1/10:  mini-batch 4198/4459:  Train loss: 3.772625207901001  Test loss: 3.33510160446167 \n",
      "Epoch: 1/10:  mini-batch 4199/4459:  Train loss: 3.166166305541992  Test loss: 3.334562063217163 \n",
      "Epoch: 1/10:  mini-batch 4200/4459:  Train loss: 3.460101366043091  Test loss: 3.3341612815856934 \n",
      "Epoch: 1/10:  mini-batch 4201/4459:  Train loss: 3.0925464630126953  Test loss: 3.3336706161499023 \n",
      "Epoch: 1/10:  mini-batch 4202/4459:  Train loss: 3.057197093963623  Test loss: 3.3333678245544434 \n",
      "Epoch: 1/10:  mini-batch 4203/4459:  Train loss: 3.6418676376342773  Test loss: 3.332848310470581 \n",
      "Epoch: 1/10:  mini-batch 4204/4459:  Train loss: 3.6178414821624756  Test loss: 3.3327078819274902 \n",
      "Epoch: 1/10:  mini-batch 4205/4459:  Train loss: 2.955653667449951  Test loss: 3.3328917026519775 \n",
      "Epoch: 1/10:  mini-batch 4206/4459:  Train loss: 2.9493658542633057  Test loss: 3.3331737518310547 \n",
      "Epoch: 1/10:  mini-batch 4207/4459:  Train loss: 3.032820463180542  Test loss: 3.3335320949554443 \n",
      "Epoch: 1/10:  mini-batch 4208/4459:  Train loss: 3.4911129474639893  Test loss: 3.333858013153076 \n",
      "Epoch: 1/10:  mini-batch 4209/4459:  Train loss: 3.5252532958984375  Test loss: 3.3337972164154053 \n",
      "Epoch: 1/10:  mini-batch 4210/4459:  Train loss: 3.3732142448425293  Test loss: 3.3335886001586914 \n",
      "Epoch: 1/10:  mini-batch 4211/4459:  Train loss: 3.2477307319641113  Test loss: 3.3335251808166504 \n",
      "Epoch: 1/10:  mini-batch 4212/4459:  Train loss: 3.3228790760040283  Test loss: 3.3332724571228027 \n",
      "Epoch: 1/10:  mini-batch 4213/4459:  Train loss: 3.374706506729126  Test loss: 3.3329854011535645 \n",
      "Epoch: 1/10:  mini-batch 4214/4459:  Train loss: 3.423140525817871  Test loss: 3.3329408168792725 \n",
      "Epoch: 1/10:  mini-batch 4215/4459:  Train loss: 3.1956756114959717  Test loss: 3.332585334777832 \n",
      "Epoch: 1/10:  mini-batch 4216/4459:  Train loss: 3.6849770545959473  Test loss: 3.332406997680664 \n",
      "Epoch: 1/10:  mini-batch 4217/4459:  Train loss: 3.56760573387146  Test loss: 3.332263946533203 \n",
      "Epoch: 1/10:  mini-batch 4218/4459:  Train loss: 3.0980372428894043  Test loss: 3.3322412967681885 \n",
      "Epoch: 1/10:  mini-batch 4219/4459:  Train loss: 3.422330856323242  Test loss: 3.332486152648926 \n",
      "Epoch: 1/10:  mini-batch 4220/4459:  Train loss: 3.3565962314605713  Test loss: 3.332793951034546 \n",
      "Epoch: 1/10:  mini-batch 4221/4459:  Train loss: 3.422715187072754  Test loss: 3.333064079284668 \n",
      "Epoch: 1/10:  mini-batch 4222/4459:  Train loss: 3.2555108070373535  Test loss: 3.333277702331543 \n",
      "Epoch: 1/10:  mini-batch 4223/4459:  Train loss: 2.7588579654693604  Test loss: 3.3336033821105957 \n",
      "Epoch: 1/10:  mini-batch 4224/4459:  Train loss: 3.3120241165161133  Test loss: 3.3336784839630127 \n",
      "Epoch: 1/10:  mini-batch 4225/4459:  Train loss: 3.3812403678894043  Test loss: 3.333761692047119 \n",
      "Epoch: 1/10:  mini-batch 4226/4459:  Train loss: 3.2123897075653076  Test loss: 3.33341646194458 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 4227/4459:  Train loss: 3.095644474029541  Test loss: 3.332859516143799 \n",
      "Epoch: 1/10:  mini-batch 4228/4459:  Train loss: 3.533557891845703  Test loss: 3.331812620162964 \n",
      "Epoch: 1/10:  mini-batch 4229/4459:  Train loss: 3.24700927734375  Test loss: 3.330778121948242 \n",
      "Epoch: 1/10:  mini-batch 4230/4459:  Train loss: 3.12992525100708  Test loss: 3.32999849319458 \n",
      "Epoch: 1/10:  mini-batch 4231/4459:  Train loss: 3.627220630645752  Test loss: 3.3294968605041504 \n",
      "Epoch: 1/10:  mini-batch 4232/4459:  Train loss: 3.711759328842163  Test loss: 3.32926082611084 \n",
      "Epoch: 1/10:  mini-batch 4233/4459:  Train loss: 3.3183155059814453  Test loss: 3.329073429107666 \n",
      "Epoch: 1/10:  mini-batch 4234/4459:  Train loss: 2.8170084953308105  Test loss: 3.328953981399536 \n",
      "Epoch: 1/10:  mini-batch 4235/4459:  Train loss: 3.7668912410736084  Test loss: 3.329204559326172 \n",
      "Epoch: 1/10:  mini-batch 4236/4459:  Train loss: 3.391955852508545  Test loss: 3.3291494846343994 \n",
      "Epoch: 1/10:  mini-batch 4237/4459:  Train loss: 3.091745138168335  Test loss: 3.328801155090332 \n",
      "Epoch: 1/10:  mini-batch 4238/4459:  Train loss: 3.444370746612549  Test loss: 3.328409433364868 \n",
      "Epoch: 1/10:  mini-batch 4239/4459:  Train loss: 3.017202138900757  Test loss: 3.32818865776062 \n",
      "Epoch: 1/10:  mini-batch 4240/4459:  Train loss: 3.4695024490356445  Test loss: 3.328425168991089 \n",
      "Epoch: 1/10:  mini-batch 4241/4459:  Train loss: 3.5047757625579834  Test loss: 3.328491687774658 \n",
      "Epoch: 1/10:  mini-batch 4242/4459:  Train loss: 3.73248553276062  Test loss: 3.328606128692627 \n",
      "Epoch: 1/10:  mini-batch 4243/4459:  Train loss: 3.665017604827881  Test loss: 3.329141855239868 \n",
      "Epoch: 1/10:  mini-batch 4244/4459:  Train loss: 3.9424962997436523  Test loss: 3.3300464153289795 \n",
      "Epoch: 1/10:  mini-batch 4245/4459:  Train loss: 3.4803972244262695  Test loss: 3.3309712409973145 \n",
      "Epoch: 1/10:  mini-batch 4246/4459:  Train loss: 3.506432056427002  Test loss: 3.331890821456909 \n",
      "Epoch: 1/10:  mini-batch 4247/4459:  Train loss: 3.2785215377807617  Test loss: 3.3325536251068115 \n",
      "Epoch: 1/10:  mini-batch 4248/4459:  Train loss: 3.28436541557312  Test loss: 3.3330414295196533 \n",
      "Epoch: 1/10:  mini-batch 4249/4459:  Train loss: 3.211275577545166  Test loss: 3.332986831665039 \n",
      "Epoch: 1/10:  mini-batch 4250/4459:  Train loss: 3.1391589641571045  Test loss: 3.332712173461914 \n",
      "Epoch: 1/10:  mini-batch 4251/4459:  Train loss: 3.7131757736206055  Test loss: 3.3327558040618896 \n",
      "Epoch: 1/10:  mini-batch 4252/4459:  Train loss: 3.0997490882873535  Test loss: 3.332973003387451 \n",
      "Epoch: 1/10:  mini-batch 4253/4459:  Train loss: 3.410548686981201  Test loss: 3.333200454711914 \n",
      "Epoch: 1/10:  mini-batch 4254/4459:  Train loss: 3.3333942890167236  Test loss: 3.3335049152374268 \n",
      "Epoch: 1/10:  mini-batch 4255/4459:  Train loss: 3.3881893157958984  Test loss: 3.333604574203491 \n",
      "Epoch: 1/10:  mini-batch 4256/4459:  Train loss: 3.342622756958008  Test loss: 3.3335886001586914 \n",
      "Epoch: 1/10:  mini-batch 4257/4459:  Train loss: 3.2824833393096924  Test loss: 3.333639144897461 \n",
      "Epoch: 1/10:  mini-batch 4258/4459:  Train loss: 3.1110236644744873  Test loss: 3.33384370803833 \n",
      "Epoch: 1/10:  mini-batch 4259/4459:  Train loss: 3.697416305541992  Test loss: 3.334381580352783 \n",
      "Epoch: 1/10:  mini-batch 4260/4459:  Train loss: 3.4052016735076904  Test loss: 3.3350143432617188 \n",
      "Epoch: 1/10:  mini-batch 4261/4459:  Train loss: 3.680224895477295  Test loss: 3.3356423377990723 \n",
      "Epoch: 1/10:  mini-batch 4262/4459:  Train loss: 3.4233248233795166  Test loss: 3.33616042137146 \n",
      "Epoch: 1/10:  mini-batch 4263/4459:  Train loss: 3.679628849029541  Test loss: 3.336916208267212 \n",
      "Epoch: 1/10:  mini-batch 4264/4459:  Train loss: 3.279672622680664  Test loss: 3.3376426696777344 \n",
      "Epoch: 1/10:  mini-batch 4265/4459:  Train loss: 3.88204026222229  Test loss: 3.3387608528137207 \n",
      "Epoch: 1/10:  mini-batch 4266/4459:  Train loss: 3.3856945037841797  Test loss: 3.3398783206939697 \n",
      "Epoch: 1/10:  mini-batch 4267/4459:  Train loss: 3.448607921600342  Test loss: 3.3407516479492188 \n",
      "Epoch: 1/10:  mini-batch 4268/4459:  Train loss: 3.474294900894165  Test loss: 3.3416244983673096 \n",
      "Epoch: 1/10:  mini-batch 4269/4459:  Train loss: 3.551089286804199  Test loss: 3.342400074005127 \n",
      "Epoch: 1/10:  mini-batch 4270/4459:  Train loss: 3.1225247383117676  Test loss: 3.3430402278900146 \n",
      "Epoch: 1/10:  mini-batch 4271/4459:  Train loss: 3.578526020050049  Test loss: 3.3435261249542236 \n",
      "Epoch: 1/10:  mini-batch 4272/4459:  Train loss: 3.627190351486206  Test loss: 3.344219446182251 \n",
      "Epoch: 1/10:  mini-batch 4273/4459:  Train loss: 3.293337821960449  Test loss: 3.344855785369873 \n",
      "Epoch: 1/10:  mini-batch 4274/4459:  Train loss: 3.297741174697876  Test loss: 3.3454227447509766 \n",
      "Epoch: 1/10:  mini-batch 4275/4459:  Train loss: 3.4524052143096924  Test loss: 3.3460733890533447 \n",
      "Epoch: 1/10:  mini-batch 4276/4459:  Train loss: 3.5542523860931396  Test loss: 3.346815586090088 \n",
      "Epoch: 1/10:  mini-batch 4277/4459:  Train loss: 3.3947818279266357  Test loss: 3.3474462032318115 \n",
      "Epoch: 1/10:  mini-batch 4278/4459:  Train loss: 3.5858218669891357  Test loss: 3.3480372428894043 \n",
      "Epoch: 1/10:  mini-batch 4279/4459:  Train loss: 3.2049078941345215  Test loss: 3.348419666290283 \n",
      "Epoch: 1/10:  mini-batch 4280/4459:  Train loss: 3.628239154815674  Test loss: 3.3488986492156982 \n",
      "Epoch: 1/10:  mini-batch 4281/4459:  Train loss: 3.444927215576172  Test loss: 3.34938383102417 \n",
      "Epoch: 1/10:  mini-batch 4282/4459:  Train loss: 3.4455630779266357  Test loss: 3.349783420562744 \n",
      "Epoch: 1/10:  mini-batch 4283/4459:  Train loss: 3.318798065185547  Test loss: 3.350168228149414 \n",
      "Epoch: 1/10:  mini-batch 4284/4459:  Train loss: 3.3034377098083496  Test loss: 3.3502864837646484 \n",
      "Epoch: 1/10:  mini-batch 4285/4459:  Train loss: 3.3118772506713867  Test loss: 3.3503994941711426 \n",
      "Epoch: 1/10:  mini-batch 4286/4459:  Train loss: 3.31284761428833  Test loss: 3.350228786468506 \n",
      "Epoch: 1/10:  mini-batch 4287/4459:  Train loss: 3.4480109214782715  Test loss: 3.3500661849975586 \n",
      "Epoch: 1/10:  mini-batch 4288/4459:  Train loss: 3.5930211544036865  Test loss: 3.3502490520477295 \n",
      "Epoch: 1/10:  mini-batch 4289/4459:  Train loss: 3.474351167678833  Test loss: 3.350584030151367 \n",
      "Epoch: 1/10:  mini-batch 4290/4459:  Train loss: 3.7171430587768555  Test loss: 3.3512797355651855 \n",
      "Epoch: 1/10:  mini-batch 4291/4459:  Train loss: 3.3524973392486572  Test loss: 3.3518452644348145 \n",
      "Epoch: 1/10:  mini-batch 4292/4459:  Train loss: 3.5666651725769043  Test loss: 3.3527345657348633 \n",
      "Epoch: 1/10:  mini-batch 4293/4459:  Train loss: 3.444920301437378  Test loss: 3.353515148162842 \n",
      "Epoch: 1/10:  mini-batch 4294/4459:  Train loss: 3.517925262451172  Test loss: 3.3543519973754883 \n",
      "Epoch: 1/10:  mini-batch 4295/4459:  Train loss: 3.3652968406677246  Test loss: 3.3549633026123047 \n",
      "Epoch: 1/10:  mini-batch 4296/4459:  Train loss: 3.2135143280029297  Test loss: 3.355241060256958 \n",
      "Epoch: 1/10:  mini-batch 4297/4459:  Train loss: 3.591841697692871  Test loss: 3.3557276725769043 \n",
      "Epoch: 1/10:  mini-batch 4298/4459:  Train loss: 3.413796901702881  Test loss: 3.356224536895752 \n",
      "Epoch: 1/10:  mini-batch 4299/4459:  Train loss: 3.3167777061462402  Test loss: 3.3566622734069824 \n",
      "Epoch: 1/10:  mini-batch 4300/4459:  Train loss: 3.284144163131714  Test loss: 3.356940746307373 \n",
      "Epoch: 1/10:  mini-batch 4301/4459:  Train loss: 3.4747700691223145  Test loss: 3.357217788696289 \n",
      "Epoch: 1/10:  mini-batch 4302/4459:  Train loss: 3.4613564014434814  Test loss: 3.3574090003967285 \n",
      "Epoch: 1/10:  mini-batch 4303/4459:  Train loss: 3.588636875152588  Test loss: 3.3578715324401855 \n",
      "Epoch: 1/10:  mini-batch 4304/4459:  Train loss: 3.2669572830200195  Test loss: 3.3579673767089844 \n",
      "Epoch: 1/10:  mini-batch 4305/4459:  Train loss: 3.315959930419922  Test loss: 3.357801914215088 \n",
      "Epoch: 1/10:  mini-batch 4306/4459:  Train loss: 3.3870444297790527  Test loss: 3.357588291168213 \n",
      "Epoch: 1/10:  mini-batch 4307/4459:  Train loss: 3.5089056491851807  Test loss: 3.3574602603912354 \n",
      "Epoch: 1/10:  mini-batch 4308/4459:  Train loss: 3.3336873054504395  Test loss: 3.3573172092437744 \n",
      "Epoch: 1/10:  mini-batch 4309/4459:  Train loss: 3.4377365112304688  Test loss: 3.357377052307129 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 4310/4459:  Train loss: 3.5240402221679688  Test loss: 3.357285976409912 \n",
      "Epoch: 1/10:  mini-batch 4311/4459:  Train loss: 3.2671618461608887  Test loss: 3.3567404747009277 \n",
      "Epoch: 1/10:  mini-batch 4312/4459:  Train loss: 3.489553451538086  Test loss: 3.3563618659973145 \n",
      "Epoch: 1/10:  mini-batch 4313/4459:  Train loss: 3.365741729736328  Test loss: 3.356306314468384 \n",
      "Epoch: 1/10:  mini-batch 4314/4459:  Train loss: 3.548025369644165  Test loss: 3.3565351963043213 \n",
      "Epoch: 1/10:  mini-batch 4315/4459:  Train loss: 3.415739059448242  Test loss: 3.356752395629883 \n",
      "Epoch: 1/10:  mini-batch 4316/4459:  Train loss: 3.3604204654693604  Test loss: 3.3567776679992676 \n",
      "Epoch: 1/10:  mini-batch 4317/4459:  Train loss: 3.315526247024536  Test loss: 3.356719970703125 \n",
      "Epoch: 1/10:  mini-batch 4318/4459:  Train loss: 3.48050594329834  Test loss: 3.3567397594451904 \n",
      "Epoch: 1/10:  mini-batch 4319/4459:  Train loss: 3.232316493988037  Test loss: 3.3567116260528564 \n",
      "Epoch: 1/10:  mini-batch 4320/4459:  Train loss: 3.4220328330993652  Test loss: 3.356255292892456 \n",
      "Epoch: 1/10:  mini-batch 4321/4459:  Train loss: 3.4554007053375244  Test loss: 3.3557608127593994 \n",
      "Epoch: 1/10:  mini-batch 4322/4459:  Train loss: 3.52230167388916  Test loss: 3.3556480407714844 \n",
      "Epoch: 1/10:  mini-batch 4323/4459:  Train loss: 3.51650071144104  Test loss: 3.355811357498169 \n",
      "Epoch: 1/10:  mini-batch 4324/4459:  Train loss: 3.5191752910614014  Test loss: 3.35601544380188 \n",
      "Epoch: 1/10:  mini-batch 4325/4459:  Train loss: 3.5356640815734863  Test loss: 3.3562815189361572 \n",
      "Epoch: 1/10:  mini-batch 4326/4459:  Train loss: 3.122128963470459  Test loss: 3.3563404083251953 \n",
      "Epoch: 1/10:  mini-batch 4327/4459:  Train loss: 3.59600830078125  Test loss: 3.3567779064178467 \n",
      "Epoch: 1/10:  mini-batch 4328/4459:  Train loss: 3.23309063911438  Test loss: 3.3569912910461426 \n",
      "Epoch: 1/10:  mini-batch 4329/4459:  Train loss: 3.20100736618042  Test loss: 3.3565359115600586 \n",
      "Epoch: 1/10:  mini-batch 4330/4459:  Train loss: 3.7443432807922363  Test loss: 3.3566975593566895 \n",
      "Epoch: 1/10:  mini-batch 4331/4459:  Train loss: 3.2351925373077393  Test loss: 3.356398105621338 \n",
      "Epoch: 1/10:  mini-batch 4332/4459:  Train loss: 3.253126382827759  Test loss: 3.3556766510009766 \n",
      "Epoch: 1/10:  mini-batch 4333/4459:  Train loss: 3.237363338470459  Test loss: 3.3546528816223145 \n",
      "Epoch: 1/10:  mini-batch 4334/4459:  Train loss: 3.2308189868927  Test loss: 3.3537240028381348 \n",
      "Epoch: 1/10:  mini-batch 4335/4459:  Train loss: 3.41790771484375  Test loss: 3.352972984313965 \n",
      "Epoch: 1/10:  mini-batch 4336/4459:  Train loss: 3.3678479194641113  Test loss: 3.3522775173187256 \n",
      "Epoch: 1/10:  mini-batch 4337/4459:  Train loss: 3.3693442344665527  Test loss: 3.3516807556152344 \n",
      "Epoch: 1/10:  mini-batch 4338/4459:  Train loss: 3.530866861343384  Test loss: 3.351381301879883 \n",
      "Epoch: 1/10:  mini-batch 4339/4459:  Train loss: 3.5308189392089844  Test loss: 3.351093292236328 \n",
      "Epoch: 1/10:  mini-batch 4340/4459:  Train loss: 3.4333555698394775  Test loss: 3.3508005142211914 \n",
      "Epoch: 1/10:  mini-batch 4341/4459:  Train loss: 3.360934257507324  Test loss: 3.3503198623657227 \n",
      "Epoch: 1/10:  mini-batch 4342/4459:  Train loss: 3.37164568901062  Test loss: 3.3498072624206543 \n",
      "Epoch: 1/10:  mini-batch 4343/4459:  Train loss: 3.297482967376709  Test loss: 3.3490519523620605 \n",
      "Epoch: 1/10:  mini-batch 4344/4459:  Train loss: 3.4467601776123047  Test loss: 3.3484809398651123 \n",
      "Epoch: 1/10:  mini-batch 4345/4459:  Train loss: 3.2466256618499756  Test loss: 3.347782611846924 \n",
      "Epoch: 1/10:  mini-batch 4346/4459:  Train loss: 3.4047884941101074  Test loss: 3.3467929363250732 \n",
      "Epoch: 1/10:  mini-batch 4347/4459:  Train loss: 3.3740487098693848  Test loss: 3.346045732498169 \n",
      "Epoch: 1/10:  mini-batch 4348/4459:  Train loss: 3.5144896507263184  Test loss: 3.345392942428589 \n",
      "Epoch: 1/10:  mini-batch 4349/4459:  Train loss: 3.1850426197052  Test loss: 3.3446455001831055 \n",
      "Epoch: 1/10:  mini-batch 4350/4459:  Train loss: 3.4220871925354004  Test loss: 3.3442087173461914 \n",
      "Epoch: 1/10:  mini-batch 4351/4459:  Train loss: 3.5831851959228516  Test loss: 3.3439948558807373 \n",
      "Epoch: 1/10:  mini-batch 4352/4459:  Train loss: 3.3379323482513428  Test loss: 3.3438801765441895 \n",
      "Epoch: 1/10:  mini-batch 4353/4459:  Train loss: 3.444403648376465  Test loss: 3.343719482421875 \n",
      "Epoch: 1/10:  mini-batch 4354/4459:  Train loss: 3.1567139625549316  Test loss: 3.3435094356536865 \n",
      "Epoch: 1/10:  mini-batch 4355/4459:  Train loss: 3.475247383117676  Test loss: 3.343454360961914 \n",
      "Epoch: 1/10:  mini-batch 4356/4459:  Train loss: 3.3533120155334473  Test loss: 3.343735694885254 \n",
      "Epoch: 1/10:  mini-batch 4357/4459:  Train loss: 3.2082011699676514  Test loss: 3.343963384628296 \n",
      "Epoch: 1/10:  mini-batch 4358/4459:  Train loss: 3.3306736946105957  Test loss: 3.344095230102539 \n",
      "Epoch: 1/10:  mini-batch 4359/4459:  Train loss: 3.073784589767456  Test loss: 3.3440375328063965 \n",
      "Epoch: 1/10:  mini-batch 4360/4459:  Train loss: 3.701991081237793  Test loss: 3.344172954559326 \n",
      "Epoch: 1/10:  mini-batch 4361/4459:  Train loss: 3.372671604156494  Test loss: 3.344407081604004 \n",
      "Epoch: 1/10:  mini-batch 4362/4459:  Train loss: 3.1804471015930176  Test loss: 3.3446850776672363 \n",
      "Epoch: 1/10:  mini-batch 4363/4459:  Train loss: 3.675720691680908  Test loss: 3.345202922821045 \n",
      "Epoch: 1/10:  mini-batch 4364/4459:  Train loss: 3.709749460220337  Test loss: 3.3463282585144043 \n",
      "Epoch: 1/10:  mini-batch 4365/4459:  Train loss: 3.3416292667388916  Test loss: 3.3474185466766357 \n",
      "Epoch: 1/10:  mini-batch 4366/4459:  Train loss: 3.4183568954467773  Test loss: 3.3486552238464355 \n",
      "Epoch: 1/10:  mini-batch 4367/4459:  Train loss: 3.685093641281128  Test loss: 3.3500916957855225 \n",
      "Epoch: 1/10:  mini-batch 4368/4459:  Train loss: 3.9650421142578125  Test loss: 3.3517701625823975 \n",
      "Epoch: 1/10:  mini-batch 4369/4459:  Train loss: 3.431264877319336  Test loss: 3.3531811237335205 \n",
      "Epoch: 1/10:  mini-batch 4370/4459:  Train loss: 3.377788782119751  Test loss: 3.354166030883789 \n",
      "Epoch: 1/10:  mini-batch 4371/4459:  Train loss: 3.2051804065704346  Test loss: 3.354745864868164 \n",
      "Epoch: 1/10:  mini-batch 4372/4459:  Train loss: 3.1241707801818848  Test loss: 3.354915142059326 \n",
      "Epoch: 1/10:  mini-batch 4373/4459:  Train loss: 3.27724027633667  Test loss: 3.354520797729492 \n",
      "Epoch: 1/10:  mini-batch 4374/4459:  Train loss: 3.1484217643737793  Test loss: 3.3542282581329346 \n",
      "Epoch: 1/10:  mini-batch 4375/4459:  Train loss: 3.3639121055603027  Test loss: 3.3541533946990967 \n",
      "Epoch: 1/10:  mini-batch 4376/4459:  Train loss: 3.39985990524292  Test loss: 3.354185104370117 \n",
      "Epoch: 1/10:  mini-batch 4377/4459:  Train loss: 3.3432374000549316  Test loss: 3.3542771339416504 \n",
      "Epoch: 1/10:  mini-batch 4378/4459:  Train loss: 3.2291488647460938  Test loss: 3.3543636798858643 \n",
      "Epoch: 1/10:  mini-batch 4379/4459:  Train loss: 3.470423698425293  Test loss: 3.354299545288086 \n",
      "Epoch: 1/10:  mini-batch 4380/4459:  Train loss: 3.318128824234009  Test loss: 3.354506015777588 \n",
      "Epoch: 1/10:  mini-batch 4381/4459:  Train loss: 3.3357343673706055  Test loss: 3.3547263145446777 \n",
      "Epoch: 1/10:  mini-batch 4382/4459:  Train loss: 3.601710796356201  Test loss: 3.355137586593628 \n",
      "Epoch: 1/10:  mini-batch 4383/4459:  Train loss: 3.5117721557617188  Test loss: 3.3557052612304688 \n",
      "Epoch: 1/10:  mini-batch 4384/4459:  Train loss: 3.418381690979004  Test loss: 3.3561291694641113 \n",
      "Epoch: 1/10:  mini-batch 4385/4459:  Train loss: 3.2294199466705322  Test loss: 3.3564248085021973 \n",
      "Epoch: 1/10:  mini-batch 4386/4459:  Train loss: 3.4754650592803955  Test loss: 3.3568780422210693 \n",
      "Epoch: 1/10:  mini-batch 4387/4459:  Train loss: 3.4858453273773193  Test loss: 3.357792854309082 \n",
      "Epoch: 1/10:  mini-batch 4388/4459:  Train loss: 3.2499799728393555  Test loss: 3.3583648204803467 \n",
      "Epoch: 1/10:  mini-batch 4389/4459:  Train loss: 3.32772159576416  Test loss: 3.3588123321533203 \n",
      "Epoch: 1/10:  mini-batch 4390/4459:  Train loss: 3.328305721282959  Test loss: 3.3592734336853027 \n",
      "Epoch: 1/10:  mini-batch 4391/4459:  Train loss: 3.0744855403900146  Test loss: 3.3596253395080566 \n",
      "Epoch: 1/10:  mini-batch 4392/4459:  Train loss: 3.4504032135009766  Test loss: 3.3594894409179688 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10:  mini-batch 4393/4459:  Train loss: 3.275660514831543  Test loss: 3.3591690063476562 \n",
      "Epoch: 1/10:  mini-batch 4394/4459:  Train loss: 3.6197197437286377  Test loss: 3.359119415283203 \n",
      "Epoch: 1/10:  mini-batch 4395/4459:  Train loss: 3.4959840774536133  Test loss: 3.35919189453125 \n",
      "Epoch: 1/10:  mini-batch 4396/4459:  Train loss: 3.335646867752075  Test loss: 3.3591604232788086 \n",
      "Epoch: 1/10:  mini-batch 4397/4459:  Train loss: 3.494874954223633  Test loss: 3.358457326889038 \n",
      "Epoch: 1/10:  mini-batch 4398/4459:  Train loss: 3.5500693321228027  Test loss: 3.3583617210388184 \n",
      "Epoch: 1/10:  mini-batch 4399/4459:  Train loss: 3.491000175476074  Test loss: 3.3587238788604736 \n",
      "Epoch: 1/10:  mini-batch 4400/4459:  Train loss: 3.3466477394104004  Test loss: 3.3591713905334473 \n",
      "Epoch: 1/10:  mini-batch 4401/4459:  Train loss: 3.1460390090942383  Test loss: 3.3593907356262207 \n",
      "Epoch: 1/10:  mini-batch 4402/4459:  Train loss: 3.2175979614257812  Test loss: 3.3594746589660645 \n",
      "Epoch: 1/10:  mini-batch 4403/4459:  Train loss: 3.353569984436035  Test loss: 3.359666347503662 \n",
      "Epoch: 1/10:  mini-batch 4404/4459:  Train loss: 3.3888931274414062  Test loss: 3.359651803970337 \n",
      "Epoch: 1/10:  mini-batch 4405/4459:  Train loss: 3.3596529960632324  Test loss: 3.3591935634613037 \n",
      "Epoch: 1/10:  mini-batch 4406/4459:  Train loss: 3.1153459548950195  Test loss: 3.358621120452881 \n",
      "Epoch: 1/10:  mini-batch 4407/4459:  Train loss: 3.5034375190734863  Test loss: 3.3585433959960938 \n",
      "Epoch: 1/10:  mini-batch 4408/4459:  Train loss: 3.4514012336730957  Test loss: 3.3585808277130127 \n",
      "Epoch: 1/10:  mini-batch 4409/4459:  Train loss: 3.4369473457336426  Test loss: 3.358602523803711 \n",
      "Epoch: 1/10:  mini-batch 4410/4459:  Train loss: 3.3805315494537354  Test loss: 3.358527183532715 \n",
      "Epoch: 1/10:  mini-batch 4411/4459:  Train loss: 3.237654209136963  Test loss: 3.3587257862091064 \n",
      "Epoch: 1/10:  mini-batch 4412/4459:  Train loss: 3.292537212371826  Test loss: 3.358860731124878 \n",
      "Epoch: 1/10:  mini-batch 4413/4459:  Train loss: 3.305952548980713  Test loss: 3.3590571880340576 \n",
      "Epoch: 1/10:  mini-batch 4414/4459:  Train loss: 3.3047375679016113  Test loss: 3.358947277069092 \n",
      "Epoch: 1/10:  mini-batch 4415/4459:  Train loss: 3.420372247695923  Test loss: 3.3585803508758545 \n",
      "Epoch: 1/10:  mini-batch 4416/4459:  Train loss: 3.328340530395508  Test loss: 3.3576817512512207 \n",
      "Epoch: 1/10:  mini-batch 4417/4459:  Train loss: 3.470770835876465  Test loss: 3.357032537460327 \n",
      "Epoch: 1/10:  mini-batch 4418/4459:  Train loss: 3.3903861045837402  Test loss: 3.3566060066223145 \n",
      "Epoch: 1/10:  mini-batch 4419/4459:  Train loss: 3.3788411617279053  Test loss: 3.3564188480377197 \n",
      "Epoch: 1/10:  mini-batch 4420/4459:  Train loss: 3.3733482360839844  Test loss: 3.3560831546783447 \n",
      "Epoch: 1/10:  mini-batch 4421/4459:  Train loss: 3.607097625732422  Test loss: 3.3555057048797607 \n",
      "Epoch: 1/10:  mini-batch 4422/4459:  Train loss: 3.3656649589538574  Test loss: 3.3549628257751465 \n",
      "Epoch: 1/10:  mini-batch 4423/4459:  Train loss: 3.71157169342041  Test loss: 3.3544745445251465 \n",
      "Epoch: 1/10:  mini-batch 4424/4459:  Train loss: 3.4542160034179688  Test loss: 3.3540520668029785 \n",
      "Epoch: 1/10:  mini-batch 4425/4459:  Train loss: 3.3174712657928467  Test loss: 3.3536083698272705 \n",
      "Epoch: 1/10:  mini-batch 4426/4459:  Train loss: 3.492084264755249  Test loss: 3.353275775909424 \n",
      "Epoch: 1/10:  mini-batch 4427/4459:  Train loss: 3.2741851806640625  Test loss: 3.3529319763183594 \n",
      "Epoch: 1/10:  mini-batch 4428/4459:  Train loss: 3.3567872047424316  Test loss: 3.3526241779327393 \n",
      "Epoch: 1/10:  mini-batch 4429/4459:  Train loss: 3.2519779205322266  Test loss: 3.3522961139678955 \n",
      "Epoch: 1/10:  mini-batch 4430/4459:  Train loss: 3.5200729370117188  Test loss: 3.3521599769592285 \n",
      "Epoch: 1/10:  mini-batch 4431/4459:  Train loss: 3.4061803817749023  Test loss: 3.3519644737243652 \n",
      "Epoch: 1/10:  mini-batch 4432/4459:  Train loss: 3.3248705863952637  Test loss: 3.351663112640381 \n",
      "Epoch: 1/10:  mini-batch 4433/4459:  Train loss: 3.4266679286956787  Test loss: 3.3516042232513428 \n",
      "Epoch: 1/10:  mini-batch 4434/4459:  Train loss: 3.3631410598754883  Test loss: 3.351165533065796 \n",
      "Epoch: 1/10:  mini-batch 4435/4459:  Train loss: 3.5210494995117188  Test loss: 3.3512773513793945 \n",
      "Epoch: 1/10:  mini-batch 4436/4459:  Train loss: 3.2486014366149902  Test loss: 3.351245880126953 \n",
      "Epoch: 1/10:  mini-batch 4437/4459:  Train loss: 3.113645315170288  Test loss: 3.351085662841797 \n",
      "Epoch: 1/10:  mini-batch 4438/4459:  Train loss: 3.0493907928466797  Test loss: 3.3508176803588867 \n",
      "Epoch: 1/10:  mini-batch 4439/4459:  Train loss: 3.3037118911743164  Test loss: 3.3504366874694824 \n",
      "Epoch: 1/10:  mini-batch 4440/4459:  Train loss: 3.6268513202667236  Test loss: 3.3503756523132324 \n",
      "Epoch: 1/10:  mini-batch 4441/4459:  Train loss: 3.692495584487915  Test loss: 3.350620746612549 \n",
      "Epoch: 1/10:  mini-batch 4442/4459:  Train loss: 3.3211631774902344  Test loss: 3.351008653640747 \n",
      "Epoch: 1/10:  mini-batch 4443/4459:  Train loss: 3.1157429218292236  Test loss: 3.3511276245117188 \n",
      "Epoch: 1/10:  mini-batch 4444/4459:  Train loss: 3.214581251144409  Test loss: 3.350832939147949 \n",
      "Epoch: 1/10:  mini-batch 4445/4459:  Train loss: 3.0812110900878906  Test loss: 3.3507142066955566 \n",
      "Epoch: 1/10:  mini-batch 4446/4459:  Train loss: 3.1555464267730713  Test loss: 3.350860118865967 \n",
      "Epoch: 1/10:  mini-batch 4447/4459:  Train loss: 3.270005702972412  Test loss: 3.351087808609009 \n",
      "Epoch: 1/10:  mini-batch 4448/4459:  Train loss: 3.486210346221924  Test loss: 3.350882053375244 \n",
      "Epoch: 1/10:  mini-batch 4449/4459:  Train loss: 3.6035327911376953  Test loss: 3.350475788116455 \n",
      "Epoch: 1/10:  mini-batch 4450/4459:  Train loss: 3.42602276802063  Test loss: 3.3499221801757812 \n",
      "Epoch: 1/10:  mini-batch 4451/4459:  Train loss: 3.378333568572998  Test loss: 3.3489222526550293 \n",
      "Epoch: 1/10:  mini-batch 4452/4459:  Train loss: 3.2855725288391113  Test loss: 3.3479971885681152 \n",
      "Epoch: 1/10:  mini-batch 4453/4459:  Train loss: 3.5145814418792725  Test loss: 3.3474602699279785 \n",
      "Epoch: 1/10:  mini-batch 4454/4459:  Train loss: 3.2905545234680176  Test loss: 3.3468434810638428 \n",
      "Epoch: 1/10:  mini-batch 4455/4459:  Train loss: 3.1215319633483887  Test loss: 3.3463521003723145 \n",
      "Epoch: 1/10:  mini-batch 4456/4459:  Train loss: 3.118929862976074  Test loss: 3.346242666244507 \n",
      "Epoch: 1/10:  mini-batch 4457/4459:  Train loss: 3.2639925479888916  Test loss: 3.3462729454040527 \n",
      "Epoch: 1/10:  mini-batch 4458/4459:  Train loss: 3.5429961681365967  Test loss: 3.345628261566162 \n",
      "Epoch: 1/10:  mini-batch 4459/4459:  Train loss: 3.3416624069213867  Test loss: 3.3450229167938232 \n",
      "Epoch: 2/10:  mini-batch 1/4459:  Train loss: 3.2274980545043945  Test loss: 3.344597339630127 \n",
      "Epoch: 2/10:  mini-batch 2/4459:  Train loss: 3.277329206466675  Test loss: 3.3441367149353027 \n",
      "Epoch: 2/10:  mini-batch 3/4459:  Train loss: 4.137572288513184  Test loss: 3.343876838684082 \n",
      "Epoch: 2/10:  mini-batch 4/4459:  Train loss: 3.8609819412231445  Test loss: 3.343749523162842 \n",
      "Epoch: 2/10:  mini-batch 5/4459:  Train loss: 3.7660603523254395  Test loss: 3.3439102172851562 \n",
      "Epoch: 2/10:  mini-batch 6/4459:  Train loss: 3.4407784938812256  Test loss: 3.344028949737549 \n",
      "Epoch: 2/10:  mini-batch 7/4459:  Train loss: 3.8691344261169434  Test loss: 3.3444201946258545 \n",
      "Epoch: 2/10:  mini-batch 8/4459:  Train loss: 3.4118762016296387  Test loss: 3.3446786403656006 \n",
      "Epoch: 2/10:  mini-batch 9/4459:  Train loss: 3.2184629440307617  Test loss: 3.344710350036621 \n",
      "Epoch: 2/10:  mini-batch 10/4459:  Train loss: 3.2604482173919678  Test loss: 3.3446426391601562 \n",
      "Epoch: 2/10:  mini-batch 11/4459:  Train loss: 3.192093849182129  Test loss: 3.3442208766937256 \n",
      "Epoch: 2/10:  mini-batch 12/4459:  Train loss: 3.5466971397399902  Test loss: 3.3440074920654297 \n",
      "Epoch: 2/10:  mini-batch 13/4459:  Train loss: 3.5453412532806396  Test loss: 3.3441121578216553 \n",
      "Epoch: 2/10:  mini-batch 14/4459:  Train loss: 3.151423931121826  Test loss: 3.3436810970306396 \n",
      "Epoch: 2/10:  mini-batch 15/4459:  Train loss: 3.2642476558685303  Test loss: 3.3433547019958496 \n",
      "Epoch: 2/10:  mini-batch 16/4459:  Train loss: 3.21016263961792  Test loss: 3.3427014350891113 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 17/4459:  Train loss: 3.470407485961914  Test loss: 3.342186450958252 \n",
      "Epoch: 2/10:  mini-batch 18/4459:  Train loss: 3.659435749053955  Test loss: 3.3419971466064453 \n",
      "Epoch: 2/10:  mini-batch 19/4459:  Train loss: 3.8438174724578857  Test loss: 3.342362403869629 \n",
      "Epoch: 2/10:  mini-batch 20/4459:  Train loss: 3.5643887519836426  Test loss: 3.342662811279297 \n",
      "Epoch: 2/10:  mini-batch 21/4459:  Train loss: 3.4928081035614014  Test loss: 3.343095302581787 \n",
      "Epoch: 2/10:  mini-batch 22/4459:  Train loss: 3.038250207901001  Test loss: 3.34309458732605 \n",
      "Epoch: 2/10:  mini-batch 23/4459:  Train loss: 3.0045976638793945  Test loss: 3.3428049087524414 \n",
      "Epoch: 2/10:  mini-batch 24/4459:  Train loss: 3.2003865242004395  Test loss: 3.3424999713897705 \n",
      "Epoch: 2/10:  mini-batch 25/4459:  Train loss: 3.240459680557251  Test loss: 3.342189311981201 \n",
      "Epoch: 2/10:  mini-batch 26/4459:  Train loss: 3.268599033355713  Test loss: 3.3417654037475586 \n",
      "Epoch: 2/10:  mini-batch 27/4459:  Train loss: 3.179286479949951  Test loss: 3.34138822555542 \n",
      "Epoch: 2/10:  mini-batch 28/4459:  Train loss: 3.312102794647217  Test loss: 3.340944290161133 \n",
      "Epoch: 2/10:  mini-batch 29/4459:  Train loss: 3.4152369499206543  Test loss: 3.340127944946289 \n",
      "Epoch: 2/10:  mini-batch 30/4459:  Train loss: 3.390105724334717  Test loss: 3.3395118713378906 \n",
      "Epoch: 2/10:  mini-batch 31/4459:  Train loss: 3.5047621726989746  Test loss: 3.3391599655151367 \n",
      "Epoch: 2/10:  mini-batch 32/4459:  Train loss: 3.363114595413208  Test loss: 3.3389697074890137 \n",
      "Epoch: 2/10:  mini-batch 33/4459:  Train loss: 3.4916625022888184  Test loss: 3.339046001434326 \n",
      "Epoch: 2/10:  mini-batch 34/4459:  Train loss: 3.4885950088500977  Test loss: 3.339231014251709 \n",
      "Epoch: 2/10:  mini-batch 35/4459:  Train loss: 3.424091339111328  Test loss: 3.339566946029663 \n",
      "Epoch: 2/10:  mini-batch 36/4459:  Train loss: 3.442044258117676  Test loss: 3.3401286602020264 \n",
      "Epoch: 2/10:  mini-batch 37/4459:  Train loss: 3.1352291107177734  Test loss: 3.340501546859741 \n",
      "Epoch: 2/10:  mini-batch 38/4459:  Train loss: 3.5097713470458984  Test loss: 3.3412461280822754 \n",
      "Epoch: 2/10:  mini-batch 39/4459:  Train loss: 3.188113212585449  Test loss: 3.3419995307922363 \n",
      "Epoch: 2/10:  mini-batch 40/4459:  Train loss: 3.2153987884521484  Test loss: 3.3428773880004883 \n",
      "Epoch: 2/10:  mini-batch 41/4459:  Train loss: 3.15449857711792  Test loss: 3.34378981590271 \n",
      "Epoch: 2/10:  mini-batch 42/4459:  Train loss: 3.251710891723633  Test loss: 3.344749689102173 \n",
      "Epoch: 2/10:  mini-batch 43/4459:  Train loss: 3.2967429161071777  Test loss: 3.3457858562469482 \n",
      "Epoch: 2/10:  mini-batch 44/4459:  Train loss: 3.23826265335083  Test loss: 3.3470299243927 \n",
      "Epoch: 2/10:  mini-batch 45/4459:  Train loss: 3.2752294540405273  Test loss: 3.3489179611206055 \n",
      "Epoch: 2/10:  mini-batch 46/4459:  Train loss: 2.9935717582702637  Test loss: 3.349689483642578 \n",
      "Epoch: 2/10:  mini-batch 47/4459:  Train loss: 2.8827061653137207  Test loss: 3.350449800491333 \n",
      "Epoch: 2/10:  mini-batch 48/4459:  Train loss: 3.026092767715454  Test loss: 3.3515357971191406 \n",
      "Epoch: 2/10:  mini-batch 49/4459:  Train loss: 3.4440953731536865  Test loss: 3.35317325592041 \n",
      "Epoch: 2/10:  mini-batch 50/4459:  Train loss: 3.151275157928467  Test loss: 3.355039596557617 \n",
      "Epoch: 2/10:  mini-batch 51/4459:  Train loss: 2.9011425971984863  Test loss: 3.356902599334717 \n",
      "Epoch: 2/10:  mini-batch 52/4459:  Train loss: 3.172670602798462  Test loss: 3.359433650970459 \n",
      "Epoch: 2/10:  mini-batch 53/4459:  Train loss: 2.9774622917175293  Test loss: 3.362246513366699 \n",
      "Epoch: 2/10:  mini-batch 54/4459:  Train loss: 3.1714162826538086  Test loss: 3.365837574005127 \n",
      "Epoch: 2/10:  mini-batch 55/4459:  Train loss: 3.0989694595336914  Test loss: 3.3698463439941406 \n",
      "Epoch: 2/10:  mini-batch 56/4459:  Train loss: 2.9474849700927734  Test loss: 3.3742246627807617 \n",
      "Epoch: 2/10:  mini-batch 57/4459:  Train loss: 3.000896453857422  Test loss: 3.3784985542297363 \n",
      "Epoch: 2/10:  mini-batch 58/4459:  Train loss: 2.928493022918701  Test loss: 3.38340425491333 \n",
      "Epoch: 2/10:  mini-batch 59/4459:  Train loss: 3.1918516159057617  Test loss: 3.388923406600952 \n",
      "Epoch: 2/10:  mini-batch 60/4459:  Train loss: 2.6756300926208496  Test loss: 3.3956334590911865 \n",
      "Epoch: 2/10:  mini-batch 61/4459:  Train loss: 3.2952475547790527  Test loss: 3.4031171798706055 \n",
      "Epoch: 2/10:  mini-batch 62/4459:  Train loss: 3.378469944000244  Test loss: 3.4098801612854004 \n",
      "Epoch: 2/10:  mini-batch 63/4459:  Train loss: 2.6332244873046875  Test loss: 3.41805100440979 \n",
      "Epoch: 2/10:  mini-batch 64/4459:  Train loss: 3.3413400650024414  Test loss: 3.4265966415405273 \n",
      "Epoch: 2/10:  mini-batch 65/4459:  Train loss: 3.1903176307678223  Test loss: 3.4359920024871826 \n",
      "Epoch: 2/10:  mini-batch 66/4459:  Train loss: 3.0328211784362793  Test loss: 3.446272373199463 \n",
      "Epoch: 2/10:  mini-batch 67/4459:  Train loss: 2.929675340652466  Test loss: 3.457415819168091 \n",
      "Epoch: 2/10:  mini-batch 68/4459:  Train loss: 2.765439510345459  Test loss: 3.47003173828125 \n",
      "Epoch: 2/10:  mini-batch 69/4459:  Train loss: 2.9249958992004395  Test loss: 3.4841885566711426 \n",
      "Epoch: 2/10:  mini-batch 70/4459:  Train loss: 2.581275224685669  Test loss: 3.4995031356811523 \n",
      "Epoch: 2/10:  mini-batch 71/4459:  Train loss: 2.5248913764953613  Test loss: 3.51643705368042 \n",
      "Epoch: 2/10:  mini-batch 72/4459:  Train loss: 2.8658127784729004  Test loss: 3.535928726196289 \n",
      "Epoch: 2/10:  mini-batch 73/4459:  Train loss: 2.731663942337036  Test loss: 3.5561022758483887 \n",
      "Epoch: 2/10:  mini-batch 74/4459:  Train loss: 2.524852752685547  Test loss: 3.5809264183044434 \n",
      "Epoch: 2/10:  mini-batch 75/4459:  Train loss: 3.0789809226989746  Test loss: 3.6066675186157227 \n",
      "Epoch: 2/10:  mini-batch 76/4459:  Train loss: 3.38143253326416  Test loss: 3.624145746231079 \n",
      "Epoch: 2/10:  mini-batch 77/4459:  Train loss: 2.6723294258117676  Test loss: 3.646017074584961 \n",
      "Epoch: 2/10:  mini-batch 78/4459:  Train loss: 2.4072811603546143  Test loss: 3.6747050285339355 \n",
      "Epoch: 2/10:  mini-batch 79/4459:  Train loss: 2.9161205291748047  Test loss: 3.7036683559417725 \n",
      "Epoch: 2/10:  mini-batch 80/4459:  Train loss: 3.1965718269348145  Test loss: 3.7251391410827637 \n",
      "Epoch: 2/10:  mini-batch 81/4459:  Train loss: 3.0126287937164307  Test loss: 3.74147367477417 \n",
      "Epoch: 2/10:  mini-batch 82/4459:  Train loss: 2.5546326637268066  Test loss: 3.76308012008667 \n",
      "Epoch: 2/10:  mini-batch 83/4459:  Train loss: 2.6574347019195557  Test loss: 3.7879815101623535 \n",
      "Epoch: 2/10:  mini-batch 84/4459:  Train loss: 2.7186431884765625  Test loss: 3.8156304359436035 \n",
      "Epoch: 2/10:  mini-batch 85/4459:  Train loss: 2.3598716259002686  Test loss: 3.8532464504241943 \n",
      "Epoch: 2/10:  mini-batch 86/4459:  Train loss: 2.884748697280884  Test loss: 3.8902504444122314 \n",
      "Epoch: 2/10:  mini-batch 87/4459:  Train loss: 1.9567234516143799  Test loss: 3.9508297443389893 \n",
      "Epoch: 2/10:  mini-batch 88/4459:  Train loss: 2.4756388664245605  Test loss: 4.033883094787598 \n",
      "Epoch: 2/10:  mini-batch 89/4459:  Train loss: 2.662202835083008  Test loss: 4.1215314865112305 \n",
      "Epoch: 2/10:  mini-batch 90/4459:  Train loss: 2.9153451919555664  Test loss: 4.171377182006836 \n",
      "Epoch: 2/10:  mini-batch 91/4459:  Train loss: 2.9993910789489746  Test loss: 4.196049213409424 \n",
      "Epoch: 2/10:  mini-batch 92/4459:  Train loss: 2.257598876953125  Test loss: 4.233927249908447 \n",
      "Epoch: 2/10:  mini-batch 93/4459:  Train loss: 2.3799681663513184  Test loss: 4.282568454742432 \n",
      "Epoch: 2/10:  mini-batch 94/4459:  Train loss: 3.5328803062438965  Test loss: 4.2624406814575195 \n",
      "Epoch: 2/10:  mini-batch 95/4459:  Train loss: 2.331374168395996  Test loss: 4.248581886291504 \n",
      "Epoch: 2/10:  mini-batch 96/4459:  Train loss: 2.018129825592041  Test loss: 4.262922763824463 \n",
      "Epoch: 2/10:  mini-batch 97/4459:  Train loss: 2.8411355018615723  Test loss: 4.269126892089844 \n",
      "Epoch: 2/10:  mini-batch 98/4459:  Train loss: 2.879645824432373  Test loss: 4.258605003356934 \n",
      "Epoch: 2/10:  mini-batch 99/4459:  Train loss: 1.906930923461914  Test loss: 4.267406463623047 \n",
      "Epoch: 2/10:  mini-batch 100/4459:  Train loss: 2.199535369873047  Test loss: 4.28569221496582 \n",
      "Epoch: 2/10:  mini-batch 101/4459:  Train loss: 2.9761223793029785  Test loss: 4.294643878936768 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 102/4459:  Train loss: 2.61848521232605  Test loss: 4.305170059204102 \n",
      "Epoch: 2/10:  mini-batch 103/4459:  Train loss: 2.8522114753723145  Test loss: 4.3083295822143555 \n",
      "Epoch: 2/10:  mini-batch 104/4459:  Train loss: 2.384995222091675  Test loss: 4.320925235748291 \n",
      "Epoch: 2/10:  mini-batch 105/4459:  Train loss: 2.9318833351135254  Test loss: 4.309662342071533 \n",
      "Epoch: 2/10:  mini-batch 106/4459:  Train loss: 2.7011067867279053  Test loss: 4.292078495025635 \n",
      "Epoch: 2/10:  mini-batch 107/4459:  Train loss: 2.9648966789245605  Test loss: 4.262239933013916 \n",
      "Epoch: 2/10:  mini-batch 108/4459:  Train loss: 2.4010915756225586  Test loss: 4.244223117828369 \n",
      "Epoch: 2/10:  mini-batch 109/4459:  Train loss: 1.8878161907196045  Test loss: 4.250369071960449 \n",
      "Epoch: 2/10:  mini-batch 110/4459:  Train loss: 1.9339468479156494  Test loss: 4.2731499671936035 \n",
      "Epoch: 2/10:  mini-batch 111/4459:  Train loss: 2.3047995567321777  Test loss: 4.29816198348999 \n",
      "Epoch: 2/10:  mini-batch 112/4459:  Train loss: 2.6121325492858887  Test loss: 4.3246660232543945 \n",
      "Epoch: 2/10:  mini-batch 113/4459:  Train loss: 2.722317695617676  Test loss: 4.3462934494018555 \n",
      "Epoch: 2/10:  mini-batch 114/4459:  Train loss: 2.288259983062744  Test loss: 4.374094009399414 \n",
      "Epoch: 2/10:  mini-batch 115/4459:  Train loss: 2.146482467651367  Test loss: 4.410808086395264 \n",
      "Epoch: 2/10:  mini-batch 116/4459:  Train loss: 2.7511143684387207  Test loss: 4.439127445220947 \n",
      "Epoch: 2/10:  mini-batch 117/4459:  Train loss: 2.3207483291625977  Test loss: 4.477410316467285 \n",
      "Epoch: 2/10:  mini-batch 118/4459:  Train loss: 3.1657590866088867  Test loss: 4.476431369781494 \n",
      "Epoch: 2/10:  mini-batch 119/4459:  Train loss: 3.3687984943389893  Test loss: 4.431756496429443 \n",
      "Epoch: 2/10:  mini-batch 120/4459:  Train loss: 2.1951117515563965  Test loss: 4.401254177093506 \n",
      "Epoch: 2/10:  mini-batch 121/4459:  Train loss: 2.386913299560547  Test loss: 4.38340425491333 \n",
      "Epoch: 2/10:  mini-batch 122/4459:  Train loss: 2.778258800506592  Test loss: 4.362734794616699 \n",
      "Epoch: 2/10:  mini-batch 123/4459:  Train loss: 2.6916704177856445  Test loss: 4.341044902801514 \n",
      "Epoch: 2/10:  mini-batch 124/4459:  Train loss: 2.7381134033203125  Test loss: 4.321044921875 \n",
      "Epoch: 2/10:  mini-batch 125/4459:  Train loss: 2.876264810562134  Test loss: 4.293320655822754 \n",
      "Epoch: 2/10:  mini-batch 126/4459:  Train loss: 3.239396095275879  Test loss: 4.255390644073486 \n",
      "Epoch: 2/10:  mini-batch 127/4459:  Train loss: 2.5171561241149902  Test loss: 4.227867126464844 \n",
      "Epoch: 2/10:  mini-batch 128/4459:  Train loss: 3.1227471828460693  Test loss: 4.194109916687012 \n",
      "Epoch: 2/10:  mini-batch 129/4459:  Train loss: 2.4792559146881104  Test loss: 4.16979455947876 \n",
      "Epoch: 2/10:  mini-batch 130/4459:  Train loss: 2.4986989498138428  Test loss: 4.154733657836914 \n",
      "Epoch: 2/10:  mini-batch 131/4459:  Train loss: 2.911485433578491  Test loss: 4.141547203063965 \n",
      "Epoch: 2/10:  mini-batch 132/4459:  Train loss: 2.686107873916626  Test loss: 4.132001876831055 \n",
      "Epoch: 2/10:  mini-batch 133/4459:  Train loss: 2.367534875869751  Test loss: 4.12727165222168 \n",
      "Epoch: 2/10:  mini-batch 134/4459:  Train loss: 2.7629551887512207  Test loss: 4.126054763793945 \n",
      "Epoch: 2/10:  mini-batch 135/4459:  Train loss: 2.7321784496307373  Test loss: 4.130251884460449 \n",
      "Epoch: 2/10:  mini-batch 136/4459:  Train loss: 2.620548725128174  Test loss: 4.1379265785217285 \n",
      "Epoch: 2/10:  mini-batch 137/4459:  Train loss: 2.7302334308624268  Test loss: 4.148830413818359 \n",
      "Epoch: 2/10:  mini-batch 138/4459:  Train loss: 2.574422597885132  Test loss: 4.164887428283691 \n",
      "Epoch: 2/10:  mini-batch 139/4459:  Train loss: 2.26379132270813  Test loss: 4.183901786804199 \n",
      "Epoch: 2/10:  mini-batch 140/4459:  Train loss: 2.7219929695129395  Test loss: 4.2065019607543945 \n",
      "Epoch: 2/10:  mini-batch 141/4459:  Train loss: 2.331211566925049  Test loss: 4.230499267578125 \n",
      "Epoch: 2/10:  mini-batch 142/4459:  Train loss: 2.240143299102783  Test loss: 4.2544636726379395 \n",
      "Epoch: 2/10:  mini-batch 143/4459:  Train loss: 2.470139741897583  Test loss: 4.283687114715576 \n",
      "Epoch: 2/10:  mini-batch 144/4459:  Train loss: 2.291116714477539  Test loss: 4.312976837158203 \n",
      "Epoch: 2/10:  mini-batch 145/4459:  Train loss: 2.35774827003479  Test loss: 4.341975212097168 \n",
      "Epoch: 2/10:  mini-batch 146/4459:  Train loss: 2.252897024154663  Test loss: 4.371286869049072 \n",
      "Epoch: 2/10:  mini-batch 147/4459:  Train loss: 2.9023165702819824  Test loss: 4.392735958099365 \n",
      "Epoch: 2/10:  mini-batch 148/4459:  Train loss: 2.5715322494506836  Test loss: 4.413568019866943 \n",
      "Epoch: 2/10:  mini-batch 149/4459:  Train loss: 2.679605484008789  Test loss: 4.436047077178955 \n",
      "Epoch: 2/10:  mini-batch 150/4459:  Train loss: 3.001079559326172  Test loss: 4.449925899505615 \n",
      "Epoch: 2/10:  mini-batch 151/4459:  Train loss: 3.1446421146392822  Test loss: 4.451218605041504 \n",
      "Epoch: 2/10:  mini-batch 152/4459:  Train loss: 3.5181243419647217  Test loss: 4.434013366699219 \n",
      "Epoch: 2/10:  mini-batch 153/4459:  Train loss: 3.35538911819458  Test loss: 4.403639793395996 \n",
      "Epoch: 2/10:  mini-batch 154/4459:  Train loss: 2.9057960510253906  Test loss: 4.374459266662598 \n",
      "Epoch: 2/10:  mini-batch 155/4459:  Train loss: 2.856809377670288  Test loss: 4.348416805267334 \n",
      "Epoch: 2/10:  mini-batch 156/4459:  Train loss: 2.6636767387390137  Test loss: 4.33106803894043 \n",
      "Epoch: 2/10:  mini-batch 157/4459:  Train loss: 2.6188998222351074  Test loss: 4.324519634246826 \n",
      "Epoch: 2/10:  mini-batch 158/4459:  Train loss: 2.9162259101867676  Test loss: 4.315635681152344 \n",
      "Epoch: 2/10:  mini-batch 159/4459:  Train loss: 2.695192337036133  Test loss: 4.313432216644287 \n",
      "Epoch: 2/10:  mini-batch 160/4459:  Train loss: 3.753286361694336  Test loss: 4.293863296508789 \n",
      "Epoch: 2/10:  mini-batch 161/4459:  Train loss: 2.422377109527588  Test loss: 4.280142784118652 \n",
      "Epoch: 2/10:  mini-batch 162/4459:  Train loss: 2.5031027793884277  Test loss: 4.268589496612549 \n",
      "Epoch: 2/10:  mini-batch 163/4459:  Train loss: 2.513864517211914  Test loss: 4.25862455368042 \n",
      "Epoch: 2/10:  mini-batch 164/4459:  Train loss: 2.724303722381592  Test loss: 4.253686904907227 \n",
      "Epoch: 2/10:  mini-batch 165/4459:  Train loss: 2.5036749839782715  Test loss: 4.252305507659912 \n",
      "Epoch: 2/10:  mini-batch 166/4459:  Train loss: 2.7038586139678955  Test loss: 4.252442359924316 \n",
      "Epoch: 2/10:  mini-batch 167/4459:  Train loss: 2.4313673973083496  Test loss: 4.258749485015869 \n",
      "Epoch: 2/10:  mini-batch 168/4459:  Train loss: 2.6014349460601807  Test loss: 4.267508029937744 \n",
      "Epoch: 2/10:  mini-batch 169/4459:  Train loss: 2.4569549560546875  Test loss: 4.278256893157959 \n",
      "Epoch: 2/10:  mini-batch 170/4459:  Train loss: 2.6132044792175293  Test loss: 4.291930675506592 \n",
      "Epoch: 2/10:  mini-batch 171/4459:  Train loss: 2.6174025535583496  Test loss: 4.309789180755615 \n",
      "Epoch: 2/10:  mini-batch 172/4459:  Train loss: 2.5302155017852783  Test loss: 4.33193826675415 \n",
      "Epoch: 2/10:  mini-batch 173/4459:  Train loss: 2.4772462844848633  Test loss: 4.359127521514893 \n",
      "Epoch: 2/10:  mini-batch 174/4459:  Train loss: 2.496112823486328  Test loss: 4.390509605407715 \n",
      "Epoch: 2/10:  mini-batch 175/4459:  Train loss: 2.502295732498169  Test loss: 4.425637245178223 \n",
      "Epoch: 2/10:  mini-batch 176/4459:  Train loss: 2.5058913230895996  Test loss: 4.46182107925415 \n",
      "Epoch: 2/10:  mini-batch 177/4459:  Train loss: 2.494751214981079  Test loss: 4.499323844909668 \n",
      "Epoch: 2/10:  mini-batch 178/4459:  Train loss: 2.652428150177002  Test loss: 4.533054828643799 \n",
      "Epoch: 2/10:  mini-batch 179/4459:  Train loss: 2.441789150238037  Test loss: 4.5680999755859375 \n",
      "Epoch: 2/10:  mini-batch 180/4459:  Train loss: 2.691037654876709  Test loss: 4.600886344909668 \n",
      "Epoch: 2/10:  mini-batch 181/4459:  Train loss: 2.518247365951538  Test loss: 4.631458282470703 \n",
      "Epoch: 2/10:  mini-batch 182/4459:  Train loss: 2.4997730255126953  Test loss: 4.660545825958252 \n",
      "Epoch: 2/10:  mini-batch 183/4459:  Train loss: 2.3723907470703125  Test loss: 4.695222854614258 \n",
      "Epoch: 2/10:  mini-batch 184/4459:  Train loss: 2.480572462081909  Test loss: 4.73012638092041 \n",
      "Epoch: 2/10:  mini-batch 185/4459:  Train loss: 2.6399402618408203  Test loss: 4.758970737457275 \n",
      "Epoch: 2/10:  mini-batch 186/4459:  Train loss: 2.7900550365448  Test loss: 4.766115665435791 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 187/4459:  Train loss: 3.0832087993621826  Test loss: 4.748068332672119 \n",
      "Epoch: 2/10:  mini-batch 188/4459:  Train loss: 3.251262903213501  Test loss: 4.704370498657227 \n",
      "Epoch: 2/10:  mini-batch 189/4459:  Train loss: 2.4806156158447266  Test loss: 4.664562225341797 \n",
      "Epoch: 2/10:  mini-batch 190/4459:  Train loss: 2.947798490524292  Test loss: 4.617734909057617 \n",
      "Epoch: 2/10:  mini-batch 191/4459:  Train loss: 2.8109235763549805  Test loss: 4.568702697753906 \n",
      "Epoch: 2/10:  mini-batch 192/4459:  Train loss: 4.116987705230713  Test loss: 4.483908653259277 \n",
      "Epoch: 2/10:  mini-batch 193/4459:  Train loss: 3.072822093963623  Test loss: 4.403573036193848 \n",
      "Epoch: 2/10:  mini-batch 194/4459:  Train loss: 2.3707783222198486  Test loss: 4.344349384307861 \n",
      "Epoch: 2/10:  mini-batch 195/4459:  Train loss: 3.1126890182495117  Test loss: 4.287747859954834 \n",
      "Epoch: 2/10:  mini-batch 196/4459:  Train loss: 3.8744587898254395  Test loss: 4.222136974334717 \n",
      "Epoch: 2/10:  mini-batch 197/4459:  Train loss: 3.2553162574768066  Test loss: 4.158127784729004 \n",
      "Epoch: 2/10:  mini-batch 198/4459:  Train loss: 4.228105545043945  Test loss: 4.0857462882995605 \n",
      "Epoch: 2/10:  mini-batch 199/4459:  Train loss: 3.3355095386505127  Test loss: 4.018192768096924 \n",
      "Epoch: 2/10:  mini-batch 200/4459:  Train loss: 2.637977123260498  Test loss: 3.9678595066070557 \n",
      "Epoch: 2/10:  mini-batch 201/4459:  Train loss: 2.7859578132629395  Test loss: 3.9297940731048584 \n",
      "Epoch: 2/10:  mini-batch 202/4459:  Train loss: 2.691368818283081  Test loss: 3.9063854217529297 \n",
      "Epoch: 2/10:  mini-batch 203/4459:  Train loss: 2.6787593364715576  Test loss: 3.895402431488037 \n",
      "Epoch: 2/10:  mini-batch 204/4459:  Train loss: 2.6557576656341553  Test loss: 3.8940956592559814 \n",
      "Epoch: 2/10:  mini-batch 205/4459:  Train loss: 3.0562987327575684  Test loss: 3.893942356109619 \n",
      "Epoch: 2/10:  mini-batch 206/4459:  Train loss: 3.888715982437134  Test loss: 3.880828380584717 \n",
      "Epoch: 2/10:  mini-batch 207/4459:  Train loss: 3.964290142059326  Test loss: 3.857044219970703 \n",
      "Epoch: 2/10:  mini-batch 208/4459:  Train loss: 3.8003809452056885  Test loss: 3.826042652130127 \n",
      "Epoch: 2/10:  mini-batch 209/4459:  Train loss: 3.77852725982666  Test loss: 3.7888894081115723 \n",
      "Epoch: 2/10:  mini-batch 210/4459:  Train loss: 3.241546869277954  Test loss: 3.7526965141296387 \n",
      "Epoch: 2/10:  mini-batch 211/4459:  Train loss: 3.0567407608032227  Test loss: 3.7317848205566406 \n",
      "Epoch: 2/10:  mini-batch 212/4459:  Train loss: 3.4849772453308105  Test loss: 3.708622455596924 \n",
      "Epoch: 2/10:  mini-batch 213/4459:  Train loss: 3.5845606327056885  Test loss: 3.6807572841644287 \n",
      "Epoch: 2/10:  mini-batch 214/4459:  Train loss: 3.2958948612213135  Test loss: 3.664276599884033 \n",
      "Epoch: 2/10:  mini-batch 215/4459:  Train loss: 3.4652552604675293  Test loss: 3.648453950881958 \n",
      "Epoch: 2/10:  mini-batch 216/4459:  Train loss: 3.417360305786133  Test loss: 3.6404948234558105 \n",
      "Epoch: 2/10:  mini-batch 217/4459:  Train loss: 3.3207850456237793  Test loss: 3.645573616027832 \n",
      "Epoch: 2/10:  mini-batch 218/4459:  Train loss: 3.0951433181762695  Test loss: 3.661562442779541 \n",
      "Epoch: 2/10:  mini-batch 219/4459:  Train loss: 3.3126564025878906  Test loss: 3.6768875122070312 \n",
      "Epoch: 2/10:  mini-batch 220/4459:  Train loss: 3.3449110984802246  Test loss: 3.693264961242676 \n",
      "Epoch: 2/10:  mini-batch 221/4459:  Train loss: 3.3829777240753174  Test loss: 3.709690809249878 \n",
      "Epoch: 2/10:  mini-batch 222/4459:  Train loss: 3.4394471645355225  Test loss: 3.7249927520751953 \n",
      "Epoch: 2/10:  mini-batch 223/4459:  Train loss: 3.108135461807251  Test loss: 3.7458109855651855 \n",
      "Epoch: 2/10:  mini-batch 224/4459:  Train loss: 2.9113168716430664  Test loss: 3.7745649814605713 \n",
      "Epoch: 2/10:  mini-batch 225/4459:  Train loss: 3.0362672805786133  Test loss: 3.802119255065918 \n",
      "Epoch: 2/10:  mini-batch 226/4459:  Train loss: 2.892584800720215  Test loss: 3.835724353790283 \n",
      "Epoch: 2/10:  mini-batch 227/4459:  Train loss: 2.7758312225341797  Test loss: 3.876436948776245 \n",
      "Epoch: 2/10:  mini-batch 228/4459:  Train loss: 2.7798495292663574  Test loss: 3.9209775924682617 \n",
      "Epoch: 2/10:  mini-batch 229/4459:  Train loss: 2.9488449096679688  Test loss: 3.960702896118164 \n",
      "Epoch: 2/10:  mini-batch 230/4459:  Train loss: 2.710937976837158  Test loss: 4.006348133087158 \n",
      "Epoch: 2/10:  mini-batch 231/4459:  Train loss: 2.6954245567321777  Test loss: 4.0570878982543945 \n",
      "Epoch: 2/10:  mini-batch 232/4459:  Train loss: 2.761277198791504  Test loss: 4.107910633087158 \n",
      "Epoch: 2/10:  mini-batch 233/4459:  Train loss: 2.803008794784546  Test loss: 4.161032199859619 \n",
      "Epoch: 2/10:  mini-batch 234/4459:  Train loss: 3.658834457397461  Test loss: 4.193171501159668 \n",
      "Epoch: 2/10:  mini-batch 235/4459:  Train loss: 3.815840244293213  Test loss: 4.202818393707275 \n",
      "Epoch: 2/10:  mini-batch 236/4459:  Train loss: 3.9448890686035156  Test loss: 4.181613445281982 \n",
      "Epoch: 2/10:  mini-batch 237/4459:  Train loss: 3.889493227005005  Test loss: 4.141998767852783 \n",
      "Epoch: 2/10:  mini-batch 238/4459:  Train loss: 4.017311096191406  Test loss: 4.087691783905029 \n",
      "Epoch: 2/10:  mini-batch 239/4459:  Train loss: 4.138644218444824  Test loss: 4.028075695037842 \n",
      "Epoch: 2/10:  mini-batch 240/4459:  Train loss: 4.200214385986328  Test loss: 3.9658501148223877 \n",
      "Epoch: 2/10:  mini-batch 241/4459:  Train loss: 3.829500913619995  Test loss: 3.9115166664123535 \n",
      "Epoch: 2/10:  mini-batch 242/4459:  Train loss: 3.8867032527923584  Test loss: 3.862943649291992 \n",
      "Epoch: 2/10:  mini-batch 243/4459:  Train loss: 3.9445433616638184  Test loss: 3.8196864128112793 \n",
      "Epoch: 2/10:  mini-batch 244/4459:  Train loss: 3.9506423473358154  Test loss: 3.7799148559570312 \n",
      "Epoch: 2/10:  mini-batch 245/4459:  Train loss: 3.795182943344116  Test loss: 3.743459939956665 \n",
      "Epoch: 2/10:  mini-batch 246/4459:  Train loss: 3.6706137657165527  Test loss: 3.7109551429748535 \n",
      "Epoch: 2/10:  mini-batch 247/4459:  Train loss: 3.646282434463501  Test loss: 3.6824393272399902 \n",
      "Epoch: 2/10:  mini-batch 248/4459:  Train loss: 3.5121898651123047  Test loss: 3.6574668884277344 \n",
      "Epoch: 2/10:  mini-batch 249/4459:  Train loss: 3.405219793319702  Test loss: 3.6369986534118652 \n",
      "Epoch: 2/10:  mini-batch 250/4459:  Train loss: 3.644723892211914  Test loss: 3.6187477111816406 \n",
      "Epoch: 2/10:  mini-batch 251/4459:  Train loss: 3.626044273376465  Test loss: 3.6031899452209473 \n",
      "Epoch: 2/10:  mini-batch 252/4459:  Train loss: 3.430506706237793  Test loss: 3.5898561477661133 \n",
      "Epoch: 2/10:  mini-batch 253/4459:  Train loss: 3.7186174392700195  Test loss: 3.577322006225586 \n",
      "Epoch: 2/10:  mini-batch 254/4459:  Train loss: 3.7149198055267334  Test loss: 3.5662662982940674 \n",
      "Epoch: 2/10:  mini-batch 255/4459:  Train loss: 3.3037524223327637  Test loss: 3.5573534965515137 \n",
      "Epoch: 2/10:  mini-batch 256/4459:  Train loss: 3.4582626819610596  Test loss: 3.54939603805542 \n",
      "Epoch: 2/10:  mini-batch 257/4459:  Train loss: 3.743529796600342  Test loss: 3.541896343231201 \n",
      "Epoch: 2/10:  mini-batch 258/4459:  Train loss: 3.6892452239990234  Test loss: 3.535264492034912 \n",
      "Epoch: 2/10:  mini-batch 259/4459:  Train loss: 3.4554824829101562  Test loss: 3.5291991233825684 \n",
      "Epoch: 2/10:  mini-batch 260/4459:  Train loss: 3.5365209579467773  Test loss: 3.5252084732055664 \n",
      "Epoch: 2/10:  mini-batch 261/4459:  Train loss: 3.3187649250030518  Test loss: 3.521300792694092 \n",
      "Epoch: 2/10:  mini-batch 262/4459:  Train loss: 3.6177334785461426  Test loss: 3.5170235633850098 \n",
      "Epoch: 2/10:  mini-batch 263/4459:  Train loss: 3.5991413593292236  Test loss: 3.5135064125061035 \n",
      "Epoch: 2/10:  mini-batch 264/4459:  Train loss: 3.5307631492614746  Test loss: 3.5100245475769043 \n",
      "Epoch: 2/10:  mini-batch 265/4459:  Train loss: 3.4957382678985596  Test loss: 3.507000684738159 \n",
      "Epoch: 2/10:  mini-batch 266/4459:  Train loss: 3.4625654220581055  Test loss: 3.5036635398864746 \n",
      "Epoch: 2/10:  mini-batch 267/4459:  Train loss: 3.314331293106079  Test loss: 3.499969482421875 \n",
      "Epoch: 2/10:  mini-batch 268/4459:  Train loss: 3.56156063079834  Test loss: 3.496046543121338 \n",
      "Epoch: 2/10:  mini-batch 269/4459:  Train loss: 3.573209285736084  Test loss: 3.492790460586548 \n",
      "Epoch: 2/10:  mini-batch 270/4459:  Train loss: 3.556321859359741  Test loss: 3.490161418914795 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 271/4459:  Train loss: 3.423579216003418  Test loss: 3.4879918098449707 \n",
      "Epoch: 2/10:  mini-batch 272/4459:  Train loss: 3.494840621948242  Test loss: 3.485116958618164 \n",
      "Epoch: 2/10:  mini-batch 273/4459:  Train loss: 3.47149658203125  Test loss: 3.4820988178253174 \n",
      "Epoch: 2/10:  mini-batch 274/4459:  Train loss: 3.28891658782959  Test loss: 3.478989362716675 \n",
      "Epoch: 2/10:  mini-batch 275/4459:  Train loss: 3.497194766998291  Test loss: 3.4753313064575195 \n",
      "Epoch: 2/10:  mini-batch 276/4459:  Train loss: 3.5473265647888184  Test loss: 3.472029685974121 \n",
      "Epoch: 2/10:  mini-batch 277/4459:  Train loss: 3.522822856903076  Test loss: 3.4688639640808105 \n",
      "Epoch: 2/10:  mini-batch 278/4459:  Train loss: 3.4548168182373047  Test loss: 3.4657084941864014 \n",
      "Epoch: 2/10:  mini-batch 279/4459:  Train loss: 3.5586276054382324  Test loss: 3.462317943572998 \n",
      "Epoch: 2/10:  mini-batch 280/4459:  Train loss: 3.4307005405426025  Test loss: 3.4579977989196777 \n",
      "Epoch: 2/10:  mini-batch 281/4459:  Train loss: 3.452484607696533  Test loss: 3.4525527954101562 \n",
      "Epoch: 2/10:  mini-batch 282/4459:  Train loss: 3.663351058959961  Test loss: 3.447902202606201 \n",
      "Epoch: 2/10:  mini-batch 283/4459:  Train loss: 3.382072687149048  Test loss: 3.4424517154693604 \n",
      "Epoch: 2/10:  mini-batch 284/4459:  Train loss: 3.5178656578063965  Test loss: 3.4370579719543457 \n",
      "Epoch: 2/10:  mini-batch 285/4459:  Train loss: 3.554093360900879  Test loss: 3.4321746826171875 \n",
      "Epoch: 2/10:  mini-batch 286/4459:  Train loss: 3.479759454727173  Test loss: 3.427542209625244 \n",
      "Epoch: 2/10:  mini-batch 287/4459:  Train loss: 3.4807798862457275  Test loss: 3.4238028526306152 \n",
      "Epoch: 2/10:  mini-batch 288/4459:  Train loss: 3.4080498218536377  Test loss: 3.4203758239746094 \n",
      "Epoch: 2/10:  mini-batch 289/4459:  Train loss: 3.4256038665771484  Test loss: 3.417494297027588 \n",
      "Epoch: 2/10:  mini-batch 290/4459:  Train loss: 3.205902099609375  Test loss: 3.413506507873535 \n",
      "Epoch: 2/10:  mini-batch 291/4459:  Train loss: 3.333671808242798  Test loss: 3.408806800842285 \n",
      "Epoch: 2/10:  mini-batch 292/4459:  Train loss: 3.3479065895080566  Test loss: 3.405085802078247 \n",
      "Epoch: 2/10:  mini-batch 293/4459:  Train loss: 3.5209717750549316  Test loss: 3.4016342163085938 \n",
      "Epoch: 2/10:  mini-batch 294/4459:  Train loss: 3.4339780807495117  Test loss: 3.3979644775390625 \n",
      "Epoch: 2/10:  mini-batch 295/4459:  Train loss: 3.4905848503112793  Test loss: 3.3950443267822266 \n",
      "Epoch: 2/10:  mini-batch 296/4459:  Train loss: 3.3438613414764404  Test loss: 3.3910179138183594 \n",
      "Epoch: 2/10:  mini-batch 297/4459:  Train loss: 3.509763240814209  Test loss: 3.3877601623535156 \n",
      "Epoch: 2/10:  mini-batch 298/4459:  Train loss: 3.371537923812866  Test loss: 3.384190082550049 \n",
      "Epoch: 2/10:  mini-batch 299/4459:  Train loss: 3.3026158809661865  Test loss: 3.3803579807281494 \n",
      "Epoch: 2/10:  mini-batch 300/4459:  Train loss: 3.493518829345703  Test loss: 3.3768374919891357 \n",
      "Epoch: 2/10:  mini-batch 301/4459:  Train loss: 3.438180685043335  Test loss: 3.3727169036865234 \n",
      "Epoch: 2/10:  mini-batch 302/4459:  Train loss: 3.4148850440979004  Test loss: 3.369845390319824 \n",
      "Epoch: 2/10:  mini-batch 303/4459:  Train loss: 3.453615427017212  Test loss: 3.366579532623291 \n",
      "Epoch: 2/10:  mini-batch 304/4459:  Train loss: 3.2040231227874756  Test loss: 3.3630127906799316 \n",
      "Epoch: 2/10:  mini-batch 305/4459:  Train loss: 3.3781673908233643  Test loss: 3.3594698905944824 \n",
      "Epoch: 2/10:  mini-batch 306/4459:  Train loss: 3.4990642070770264  Test loss: 3.3570756912231445 \n",
      "Epoch: 2/10:  mini-batch 307/4459:  Train loss: 3.743957996368408  Test loss: 3.356642961502075 \n",
      "Epoch: 2/10:  mini-batch 308/4459:  Train loss: 3.433190107345581  Test loss: 3.3574929237365723 \n",
      "Epoch: 2/10:  mini-batch 309/4459:  Train loss: 3.558321237564087  Test loss: 3.3593411445617676 \n",
      "Epoch: 2/10:  mini-batch 310/4459:  Train loss: 3.1188907623291016  Test loss: 3.360665798187256 \n",
      "Epoch: 2/10:  mini-batch 311/4459:  Train loss: 3.634587049484253  Test loss: 3.3633511066436768 \n",
      "Epoch: 2/10:  mini-batch 312/4459:  Train loss: 3.3320765495300293  Test loss: 3.3652288913726807 \n",
      "Epoch: 2/10:  mini-batch 313/4459:  Train loss: 3.3237311840057373  Test loss: 3.367762565612793 \n",
      "Epoch: 2/10:  mini-batch 314/4459:  Train loss: 3.474231719970703  Test loss: 3.370694160461426 \n",
      "Epoch: 2/10:  mini-batch 315/4459:  Train loss: 3.4464282989501953  Test loss: 3.373255729675293 \n",
      "Epoch: 2/10:  mini-batch 316/4459:  Train loss: 3.4874415397644043  Test loss: 3.3758232593536377 \n",
      "Epoch: 2/10:  mini-batch 317/4459:  Train loss: 3.3359854221343994  Test loss: 3.3784070014953613 \n",
      "Epoch: 2/10:  mini-batch 318/4459:  Train loss: 3.6905517578125  Test loss: 3.3820338249206543 \n",
      "Epoch: 2/10:  mini-batch 319/4459:  Train loss: 3.2645797729492188  Test loss: 3.3849411010742188 \n",
      "Epoch: 2/10:  mini-batch 320/4459:  Train loss: 3.3557825088500977  Test loss: 3.3883137702941895 \n",
      "Epoch: 2/10:  mini-batch 321/4459:  Train loss: 3.3368325233459473  Test loss: 3.3907268047332764 \n",
      "Epoch: 2/10:  mini-batch 322/4459:  Train loss: 3.363297939300537  Test loss: 3.393373966217041 \n",
      "Epoch: 2/10:  mini-batch 323/4459:  Train loss: 3.3710761070251465  Test loss: 3.395461082458496 \n",
      "Epoch: 2/10:  mini-batch 324/4459:  Train loss: 3.3659467697143555  Test loss: 3.3967018127441406 \n",
      "Epoch: 2/10:  mini-batch 325/4459:  Train loss: 3.143747091293335  Test loss: 3.3968162536621094 \n",
      "Epoch: 2/10:  mini-batch 326/4459:  Train loss: 3.5599775314331055  Test loss: 3.3969266414642334 \n",
      "Epoch: 2/10:  mini-batch 327/4459:  Train loss: 3.31674861907959  Test loss: 3.3961915969848633 \n",
      "Epoch: 2/10:  mini-batch 328/4459:  Train loss: 3.22017765045166  Test loss: 3.394808292388916 \n",
      "Epoch: 2/10:  mini-batch 329/4459:  Train loss: 3.1076676845550537  Test loss: 3.3937671184539795 \n",
      "Epoch: 2/10:  mini-batch 330/4459:  Train loss: 3.3741722106933594  Test loss: 3.3926610946655273 \n",
      "Epoch: 2/10:  mini-batch 331/4459:  Train loss: 3.177159070968628  Test loss: 3.3915443420410156 \n",
      "Epoch: 2/10:  mini-batch 332/4459:  Train loss: 3.0197479724884033  Test loss: 3.3913159370422363 \n",
      "Epoch: 2/10:  mini-batch 333/4459:  Train loss: 3.079069137573242  Test loss: 3.3911221027374268 \n",
      "Epoch: 2/10:  mini-batch 334/4459:  Train loss: 3.1614763736724854  Test loss: 3.3912084102630615 \n",
      "Epoch: 2/10:  mini-batch 335/4459:  Train loss: 3.314847469329834  Test loss: 3.3916845321655273 \n",
      "Epoch: 2/10:  mini-batch 336/4459:  Train loss: 3.3361401557922363  Test loss: 3.39308500289917 \n",
      "Epoch: 2/10:  mini-batch 337/4459:  Train loss: 3.142214298248291  Test loss: 3.3957748413085938 \n",
      "Epoch: 2/10:  mini-batch 338/4459:  Train loss: 3.441772699356079  Test loss: 3.3961262702941895 \n",
      "Epoch: 2/10:  mini-batch 339/4459:  Train loss: 3.0530080795288086  Test loss: 3.399291515350342 \n",
      "Epoch: 2/10:  mini-batch 340/4459:  Train loss: 3.538492202758789  Test loss: 3.397733449935913 \n",
      "Epoch: 2/10:  mini-batch 341/4459:  Train loss: 3.0686087608337402  Test loss: 3.396674633026123 \n",
      "Epoch: 2/10:  mini-batch 342/4459:  Train loss: 3.326291561126709  Test loss: 3.3957202434539795 \n",
      "Epoch: 2/10:  mini-batch 343/4459:  Train loss: 3.5331385135650635  Test loss: 3.391691207885742 \n",
      "Epoch: 2/10:  mini-batch 344/4459:  Train loss: 3.8033554553985596  Test loss: 3.384364604949951 \n",
      "Epoch: 2/10:  mini-batch 345/4459:  Train loss: 2.951732635498047  Test loss: 3.3804802894592285 \n",
      "Epoch: 2/10:  mini-batch 346/4459:  Train loss: 2.780992269515991  Test loss: 3.3796310424804688 \n",
      "Epoch: 2/10:  mini-batch 347/4459:  Train loss: 3.3470897674560547  Test loss: 3.3756611347198486 \n",
      "Epoch: 2/10:  mini-batch 348/4459:  Train loss: 3.3537826538085938  Test loss: 3.3721108436584473 \n",
      "Epoch: 2/10:  mini-batch 349/4459:  Train loss: 3.4157962799072266  Test loss: 3.3686819076538086 \n",
      "Epoch: 2/10:  mini-batch 350/4459:  Train loss: 3.2888426780700684  Test loss: 3.364229202270508 \n",
      "Epoch: 2/10:  mini-batch 351/4459:  Train loss: 3.809593677520752  Test loss: 3.357879638671875 \n",
      "Epoch: 2/10:  mini-batch 352/4459:  Train loss: 3.377676248550415  Test loss: 3.3539061546325684 \n",
      "Epoch: 2/10:  mini-batch 353/4459:  Train loss: 3.336275100708008  Test loss: 3.3507838249206543 \n",
      "Epoch: 2/10:  mini-batch 354/4459:  Train loss: 3.044774055480957  Test loss: 3.3482370376586914 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 355/4459:  Train loss: 3.17350435256958  Test loss: 3.346423625946045 \n",
      "Epoch: 2/10:  mini-batch 356/4459:  Train loss: 3.6042604446411133  Test loss: 3.344058036804199 \n",
      "Epoch: 2/10:  mini-batch 357/4459:  Train loss: 3.3427579402923584  Test loss: 3.341797351837158 \n",
      "Epoch: 2/10:  mini-batch 358/4459:  Train loss: 3.323948621749878  Test loss: 3.338801622390747 \n",
      "Epoch: 2/10:  mini-batch 359/4459:  Train loss: 3.5410752296447754  Test loss: 3.336899995803833 \n",
      "Epoch: 2/10:  mini-batch 360/4459:  Train loss: 3.44936466217041  Test loss: 3.3352389335632324 \n",
      "Epoch: 2/10:  mini-batch 361/4459:  Train loss: 3.3698577880859375  Test loss: 3.333042621612549 \n",
      "Epoch: 2/10:  mini-batch 362/4459:  Train loss: 3.7031617164611816  Test loss: 3.3315320014953613 \n",
      "Epoch: 2/10:  mini-batch 363/4459:  Train loss: 3.297891616821289  Test loss: 3.332037925720215 \n",
      "Epoch: 2/10:  mini-batch 364/4459:  Train loss: 3.861950159072876  Test loss: 3.3341848850250244 \n",
      "Epoch: 2/10:  mini-batch 365/4459:  Train loss: 3.116665840148926  Test loss: 3.336933135986328 \n",
      "Epoch: 2/10:  mini-batch 366/4459:  Train loss: 3.5162405967712402  Test loss: 3.33982515335083 \n",
      "Epoch: 2/10:  mini-batch 367/4459:  Train loss: 3.3581550121307373  Test loss: 3.3417065143585205 \n",
      "Epoch: 2/10:  mini-batch 368/4459:  Train loss: 3.27433180809021  Test loss: 3.3426640033721924 \n",
      "Epoch: 2/10:  mini-batch 369/4459:  Train loss: 3.3188703060150146  Test loss: 3.3436262607574463 \n",
      "Epoch: 2/10:  mini-batch 370/4459:  Train loss: 3.3780670166015625  Test loss: 3.344449043273926 \n",
      "Epoch: 2/10:  mini-batch 371/4459:  Train loss: 3.549496650695801  Test loss: 3.345700979232788 \n",
      "Epoch: 2/10:  mini-batch 372/4459:  Train loss: 3.3382625579833984  Test loss: 3.346447467803955 \n",
      "Epoch: 2/10:  mini-batch 373/4459:  Train loss: 3.273653268814087  Test loss: 3.34684419631958 \n",
      "Epoch: 2/10:  mini-batch 374/4459:  Train loss: 3.375974655151367  Test loss: 3.347280979156494 \n",
      "Epoch: 2/10:  mini-batch 375/4459:  Train loss: 3.139091968536377  Test loss: 3.3479344844818115 \n",
      "Epoch: 2/10:  mini-batch 376/4459:  Train loss: 3.341618776321411  Test loss: 3.347775459289551 \n",
      "Epoch: 2/10:  mini-batch 377/4459:  Train loss: 3.1410818099975586  Test loss: 3.346920967102051 \n",
      "Epoch: 2/10:  mini-batch 378/4459:  Train loss: 3.1469178199768066  Test loss: 3.3460562229156494 \n",
      "Epoch: 2/10:  mini-batch 379/4459:  Train loss: 3.214815855026245  Test loss: 3.3453898429870605 \n",
      "Epoch: 2/10:  mini-batch 380/4459:  Train loss: 3.421456813812256  Test loss: 3.344942331314087 \n",
      "Epoch: 2/10:  mini-batch 381/4459:  Train loss: 3.3284072875976562  Test loss: 3.344526529312134 \n",
      "Epoch: 2/10:  mini-batch 382/4459:  Train loss: 3.318260669708252  Test loss: 3.34407901763916 \n",
      "Epoch: 2/10:  mini-batch 383/4459:  Train loss: 3.3892927169799805  Test loss: 3.3431355953216553 \n",
      "Epoch: 2/10:  mini-batch 384/4459:  Train loss: 3.7778074741363525  Test loss: 3.3428397178649902 \n",
      "Epoch: 2/10:  mini-batch 385/4459:  Train loss: 3.403019905090332  Test loss: 3.3421225547790527 \n",
      "Epoch: 2/10:  mini-batch 386/4459:  Train loss: 3.334731340408325  Test loss: 3.341578960418701 \n",
      "Epoch: 2/10:  mini-batch 387/4459:  Train loss: 3.6624293327331543  Test loss: 3.3415088653564453 \n",
      "Epoch: 2/10:  mini-batch 388/4459:  Train loss: 2.9264893531799316  Test loss: 3.3417351245880127 \n",
      "Epoch: 2/10:  mini-batch 389/4459:  Train loss: 3.5755531787872314  Test loss: 3.342411518096924 \n",
      "Epoch: 2/10:  mini-batch 390/4459:  Train loss: 3.519874095916748  Test loss: 3.343069314956665 \n",
      "Epoch: 2/10:  mini-batch 391/4459:  Train loss: 3.158853530883789  Test loss: 3.343437910079956 \n",
      "Epoch: 2/10:  mini-batch 392/4459:  Train loss: 3.0711045265197754  Test loss: 3.3437752723693848 \n",
      "Epoch: 2/10:  mini-batch 393/4459:  Train loss: 3.384753704071045  Test loss: 3.343991279602051 \n",
      "Epoch: 2/10:  mini-batch 394/4459:  Train loss: 3.11045503616333  Test loss: 3.3442952632904053 \n",
      "Epoch: 2/10:  mini-batch 395/4459:  Train loss: 2.902888774871826  Test loss: 3.344744920730591 \n",
      "Epoch: 2/10:  mini-batch 396/4459:  Train loss: 3.517855405807495  Test loss: 3.344874382019043 \n",
      "Epoch: 2/10:  mini-batch 397/4459:  Train loss: 3.2427921295166016  Test loss: 3.345496892929077 \n",
      "Epoch: 2/10:  mini-batch 398/4459:  Train loss: 3.4781036376953125  Test loss: 3.346266746520996 \n",
      "Epoch: 2/10:  mini-batch 399/4459:  Train loss: 3.710824489593506  Test loss: 3.3475630283355713 \n",
      "Epoch: 2/10:  mini-batch 400/4459:  Train loss: 3.393507480621338  Test loss: 3.3483400344848633 \n",
      "Epoch: 2/10:  mini-batch 401/4459:  Train loss: 3.469322919845581  Test loss: 3.3495635986328125 \n",
      "Epoch: 2/10:  mini-batch 402/4459:  Train loss: 3.316246271133423  Test loss: 3.3503975868225098 \n",
      "Epoch: 2/10:  mini-batch 403/4459:  Train loss: 3.1447360515594482  Test loss: 3.350800037384033 \n",
      "Epoch: 2/10:  mini-batch 404/4459:  Train loss: 3.3870415687561035  Test loss: 3.3507132530212402 \n",
      "Epoch: 2/10:  mini-batch 405/4459:  Train loss: 3.0790276527404785  Test loss: 3.351001739501953 \n",
      "Epoch: 2/10:  mini-batch 406/4459:  Train loss: 3.4314613342285156  Test loss: 3.351250648498535 \n",
      "Epoch: 2/10:  mini-batch 407/4459:  Train loss: 3.5324764251708984  Test loss: 3.3512048721313477 \n",
      "Epoch: 2/10:  mini-batch 408/4459:  Train loss: 3.296271800994873  Test loss: 3.35070538520813 \n",
      "Epoch: 2/10:  mini-batch 409/4459:  Train loss: 3.040386199951172  Test loss: 3.350625514984131 \n",
      "Epoch: 2/10:  mini-batch 410/4459:  Train loss: 3.10667085647583  Test loss: 3.3509726524353027 \n",
      "Epoch: 2/10:  mini-batch 411/4459:  Train loss: 2.842214584350586  Test loss: 3.3520667552948 \n",
      "Epoch: 2/10:  mini-batch 412/4459:  Train loss: 3.309116840362549  Test loss: 3.3532848358154297 \n",
      "Epoch: 2/10:  mini-batch 413/4459:  Train loss: 3.739198684692383  Test loss: 3.3544957637786865 \n",
      "Epoch: 2/10:  mini-batch 414/4459:  Train loss: 3.6777851581573486  Test loss: 3.355560302734375 \n",
      "Epoch: 2/10:  mini-batch 415/4459:  Train loss: 3.589097023010254  Test loss: 3.3567652702331543 \n",
      "Epoch: 2/10:  mini-batch 416/4459:  Train loss: 3.802617311477661  Test loss: 3.3574841022491455 \n",
      "Epoch: 2/10:  mini-batch 417/4459:  Train loss: 3.3859806060791016  Test loss: 3.3573691844940186 \n",
      "Epoch: 2/10:  mini-batch 418/4459:  Train loss: 3.267314910888672  Test loss: 3.357576370239258 \n",
      "Epoch: 2/10:  mini-batch 419/4459:  Train loss: 2.9451184272766113  Test loss: 3.3578481674194336 \n",
      "Epoch: 2/10:  mini-batch 420/4459:  Train loss: 3.3182976245880127  Test loss: 3.3577096462249756 \n",
      "Epoch: 2/10:  mini-batch 421/4459:  Train loss: 3.1679978370666504  Test loss: 3.3571860790252686 \n",
      "Epoch: 2/10:  mini-batch 422/4459:  Train loss: 3.329375982284546  Test loss: 3.3568835258483887 \n",
      "Epoch: 2/10:  mini-batch 423/4459:  Train loss: 3.1681151390075684  Test loss: 3.3568921089172363 \n",
      "Epoch: 2/10:  mini-batch 424/4459:  Train loss: 3.332850456237793  Test loss: 3.356668472290039 \n",
      "Epoch: 2/10:  mini-batch 425/4459:  Train loss: 3.4352476596832275  Test loss: 3.356154680252075 \n",
      "Epoch: 2/10:  mini-batch 426/4459:  Train loss: 3.5390353202819824  Test loss: 3.3555920124053955 \n",
      "Epoch: 2/10:  mini-batch 427/4459:  Train loss: 3.3912577629089355  Test loss: 3.355393409729004 \n",
      "Epoch: 2/10:  mini-batch 428/4459:  Train loss: 3.2628302574157715  Test loss: 3.3552775382995605 \n",
      "Epoch: 2/10:  mini-batch 429/4459:  Train loss: 3.4410605430603027  Test loss: 3.3548543453216553 \n",
      "Epoch: 2/10:  mini-batch 430/4459:  Train loss: 3.0256900787353516  Test loss: 3.354586362838745 \n",
      "Epoch: 2/10:  mini-batch 431/4459:  Train loss: 3.5155110359191895  Test loss: 3.3544933795928955 \n",
      "Epoch: 2/10:  mini-batch 432/4459:  Train loss: 3.5380663871765137  Test loss: 3.3546900749206543 \n",
      "Epoch: 2/10:  mini-batch 433/4459:  Train loss: 3.5129318237304688  Test loss: 3.3558270931243896 \n",
      "Epoch: 2/10:  mini-batch 434/4459:  Train loss: 3.5737991333007812  Test loss: 3.3569729328155518 \n",
      "Epoch: 2/10:  mini-batch 435/4459:  Train loss: 3.3115787506103516  Test loss: 3.357673168182373 \n",
      "Epoch: 2/10:  mini-batch 436/4459:  Train loss: 3.4078307151794434  Test loss: 3.358290433883667 \n",
      "Epoch: 2/10:  mini-batch 437/4459:  Train loss: 3.495436191558838  Test loss: 3.3588576316833496 \n",
      "Epoch: 2/10:  mini-batch 438/4459:  Train loss: 3.4800989627838135  Test loss: 3.3593461513519287 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 439/4459:  Train loss: 3.2934329509735107  Test loss: 3.359682083129883 \n",
      "Epoch: 2/10:  mini-batch 440/4459:  Train loss: 3.472182512283325  Test loss: 3.3604774475097656 \n",
      "Epoch: 2/10:  mini-batch 441/4459:  Train loss: 3.4550936222076416  Test loss: 3.361598253250122 \n",
      "Epoch: 2/10:  mini-batch 442/4459:  Train loss: 3.4080936908721924  Test loss: 3.362778663635254 \n",
      "Epoch: 2/10:  mini-batch 443/4459:  Train loss: 3.151090145111084  Test loss: 3.3640782833099365 \n",
      "Epoch: 2/10:  mini-batch 444/4459:  Train loss: 3.098330497741699  Test loss: 3.365452766418457 \n",
      "Epoch: 2/10:  mini-batch 445/4459:  Train loss: 3.139247179031372  Test loss: 3.366276741027832 \n",
      "Epoch: 2/10:  mini-batch 446/4459:  Train loss: 3.3527402877807617  Test loss: 3.3665826320648193 \n",
      "Epoch: 2/10:  mini-batch 447/4459:  Train loss: 3.4818663597106934  Test loss: 3.3667633533477783 \n",
      "Epoch: 2/10:  mini-batch 448/4459:  Train loss: 3.211876392364502  Test loss: 3.3668856620788574 \n",
      "Epoch: 2/10:  mini-batch 449/4459:  Train loss: 3.1345913410186768  Test loss: 3.366683006286621 \n",
      "Epoch: 2/10:  mini-batch 450/4459:  Train loss: 3.21005916595459  Test loss: 3.366448402404785 \n",
      "Epoch: 2/10:  mini-batch 451/4459:  Train loss: 3.051011085510254  Test loss: 3.3663580417633057 \n",
      "Epoch: 2/10:  mini-batch 452/4459:  Train loss: 3.569591522216797  Test loss: 3.3665404319763184 \n",
      "Epoch: 2/10:  mini-batch 453/4459:  Train loss: 3.240095376968384  Test loss: 3.3667492866516113 \n",
      "Epoch: 2/10:  mini-batch 454/4459:  Train loss: 3.053159713745117  Test loss: 3.3671674728393555 \n",
      "Epoch: 2/10:  mini-batch 455/4459:  Train loss: 3.0998713970184326  Test loss: 3.36792254447937 \n",
      "Epoch: 2/10:  mini-batch 456/4459:  Train loss: 3.3301842212677  Test loss: 3.3686017990112305 \n",
      "Epoch: 2/10:  mini-batch 457/4459:  Train loss: 3.3681023120880127  Test loss: 3.369687080383301 \n",
      "Epoch: 2/10:  mini-batch 458/4459:  Train loss: 3.482818365097046  Test loss: 3.370364189147949 \n",
      "Epoch: 2/10:  mini-batch 459/4459:  Train loss: 3.1396477222442627  Test loss: 3.3715319633483887 \n",
      "Epoch: 2/10:  mini-batch 460/4459:  Train loss: 3.3984596729278564  Test loss: 3.373093366622925 \n",
      "Epoch: 2/10:  mini-batch 461/4459:  Train loss: 3.4247803688049316  Test loss: 3.374675750732422 \n",
      "Epoch: 2/10:  mini-batch 462/4459:  Train loss: 3.406154155731201  Test loss: 3.376077890396118 \n",
      "Epoch: 2/10:  mini-batch 463/4459:  Train loss: 2.979191303253174  Test loss: 3.377399206161499 \n",
      "Epoch: 2/10:  mini-batch 464/4459:  Train loss: 3.1322150230407715  Test loss: 3.3788630962371826 \n",
      "Epoch: 2/10:  mini-batch 465/4459:  Train loss: 3.1621551513671875  Test loss: 3.3804574012756348 \n",
      "Epoch: 2/10:  mini-batch 466/4459:  Train loss: 3.4726052284240723  Test loss: 3.382139205932617 \n",
      "Epoch: 2/10:  mini-batch 467/4459:  Train loss: 3.450366973876953  Test loss: 3.3834056854248047 \n",
      "Epoch: 2/10:  mini-batch 468/4459:  Train loss: 3.289914846420288  Test loss: 3.383838653564453 \n",
      "Epoch: 2/10:  mini-batch 469/4459:  Train loss: 3.074259042739868  Test loss: 3.3845949172973633 \n",
      "Epoch: 2/10:  mini-batch 470/4459:  Train loss: 3.601393699645996  Test loss: 3.3845787048339844 \n",
      "Epoch: 2/10:  mini-batch 471/4459:  Train loss: 3.2829694747924805  Test loss: 3.3841347694396973 \n",
      "Epoch: 2/10:  mini-batch 472/4459:  Train loss: 3.283571243286133  Test loss: 3.3845064640045166 \n",
      "Epoch: 2/10:  mini-batch 473/4459:  Train loss: 3.1099448204040527  Test loss: 3.3847885131835938 \n",
      "Epoch: 2/10:  mini-batch 474/4459:  Train loss: 2.7987263202667236  Test loss: 3.38586688041687 \n",
      "Epoch: 2/10:  mini-batch 475/4459:  Train loss: 2.890953540802002  Test loss: 3.387249708175659 \n",
      "Epoch: 2/10:  mini-batch 476/4459:  Train loss: 3.2090868949890137  Test loss: 3.3883280754089355 \n",
      "Epoch: 2/10:  mini-batch 477/4459:  Train loss: 2.7962043285369873  Test loss: 3.390516519546509 \n",
      "Epoch: 2/10:  mini-batch 478/4459:  Train loss: 3.0543549060821533  Test loss: 3.3934056758880615 \n",
      "Epoch: 2/10:  mini-batch 479/4459:  Train loss: 2.999488353729248  Test loss: 3.3968358039855957 \n",
      "Epoch: 2/10:  mini-batch 480/4459:  Train loss: 3.417994976043701  Test loss: 3.399890422821045 \n",
      "Epoch: 2/10:  mini-batch 481/4459:  Train loss: 3.1103782653808594  Test loss: 3.4029321670532227 \n",
      "Epoch: 2/10:  mini-batch 482/4459:  Train loss: 3.017241954803467  Test loss: 3.4068102836608887 \n",
      "Epoch: 2/10:  mini-batch 483/4459:  Train loss: 3.6727402210235596  Test loss: 3.408313751220703 \n",
      "Epoch: 2/10:  mini-batch 484/4459:  Train loss: 3.1012158393859863  Test loss: 3.409846782684326 \n",
      "Epoch: 2/10:  mini-batch 485/4459:  Train loss: 3.8101301193237305  Test loss: 3.4082422256469727 \n",
      "Epoch: 2/10:  mini-batch 486/4459:  Train loss: 3.57875919342041  Test loss: 3.4045774936676025 \n",
      "Epoch: 2/10:  mini-batch 487/4459:  Train loss: 3.063497304916382  Test loss: 3.4016170501708984 \n",
      "Epoch: 2/10:  mini-batch 488/4459:  Train loss: 3.306705951690674  Test loss: 3.3986129760742188 \n",
      "Epoch: 2/10:  mini-batch 489/4459:  Train loss: 3.3986849784851074  Test loss: 3.3959197998046875 \n",
      "Epoch: 2/10:  mini-batch 490/4459:  Train loss: 3.4850330352783203  Test loss: 3.3928022384643555 \n",
      "Epoch: 2/10:  mini-batch 491/4459:  Train loss: 3.5074667930603027  Test loss: 3.3888611793518066 \n",
      "Epoch: 2/10:  mini-batch 492/4459:  Train loss: 3.6102259159088135  Test loss: 3.384915828704834 \n",
      "Epoch: 2/10:  mini-batch 493/4459:  Train loss: 3.2691307067871094  Test loss: 3.381190299987793 \n",
      "Epoch: 2/10:  mini-batch 494/4459:  Train loss: 3.6435775756835938  Test loss: 3.377359390258789 \n",
      "Epoch: 2/10:  mini-batch 495/4459:  Train loss: 3.377251386642456  Test loss: 3.3740153312683105 \n",
      "Epoch: 2/10:  mini-batch 496/4459:  Train loss: 3.06144118309021  Test loss: 3.3719425201416016 \n",
      "Epoch: 2/10:  mini-batch 497/4459:  Train loss: 3.9089913368225098  Test loss: 3.369798183441162 \n",
      "Epoch: 2/10:  mini-batch 498/4459:  Train loss: 3.193746566772461  Test loss: 3.3683390617370605 \n",
      "Epoch: 2/10:  mini-batch 499/4459:  Train loss: 2.972053050994873  Test loss: 3.3679122924804688 \n",
      "Epoch: 2/10:  mini-batch 500/4459:  Train loss: 3.3077855110168457  Test loss: 3.3679556846618652 \n",
      "Epoch: 2/10:  mini-batch 501/4459:  Train loss: 3.649355888366699  Test loss: 3.367739200592041 \n",
      "Epoch: 2/10:  mini-batch 502/4459:  Train loss: 3.135890007019043  Test loss: 3.3678438663482666 \n",
      "Epoch: 2/10:  mini-batch 503/4459:  Train loss: 3.3917758464813232  Test loss: 3.367842197418213 \n",
      "Epoch: 2/10:  mini-batch 504/4459:  Train loss: 3.5976126194000244  Test loss: 3.368454933166504 \n",
      "Epoch: 2/10:  mini-batch 505/4459:  Train loss: 3.942354202270508  Test loss: 3.368708372116089 \n",
      "Epoch: 2/10:  mini-batch 506/4459:  Train loss: 2.876207113265991  Test loss: 3.369319438934326 \n",
      "Epoch: 2/10:  mini-batch 507/4459:  Train loss: 3.229022264480591  Test loss: 3.370187997817993 \n",
      "Epoch: 2/10:  mini-batch 508/4459:  Train loss: 3.455075979232788  Test loss: 3.3711745738983154 \n",
      "Epoch: 2/10:  mini-batch 509/4459:  Train loss: 3.3060007095336914  Test loss: 3.37198543548584 \n",
      "Epoch: 2/10:  mini-batch 510/4459:  Train loss: 3.5517876148223877  Test loss: 3.372868061065674 \n",
      "Epoch: 2/10:  mini-batch 511/4459:  Train loss: 3.1897528171539307  Test loss: 3.3729746341705322 \n",
      "Epoch: 2/10:  mini-batch 512/4459:  Train loss: 3.1610400676727295  Test loss: 3.3729865550994873 \n",
      "Epoch: 2/10:  mini-batch 513/4459:  Train loss: 3.3703341484069824  Test loss: 3.3727736473083496 \n",
      "Epoch: 2/10:  mini-batch 514/4459:  Train loss: 3.212602376937866  Test loss: 3.3722445964813232 \n",
      "Epoch: 2/10:  mini-batch 515/4459:  Train loss: 2.8659589290618896  Test loss: 3.3718223571777344 \n",
      "Epoch: 2/10:  mini-batch 516/4459:  Train loss: 3.4724533557891846  Test loss: 3.3715507984161377 \n",
      "Epoch: 2/10:  mini-batch 517/4459:  Train loss: 3.166926383972168  Test loss: 3.3712472915649414 \n",
      "Epoch: 2/10:  mini-batch 518/4459:  Train loss: 3.3750104904174805  Test loss: 3.371288299560547 \n",
      "Epoch: 2/10:  mini-batch 519/4459:  Train loss: 3.3145458698272705  Test loss: 3.3714606761932373 \n",
      "Epoch: 2/10:  mini-batch 520/4459:  Train loss: 3.117466449737549  Test loss: 3.3716249465942383 \n",
      "Epoch: 2/10:  mini-batch 521/4459:  Train loss: 3.2054450511932373  Test loss: 3.3720436096191406 \n",
      "Epoch: 2/10:  mini-batch 522/4459:  Train loss: 3.4270055294036865  Test loss: 3.372725009918213 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 523/4459:  Train loss: 3.249335527420044  Test loss: 3.373453140258789 \n",
      "Epoch: 2/10:  mini-batch 524/4459:  Train loss: 3.413661479949951  Test loss: 3.3738176822662354 \n",
      "Epoch: 2/10:  mini-batch 525/4459:  Train loss: 3.4900496006011963  Test loss: 3.373819351196289 \n",
      "Epoch: 2/10:  mini-batch 526/4459:  Train loss: 3.1626222133636475  Test loss: 3.374392032623291 \n",
      "Epoch: 2/10:  mini-batch 527/4459:  Train loss: 3.331836462020874  Test loss: 3.374819278717041 \n",
      "Epoch: 2/10:  mini-batch 528/4459:  Train loss: 3.443236827850342  Test loss: 3.375046968460083 \n",
      "Epoch: 2/10:  mini-batch 529/4459:  Train loss: 3.260183572769165  Test loss: 3.3751635551452637 \n",
      "Epoch: 2/10:  mini-batch 530/4459:  Train loss: 3.2125394344329834  Test loss: 3.375269889831543 \n",
      "Epoch: 2/10:  mini-batch 531/4459:  Train loss: 3.0665900707244873  Test loss: 3.375404119491577 \n",
      "Epoch: 2/10:  mini-batch 532/4459:  Train loss: 3.1412253379821777  Test loss: 3.3751535415649414 \n",
      "Epoch: 2/10:  mini-batch 533/4459:  Train loss: 3.237847089767456  Test loss: 3.375256061553955 \n",
      "Epoch: 2/10:  mini-batch 534/4459:  Train loss: 2.997570753097534  Test loss: 3.375847578048706 \n",
      "Epoch: 2/10:  mini-batch 535/4459:  Train loss: 3.0940113067626953  Test loss: 3.3766822814941406 \n",
      "Epoch: 2/10:  mini-batch 536/4459:  Train loss: 3.776129961013794  Test loss: 3.376704692840576 \n",
      "Epoch: 2/10:  mini-batch 537/4459:  Train loss: 3.488316774368286  Test loss: 3.3763697147369385 \n",
      "Epoch: 2/10:  mini-batch 538/4459:  Train loss: 3.1286330223083496  Test loss: 3.3763697147369385 \n",
      "Epoch: 2/10:  mini-batch 539/4459:  Train loss: 3.088315010070801  Test loss: 3.376479148864746 \n",
      "Epoch: 2/10:  mini-batch 540/4459:  Train loss: 3.3151795864105225  Test loss: 3.3761959075927734 \n",
      "Epoch: 2/10:  mini-batch 541/4459:  Train loss: 3.2320008277893066  Test loss: 3.3760499954223633 \n",
      "Epoch: 2/10:  mini-batch 542/4459:  Train loss: 3.0614025592803955  Test loss: 3.3762216567993164 \n",
      "Epoch: 2/10:  mini-batch 543/4459:  Train loss: 3.308012008666992  Test loss: 3.376325845718384 \n",
      "Epoch: 2/10:  mini-batch 544/4459:  Train loss: 3.4996001720428467  Test loss: 3.3764588832855225 \n",
      "Epoch: 2/10:  mini-batch 545/4459:  Train loss: 2.751875877380371  Test loss: 3.377655267715454 \n",
      "Epoch: 2/10:  mini-batch 546/4459:  Train loss: 3.255657434463501  Test loss: 3.3794047832489014 \n",
      "Epoch: 2/10:  mini-batch 547/4459:  Train loss: 2.9721450805664062  Test loss: 3.3822288513183594 \n",
      "Epoch: 2/10:  mini-batch 548/4459:  Train loss: 3.3266255855560303  Test loss: 3.383584976196289 \n",
      "Epoch: 2/10:  mini-batch 549/4459:  Train loss: 3.338526725769043  Test loss: 3.3840298652648926 \n",
      "Epoch: 2/10:  mini-batch 550/4459:  Train loss: 3.9064688682556152  Test loss: 3.3824713230133057 \n",
      "Epoch: 2/10:  mini-batch 551/4459:  Train loss: 2.9524364471435547  Test loss: 3.381683588027954 \n",
      "Epoch: 2/10:  mini-batch 552/4459:  Train loss: 3.8004162311553955  Test loss: 3.380228281021118 \n",
      "Epoch: 2/10:  mini-batch 553/4459:  Train loss: 3.489570140838623  Test loss: 3.379133701324463 \n",
      "Epoch: 2/10:  mini-batch 554/4459:  Train loss: 3.4218950271606445  Test loss: 3.378180980682373 \n",
      "Epoch: 2/10:  mini-batch 555/4459:  Train loss: 3.6541366577148438  Test loss: 3.377263307571411 \n",
      "Epoch: 2/10:  mini-batch 556/4459:  Train loss: 3.2695655822753906  Test loss: 3.377023696899414 \n",
      "Epoch: 2/10:  mini-batch 557/4459:  Train loss: 3.5525879859924316  Test loss: 3.3769609928131104 \n",
      "Epoch: 2/10:  mini-batch 558/4459:  Train loss: 3.397305488586426  Test loss: 3.3769376277923584 \n",
      "Epoch: 2/10:  mini-batch 559/4459:  Train loss: 3.6753478050231934  Test loss: 3.3770904541015625 \n",
      "Epoch: 2/10:  mini-batch 560/4459:  Train loss: 3.387044906616211  Test loss: 3.377781629562378 \n",
      "Epoch: 2/10:  mini-batch 561/4459:  Train loss: 3.3864784240722656  Test loss: 3.378257989883423 \n",
      "Epoch: 2/10:  mini-batch 562/4459:  Train loss: 3.869873046875  Test loss: 3.3783042430877686 \n",
      "Epoch: 2/10:  mini-batch 563/4459:  Train loss: 3.3675055503845215  Test loss: 3.3785784244537354 \n",
      "Epoch: 2/10:  mini-batch 564/4459:  Train loss: 3.278733730316162  Test loss: 3.3788697719573975 \n",
      "Epoch: 2/10:  mini-batch 565/4459:  Train loss: 3.3544998168945312  Test loss: 3.3791935443878174 \n",
      "Epoch: 2/10:  mini-batch 566/4459:  Train loss: 2.9874684810638428  Test loss: 3.3798022270202637 \n",
      "Epoch: 2/10:  mini-batch 567/4459:  Train loss: 3.0406038761138916  Test loss: 3.380460023880005 \n",
      "Epoch: 2/10:  mini-batch 568/4459:  Train loss: 3.1865944862365723  Test loss: 3.380603790283203 \n",
      "Epoch: 2/10:  mini-batch 569/4459:  Train loss: 3.144003391265869  Test loss: 3.381033420562744 \n",
      "Epoch: 2/10:  mini-batch 570/4459:  Train loss: 3.4718077182769775  Test loss: 3.381286859512329 \n",
      "Epoch: 2/10:  mini-batch 571/4459:  Train loss: 3.278947114944458  Test loss: 3.381962776184082 \n",
      "Epoch: 2/10:  mini-batch 572/4459:  Train loss: 3.5062389373779297  Test loss: 3.382749319076538 \n",
      "Epoch: 2/10:  mini-batch 573/4459:  Train loss: 3.515429973602295  Test loss: 3.383566379547119 \n",
      "Epoch: 2/10:  mini-batch 574/4459:  Train loss: 3.450456380844116  Test loss: 3.384535789489746 \n",
      "Epoch: 2/10:  mini-batch 575/4459:  Train loss: 3.1907434463500977  Test loss: 3.385197401046753 \n",
      "Epoch: 2/10:  mini-batch 576/4459:  Train loss: 3.254748821258545  Test loss: 3.3858132362365723 \n",
      "Epoch: 2/10:  mini-batch 577/4459:  Train loss: 3.2911133766174316  Test loss: 3.385944366455078 \n",
      "Epoch: 2/10:  mini-batch 578/4459:  Train loss: 3.503234386444092  Test loss: 3.3862435817718506 \n",
      "Epoch: 2/10:  mini-batch 579/4459:  Train loss: 3.299497604370117  Test loss: 3.3863844871520996 \n",
      "Epoch: 2/10:  mini-batch 580/4459:  Train loss: 3.203516960144043  Test loss: 3.386843681335449 \n",
      "Epoch: 2/10:  mini-batch 581/4459:  Train loss: 3.2168545722961426  Test loss: 3.387150526046753 \n",
      "Epoch: 2/10:  mini-batch 582/4459:  Train loss: 3.317702531814575  Test loss: 3.3877129554748535 \n",
      "Epoch: 2/10:  mini-batch 583/4459:  Train loss: 3.5584402084350586  Test loss: 3.3883700370788574 \n",
      "Epoch: 2/10:  mini-batch 584/4459:  Train loss: 3.7117326259613037  Test loss: 3.3893003463745117 \n",
      "Epoch: 2/10:  mini-batch 585/4459:  Train loss: 3.2423391342163086  Test loss: 3.3903145790100098 \n",
      "Epoch: 2/10:  mini-batch 586/4459:  Train loss: 3.250800609588623  Test loss: 3.3908634185791016 \n",
      "Epoch: 2/10:  mini-batch 587/4459:  Train loss: 3.201328992843628  Test loss: 3.3912811279296875 \n",
      "Epoch: 2/10:  mini-batch 588/4459:  Train loss: 3.23097562789917  Test loss: 3.391298770904541 \n",
      "Epoch: 2/10:  mini-batch 589/4459:  Train loss: 3.355027675628662  Test loss: 3.391972303390503 \n",
      "Epoch: 2/10:  mini-batch 590/4459:  Train loss: 3.5046777725219727  Test loss: 3.392076015472412 \n",
      "Epoch: 2/10:  mini-batch 591/4459:  Train loss: 3.4397454261779785  Test loss: 3.3918001651763916 \n",
      "Epoch: 2/10:  mini-batch 592/4459:  Train loss: 3.700899362564087  Test loss: 3.391223669052124 \n",
      "Epoch: 2/10:  mini-batch 593/4459:  Train loss: 2.8437044620513916  Test loss: 3.391185998916626 \n",
      "Epoch: 2/10:  mini-batch 594/4459:  Train loss: 3.335678815841675  Test loss: 3.390955924987793 \n",
      "Epoch: 2/10:  mini-batch 595/4459:  Train loss: 3.5138821601867676  Test loss: 3.390317678451538 \n",
      "Epoch: 2/10:  mini-batch 596/4459:  Train loss: 3.4011149406433105  Test loss: 3.389397621154785 \n",
      "Epoch: 2/10:  mini-batch 597/4459:  Train loss: 3.579379081726074  Test loss: 3.3881356716156006 \n",
      "Epoch: 2/10:  mini-batch 598/4459:  Train loss: 3.6784305572509766  Test loss: 3.386747360229492 \n",
      "Epoch: 2/10:  mini-batch 599/4459:  Train loss: 3.2865405082702637  Test loss: 3.3857555389404297 \n",
      "Epoch: 2/10:  mini-batch 600/4459:  Train loss: 2.989837646484375  Test loss: 3.385094165802002 \n",
      "Epoch: 2/10:  mini-batch 601/4459:  Train loss: 3.170647382736206  Test loss: 3.384474277496338 \n",
      "Epoch: 2/10:  mini-batch 602/4459:  Train loss: 3.58860182762146  Test loss: 3.38356876373291 \n",
      "Epoch: 2/10:  mini-batch 603/4459:  Train loss: 3.271212577819824  Test loss: 3.382856845855713 \n",
      "Epoch: 2/10:  mini-batch 604/4459:  Train loss: 3.414431095123291  Test loss: 3.382495641708374 \n",
      "Epoch: 2/10:  mini-batch 605/4459:  Train loss: 3.0545215606689453  Test loss: 3.3821613788604736 \n",
      "Epoch: 2/10:  mini-batch 606/4459:  Train loss: 3.612844228744507  Test loss: 3.3819568157196045 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 607/4459:  Train loss: 3.087045431137085  Test loss: 3.3820888996124268 \n",
      "Epoch: 2/10:  mini-batch 608/4459:  Train loss: 2.9600839614868164  Test loss: 3.382446527481079 \n",
      "Epoch: 2/10:  mini-batch 609/4459:  Train loss: 3.1483945846557617  Test loss: 3.383023738861084 \n",
      "Epoch: 2/10:  mini-batch 610/4459:  Train loss: 3.443991184234619  Test loss: 3.3834407329559326 \n",
      "Epoch: 2/10:  mini-batch 611/4459:  Train loss: 3.646698236465454  Test loss: 3.383661985397339 \n",
      "Epoch: 2/10:  mini-batch 612/4459:  Train loss: 3.7552032470703125  Test loss: 3.383824348449707 \n",
      "Epoch: 2/10:  mini-batch 613/4459:  Train loss: 3.367466449737549  Test loss: 3.3837509155273438 \n",
      "Epoch: 2/10:  mini-batch 614/4459:  Train loss: 3.0985963344573975  Test loss: 3.3837738037109375 \n",
      "Epoch: 2/10:  mini-batch 615/4459:  Train loss: 3.2713029384613037  Test loss: 3.3838369846343994 \n",
      "Epoch: 2/10:  mini-batch 616/4459:  Train loss: 3.1283164024353027  Test loss: 3.3840200901031494 \n",
      "Epoch: 2/10:  mini-batch 617/4459:  Train loss: 3.4054388999938965  Test loss: 3.383981227874756 \n",
      "Epoch: 2/10:  mini-batch 618/4459:  Train loss: 3.2679433822631836  Test loss: 3.383634090423584 \n",
      "Epoch: 2/10:  mini-batch 619/4459:  Train loss: 3.6017074584960938  Test loss: 3.3832225799560547 \n",
      "Epoch: 2/10:  mini-batch 620/4459:  Train loss: 3.7075114250183105  Test loss: 3.382983684539795 \n",
      "Epoch: 2/10:  mini-batch 621/4459:  Train loss: 3.522094249725342  Test loss: 3.382737159729004 \n",
      "Epoch: 2/10:  mini-batch 622/4459:  Train loss: 3.2033910751342773  Test loss: 3.382473945617676 \n",
      "Epoch: 2/10:  mini-batch 623/4459:  Train loss: 3.0103909969329834  Test loss: 3.382009983062744 \n",
      "Epoch: 2/10:  mini-batch 624/4459:  Train loss: 3.288389205932617  Test loss: 3.381474494934082 \n",
      "Epoch: 2/10:  mini-batch 625/4459:  Train loss: 3.6151013374328613  Test loss: 3.3808536529541016 \n",
      "Epoch: 2/10:  mini-batch 626/4459:  Train loss: 3.3560752868652344  Test loss: 3.3805010318756104 \n",
      "Epoch: 2/10:  mini-batch 627/4459:  Train loss: 3.2219016551971436  Test loss: 3.3805079460144043 \n",
      "Epoch: 2/10:  mini-batch 628/4459:  Train loss: 3.3125460147857666  Test loss: 3.3806087970733643 \n",
      "Epoch: 2/10:  mini-batch 629/4459:  Train loss: 3.0138180255889893  Test loss: 3.3807168006896973 \n",
      "Epoch: 2/10:  mini-batch 630/4459:  Train loss: 2.89528751373291  Test loss: 3.381084442138672 \n",
      "Epoch: 2/10:  mini-batch 631/4459:  Train loss: 3.306638717651367  Test loss: 3.3814752101898193 \n",
      "Epoch: 2/10:  mini-batch 632/4459:  Train loss: 3.1149449348449707  Test loss: 3.3822109699249268 \n",
      "Epoch: 2/10:  mini-batch 633/4459:  Train loss: 3.1352975368499756  Test loss: 3.383021831512451 \n",
      "Epoch: 2/10:  mini-batch 634/4459:  Train loss: 3.8753504753112793  Test loss: 3.3831777572631836 \n",
      "Epoch: 2/10:  mini-batch 635/4459:  Train loss: 3.669734001159668  Test loss: 3.3831825256347656 \n",
      "Epoch: 2/10:  mini-batch 636/4459:  Train loss: 3.513345241546631  Test loss: 3.3830008506774902 \n",
      "Epoch: 2/10:  mini-batch 637/4459:  Train loss: 3.4198455810546875  Test loss: 3.3824572563171387 \n",
      "Epoch: 2/10:  mini-batch 638/4459:  Train loss: 3.388521194458008  Test loss: 3.3819212913513184 \n",
      "Epoch: 2/10:  mini-batch 639/4459:  Train loss: 3.20051646232605  Test loss: 3.3814523220062256 \n",
      "Epoch: 2/10:  mini-batch 640/4459:  Train loss: 3.2949137687683105  Test loss: 3.3808703422546387 \n",
      "Epoch: 2/10:  mini-batch 641/4459:  Train loss: 3.622006893157959  Test loss: 3.3800315856933594 \n",
      "Epoch: 2/10:  mini-batch 642/4459:  Train loss: 3.018958568572998  Test loss: 3.3792576789855957 \n",
      "Epoch: 2/10:  mini-batch 643/4459:  Train loss: 3.203676700592041  Test loss: 3.378909111022949 \n",
      "Epoch: 2/10:  mini-batch 644/4459:  Train loss: 3.380448341369629  Test loss: 3.3786327838897705 \n",
      "Epoch: 2/10:  mini-batch 645/4459:  Train loss: 3.3598413467407227  Test loss: 3.3786892890930176 \n",
      "Epoch: 2/10:  mini-batch 646/4459:  Train loss: 3.66093111038208  Test loss: 3.3787291049957275 \n",
      "Epoch: 2/10:  mini-batch 647/4459:  Train loss: 3.2944839000701904  Test loss: 3.37872052192688 \n",
      "Epoch: 2/10:  mini-batch 648/4459:  Train loss: 3.4718313217163086  Test loss: 3.37814998626709 \n",
      "Epoch: 2/10:  mini-batch 649/4459:  Train loss: 3.1236085891723633  Test loss: 3.3779101371765137 \n",
      "Epoch: 2/10:  mini-batch 650/4459:  Train loss: 3.356069564819336  Test loss: 3.377951145172119 \n",
      "Epoch: 2/10:  mini-batch 651/4459:  Train loss: 3.5231266021728516  Test loss: 3.3780500888824463 \n",
      "Epoch: 2/10:  mini-batch 652/4459:  Train loss: 3.093622922897339  Test loss: 3.378264904022217 \n",
      "Epoch: 2/10:  mini-batch 653/4459:  Train loss: 3.0419745445251465  Test loss: 3.378629684448242 \n",
      "Epoch: 2/10:  mini-batch 654/4459:  Train loss: 3.3169143199920654  Test loss: 3.378948926925659 \n",
      "Epoch: 2/10:  mini-batch 655/4459:  Train loss: 3.271063804626465  Test loss: 3.3790640830993652 \n",
      "Epoch: 2/10:  mini-batch 656/4459:  Train loss: 3.525949001312256  Test loss: 3.3793492317199707 \n",
      "Epoch: 2/10:  mini-batch 657/4459:  Train loss: 3.113248825073242  Test loss: 3.380091667175293 \n",
      "Epoch: 2/10:  mini-batch 658/4459:  Train loss: 3.0399866104125977  Test loss: 3.380798816680908 \n",
      "Epoch: 2/10:  mini-batch 659/4459:  Train loss: 3.4227068424224854  Test loss: 3.3812766075134277 \n",
      "Epoch: 2/10:  mini-batch 660/4459:  Train loss: 3.190284252166748  Test loss: 3.3818254470825195 \n",
      "Epoch: 2/10:  mini-batch 661/4459:  Train loss: 3.4241206645965576  Test loss: 3.382575273513794 \n",
      "Epoch: 2/10:  mini-batch 662/4459:  Train loss: 3.3427977561950684  Test loss: 3.383345603942871 \n",
      "Epoch: 2/10:  mini-batch 663/4459:  Train loss: 3.160707473754883  Test loss: 3.384229898452759 \n",
      "Epoch: 2/10:  mini-batch 664/4459:  Train loss: 3.151230812072754  Test loss: 3.384690761566162 \n",
      "Epoch: 2/10:  mini-batch 665/4459:  Train loss: 4.161516189575195  Test loss: 3.3848142623901367 \n",
      "Epoch: 2/10:  mini-batch 666/4459:  Train loss: 3.677910804748535  Test loss: 3.3847248554229736 \n",
      "Epoch: 2/10:  mini-batch 667/4459:  Train loss: 3.1271708011627197  Test loss: 3.384760856628418 \n",
      "Epoch: 2/10:  mini-batch 668/4459:  Train loss: 2.9706544876098633  Test loss: 3.384998321533203 \n",
      "Epoch: 2/10:  mini-batch 669/4459:  Train loss: 3.0134005546569824  Test loss: 3.385474681854248 \n",
      "Epoch: 2/10:  mini-batch 670/4459:  Train loss: 3.2788753509521484  Test loss: 3.3862040042877197 \n",
      "Epoch: 2/10:  mini-batch 671/4459:  Train loss: 2.912020206451416  Test loss: 3.3874194622039795 \n",
      "Epoch: 2/10:  mini-batch 672/4459:  Train loss: 3.1451590061187744  Test loss: 3.388822078704834 \n",
      "Epoch: 2/10:  mini-batch 673/4459:  Train loss: 3.344121217727661  Test loss: 3.389802932739258 \n",
      "Epoch: 2/10:  mini-batch 674/4459:  Train loss: 3.334804058074951  Test loss: 3.3903753757476807 \n",
      "Epoch: 2/10:  mini-batch 675/4459:  Train loss: 3.5553927421569824  Test loss: 3.3906471729278564 \n",
      "Epoch: 2/10:  mini-batch 676/4459:  Train loss: 3.3691110610961914  Test loss: 3.3905551433563232 \n",
      "Epoch: 2/10:  mini-batch 677/4459:  Train loss: 4.031896591186523  Test loss: 3.390016555786133 \n",
      "Epoch: 2/10:  mini-batch 678/4459:  Train loss: 3.42586612701416  Test loss: 3.389491319656372 \n",
      "Epoch: 2/10:  mini-batch 679/4459:  Train loss: 2.9493772983551025  Test loss: 3.389209747314453 \n",
      "Epoch: 2/10:  mini-batch 680/4459:  Train loss: 3.2408132553100586  Test loss: 3.3882761001586914 \n",
      "Epoch: 2/10:  mini-batch 681/4459:  Train loss: 3.222785711288452  Test loss: 3.3872358798980713 \n",
      "Epoch: 2/10:  mini-batch 682/4459:  Train loss: 3.864208698272705  Test loss: 3.3858988285064697 \n",
      "Epoch: 2/10:  mini-batch 683/4459:  Train loss: 3.2165162563323975  Test loss: 3.3847103118896484 \n",
      "Epoch: 2/10:  mini-batch 684/4459:  Train loss: 3.0656163692474365  Test loss: 3.383586883544922 \n",
      "Epoch: 2/10:  mini-batch 685/4459:  Train loss: 3.4844601154327393  Test loss: 3.382415771484375 \n",
      "Epoch: 2/10:  mini-batch 686/4459:  Train loss: 3.3444056510925293  Test loss: 3.3810977935791016 \n",
      "Epoch: 2/10:  mini-batch 687/4459:  Train loss: 3.1063854694366455  Test loss: 3.379877805709839 \n",
      "Epoch: 2/10:  mini-batch 688/4459:  Train loss: 3.6884043216705322  Test loss: 3.378538131713867 \n",
      "Epoch: 2/10:  mini-batch 689/4459:  Train loss: 3.1099143028259277  Test loss: 3.3771257400512695 \n",
      "Epoch: 2/10:  mini-batch 690/4459:  Train loss: 3.753086566925049  Test loss: 3.3753252029418945 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 691/4459:  Train loss: 3.6837728023529053  Test loss: 3.3734426498413086 \n",
      "Epoch: 2/10:  mini-batch 692/4459:  Train loss: 3.294790744781494  Test loss: 3.3716506958007812 \n",
      "Epoch: 2/10:  mini-batch 693/4459:  Train loss: 2.9190104007720947  Test loss: 3.3704729080200195 \n",
      "Epoch: 2/10:  mini-batch 694/4459:  Train loss: 3.527163505554199  Test loss: 3.3694591522216797 \n",
      "Epoch: 2/10:  mini-batch 695/4459:  Train loss: 2.537234306335449  Test loss: 3.3692445755004883 \n",
      "Epoch: 2/10:  mini-batch 696/4459:  Train loss: 3.319314479827881  Test loss: 3.369009494781494 \n",
      "Epoch: 2/10:  mini-batch 697/4459:  Train loss: 3.1605119705200195  Test loss: 3.369077205657959 \n",
      "Epoch: 2/10:  mini-batch 698/4459:  Train loss: 3.0356249809265137  Test loss: 3.369464874267578 \n",
      "Epoch: 2/10:  mini-batch 699/4459:  Train loss: 2.833406925201416  Test loss: 3.370335578918457 \n",
      "Epoch: 2/10:  mini-batch 700/4459:  Train loss: 2.9672818183898926  Test loss: 3.3716659545898438 \n",
      "Epoch: 2/10:  mini-batch 701/4459:  Train loss: 3.052717924118042  Test loss: 3.373023271560669 \n",
      "Epoch: 2/10:  mini-batch 702/4459:  Train loss: 2.927255630493164  Test loss: 3.3748512268066406 \n",
      "Epoch: 2/10:  mini-batch 703/4459:  Train loss: 3.953007459640503  Test loss: 3.3762152194976807 \n",
      "Epoch: 2/10:  mini-batch 704/4459:  Train loss: 3.5755209922790527  Test loss: 3.37725830078125 \n",
      "Epoch: 2/10:  mini-batch 705/4459:  Train loss: 3.1469147205352783  Test loss: 3.3785295486450195 \n",
      "Epoch: 2/10:  mini-batch 706/4459:  Train loss: 3.420603036880493  Test loss: 3.379289150238037 \n",
      "Epoch: 2/10:  mini-batch 707/4459:  Train loss: 2.836378574371338  Test loss: 3.381019115447998 \n",
      "Epoch: 2/10:  mini-batch 708/4459:  Train loss: 3.131631374359131  Test loss: 3.383035898208618 \n",
      "Epoch: 2/10:  mini-batch 709/4459:  Train loss: 3.3685271739959717  Test loss: 3.3849875926971436 \n",
      "Epoch: 2/10:  mini-batch 710/4459:  Train loss: 3.223677635192871  Test loss: 3.386686325073242 \n",
      "Epoch: 2/10:  mini-batch 711/4459:  Train loss: 3.0788156986236572  Test loss: 3.3885674476623535 \n",
      "Epoch: 2/10:  mini-batch 712/4459:  Train loss: 3.1531457901000977  Test loss: 3.390526294708252 \n",
      "Epoch: 2/10:  mini-batch 713/4459:  Train loss: 3.3741025924682617  Test loss: 3.3916685581207275 \n",
      "Epoch: 2/10:  mini-batch 714/4459:  Train loss: 3.31123685836792  Test loss: 3.391724109649658 \n",
      "Epoch: 2/10:  mini-batch 715/4459:  Train loss: 3.2054972648620605  Test loss: 3.3913731575012207 \n",
      "Epoch: 2/10:  mini-batch 716/4459:  Train loss: 3.496833562850952  Test loss: 3.3907766342163086 \n",
      "Epoch: 2/10:  mini-batch 717/4459:  Train loss: 3.5604610443115234  Test loss: 3.389390468597412 \n",
      "Epoch: 2/10:  mini-batch 718/4459:  Train loss: 2.9838485717773438  Test loss: 3.3884904384613037 \n",
      "Epoch: 2/10:  mini-batch 719/4459:  Train loss: 2.9784016609191895  Test loss: 3.3880486488342285 \n",
      "Epoch: 2/10:  mini-batch 720/4459:  Train loss: 3.2260727882385254  Test loss: 3.3873801231384277 \n",
      "Epoch: 2/10:  mini-batch 721/4459:  Train loss: 3.0291781425476074  Test loss: 3.3871548175811768 \n",
      "Epoch: 2/10:  mini-batch 722/4459:  Train loss: 3.167259454727173  Test loss: 3.386868476867676 \n",
      "Epoch: 2/10:  mini-batch 723/4459:  Train loss: 3.0914559364318848  Test loss: 3.3867857456207275 \n",
      "Epoch: 2/10:  mini-batch 724/4459:  Train loss: 3.098081111907959  Test loss: 3.3866045475006104 \n",
      "Epoch: 2/10:  mini-batch 725/4459:  Train loss: 3.5315427780151367  Test loss: 3.385622024536133 \n",
      "Epoch: 2/10:  mini-batch 726/4459:  Train loss: 3.6280136108398438  Test loss: 3.3836441040039062 \n",
      "Epoch: 2/10:  mini-batch 727/4459:  Train loss: 3.227717399597168  Test loss: 3.3817875385284424 \n",
      "Epoch: 2/10:  mini-batch 728/4459:  Train loss: 3.0746889114379883  Test loss: 3.380157709121704 \n",
      "Epoch: 2/10:  mini-batch 729/4459:  Train loss: 3.113398313522339  Test loss: 3.3789145946502686 \n",
      "Epoch: 2/10:  mini-batch 730/4459:  Train loss: 3.664605140686035  Test loss: 3.376896858215332 \n",
      "Epoch: 2/10:  mini-batch 731/4459:  Train loss: 3.398388385772705  Test loss: 3.374335289001465 \n",
      "Epoch: 2/10:  mini-batch 732/4459:  Train loss: 3.0322928428649902  Test loss: 3.37219500541687 \n",
      "Epoch: 2/10:  mini-batch 733/4459:  Train loss: 3.252915382385254  Test loss: 3.3701841831207275 \n",
      "Epoch: 2/10:  mini-batch 734/4459:  Train loss: 3.5132346153259277  Test loss: 3.3676345348358154 \n",
      "Epoch: 2/10:  mini-batch 735/4459:  Train loss: 3.1217541694641113  Test loss: 3.3656229972839355 \n",
      "Epoch: 2/10:  mini-batch 736/4459:  Train loss: 2.9922428131103516  Test loss: 3.364103317260742 \n",
      "Epoch: 2/10:  mini-batch 737/4459:  Train loss: 2.9358396530151367  Test loss: 3.363147258758545 \n",
      "Epoch: 2/10:  mini-batch 738/4459:  Train loss: 3.4506826400756836  Test loss: 3.3622117042541504 \n",
      "Epoch: 2/10:  mini-batch 739/4459:  Train loss: 3.310412883758545  Test loss: 3.360588550567627 \n",
      "Epoch: 2/10:  mini-batch 740/4459:  Train loss: 3.185965061187744  Test loss: 3.359295606613159 \n",
      "Epoch: 2/10:  mini-batch 741/4459:  Train loss: 3.2468318939208984  Test loss: 3.3579585552215576 \n",
      "Epoch: 2/10:  mini-batch 742/4459:  Train loss: 3.4361023902893066  Test loss: 3.3562254905700684 \n",
      "Epoch: 2/10:  mini-batch 743/4459:  Train loss: 3.228121042251587  Test loss: 3.354860544204712 \n",
      "Epoch: 2/10:  mini-batch 744/4459:  Train loss: 3.3329708576202393  Test loss: 3.3538689613342285 \n",
      "Epoch: 2/10:  mini-batch 745/4459:  Train loss: 3.767207145690918  Test loss: 3.3524138927459717 \n",
      "Epoch: 2/10:  mini-batch 746/4459:  Train loss: 2.9646530151367188  Test loss: 3.3517630100250244 \n",
      "Epoch: 2/10:  mini-batch 747/4459:  Train loss: 2.700885057449341  Test loss: 3.351933479309082 \n",
      "Epoch: 2/10:  mini-batch 748/4459:  Train loss: 3.056307315826416  Test loss: 3.3519177436828613 \n",
      "Epoch: 2/10:  mini-batch 749/4459:  Train loss: 3.4313931465148926  Test loss: 3.3516745567321777 \n",
      "Epoch: 2/10:  mini-batch 750/4459:  Train loss: 3.457888126373291  Test loss: 3.3511648178100586 \n",
      "Epoch: 2/10:  mini-batch 751/4459:  Train loss: 3.7098052501678467  Test loss: 3.3503806591033936 \n",
      "Epoch: 2/10:  mini-batch 752/4459:  Train loss: 3.446814775466919  Test loss: 3.3493056297302246 \n",
      "Epoch: 2/10:  mini-batch 753/4459:  Train loss: 3.3750219345092773  Test loss: 3.3481903076171875 \n",
      "Epoch: 2/10:  mini-batch 754/4459:  Train loss: 3.2253448963165283  Test loss: 3.3471789360046387 \n",
      "Epoch: 2/10:  mini-batch 755/4459:  Train loss: 3.0671145915985107  Test loss: 3.3465068340301514 \n",
      "Epoch: 2/10:  mini-batch 756/4459:  Train loss: 3.215235471725464  Test loss: 3.345921516418457 \n",
      "Epoch: 2/10:  mini-batch 757/4459:  Train loss: 2.890565872192383  Test loss: 3.345644474029541 \n",
      "Epoch: 2/10:  mini-batch 758/4459:  Train loss: 2.9307913780212402  Test loss: 3.345730781555176 \n",
      "Epoch: 2/10:  mini-batch 759/4459:  Train loss: 3.491677761077881  Test loss: 3.3454933166503906 \n",
      "Epoch: 2/10:  mini-batch 760/4459:  Train loss: 3.188903331756592  Test loss: 3.3452565670013428 \n",
      "Epoch: 2/10:  mini-batch 761/4459:  Train loss: 3.149512767791748  Test loss: 3.3451297283172607 \n",
      "Epoch: 2/10:  mini-batch 762/4459:  Train loss: 3.684396982192993  Test loss: 3.3447999954223633 \n",
      "Epoch: 2/10:  mini-batch 763/4459:  Train loss: 3.4333224296569824  Test loss: 3.344590902328491 \n",
      "Epoch: 2/10:  mini-batch 764/4459:  Train loss: 3.0826258659362793  Test loss: 3.3444361686706543 \n",
      "Epoch: 2/10:  mini-batch 765/4459:  Train loss: 3.5169413089752197  Test loss: 3.3441383838653564 \n",
      "Epoch: 2/10:  mini-batch 766/4459:  Train loss: 3.8063478469848633  Test loss: 3.343538522720337 \n",
      "Epoch: 2/10:  mini-batch 767/4459:  Train loss: 3.5192689895629883  Test loss: 3.3429911136627197 \n",
      "Epoch: 2/10:  mini-batch 768/4459:  Train loss: 2.9961113929748535  Test loss: 3.3427066802978516 \n",
      "Epoch: 2/10:  mini-batch 769/4459:  Train loss: 3.1665451526641846  Test loss: 3.3420968055725098 \n",
      "Epoch: 2/10:  mini-batch 770/4459:  Train loss: 3.319911241531372  Test loss: 3.341561794281006 \n",
      "Epoch: 2/10:  mini-batch 771/4459:  Train loss: 3.0874032974243164  Test loss: 3.3414313793182373 \n",
      "Epoch: 2/10:  mini-batch 772/4459:  Train loss: 3.4802699089050293  Test loss: 3.3412857055664062 \n",
      "Epoch: 2/10:  mini-batch 773/4459:  Train loss: 3.144519805908203  Test loss: 3.34147310256958 \n",
      "Epoch: 2/10:  mini-batch 774/4459:  Train loss: 3.2421650886535645  Test loss: 3.341671943664551 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 775/4459:  Train loss: 3.601130723953247  Test loss: 3.3417415618896484 \n",
      "Epoch: 2/10:  mini-batch 776/4459:  Train loss: 2.9087486267089844  Test loss: 3.3420066833496094 \n",
      "Epoch: 2/10:  mini-batch 777/4459:  Train loss: 3.024394989013672  Test loss: 3.3425307273864746 \n",
      "Epoch: 2/10:  mini-batch 778/4459:  Train loss: 3.4481945037841797  Test loss: 3.3429102897644043 \n",
      "Epoch: 2/10:  mini-batch 779/4459:  Train loss: 2.9516818523406982  Test loss: 3.343362808227539 \n",
      "Epoch: 2/10:  mini-batch 780/4459:  Train loss: 3.0353405475616455  Test loss: 3.343839406967163 \n",
      "Epoch: 2/10:  mini-batch 781/4459:  Train loss: 3.2843704223632812  Test loss: 3.3444337844848633 \n",
      "Epoch: 2/10:  mini-batch 782/4459:  Train loss: 3.641587257385254  Test loss: 3.3448092937469482 \n",
      "Epoch: 2/10:  mini-batch 783/4459:  Train loss: 3.161414861679077  Test loss: 3.344447135925293 \n",
      "Epoch: 2/10:  mini-batch 784/4459:  Train loss: 3.482419967651367  Test loss: 3.3440375328063965 \n",
      "Epoch: 2/10:  mini-batch 785/4459:  Train loss: 3.472179412841797  Test loss: 3.343628168106079 \n",
      "Epoch: 2/10:  mini-batch 786/4459:  Train loss: 3.6258678436279297  Test loss: 3.3426923751831055 \n",
      "Epoch: 2/10:  mini-batch 787/4459:  Train loss: 3.0859570503234863  Test loss: 3.342097520828247 \n",
      "Epoch: 2/10:  mini-batch 788/4459:  Train loss: 3.564305305480957  Test loss: 3.3417067527770996 \n",
      "Epoch: 2/10:  mini-batch 789/4459:  Train loss: 3.8539862632751465  Test loss: 3.3416452407836914 \n",
      "Epoch: 2/10:  mini-batch 790/4459:  Train loss: 3.5846846103668213  Test loss: 3.341691255569458 \n",
      "Epoch: 2/10:  mini-batch 791/4459:  Train loss: 3.807615280151367  Test loss: 3.3417458534240723 \n",
      "Epoch: 2/10:  mini-batch 792/4459:  Train loss: 2.8967294692993164  Test loss: 3.3421764373779297 \n",
      "Epoch: 2/10:  mini-batch 793/4459:  Train loss: 3.487323760986328  Test loss: 3.3425731658935547 \n",
      "Epoch: 2/10:  mini-batch 794/4459:  Train loss: 3.154463768005371  Test loss: 3.3432376384735107 \n",
      "Epoch: 2/10:  mini-batch 795/4459:  Train loss: 3.017359733581543  Test loss: 3.3437185287475586 \n",
      "Epoch: 2/10:  mini-batch 796/4459:  Train loss: 3.063159704208374  Test loss: 3.343712329864502 \n",
      "Epoch: 2/10:  mini-batch 797/4459:  Train loss: 3.384294033050537  Test loss: 3.3438963890075684 \n",
      "Epoch: 2/10:  mini-batch 798/4459:  Train loss: 3.4935216903686523  Test loss: 3.3441696166992188 \n",
      "Epoch: 2/10:  mini-batch 799/4459:  Train loss: 2.9694738388061523  Test loss: 3.3441567420959473 \n",
      "Epoch: 2/10:  mini-batch 800/4459:  Train loss: 3.010483980178833  Test loss: 3.3443312644958496 \n",
      "Epoch: 2/10:  mini-batch 801/4459:  Train loss: 3.4106979370117188  Test loss: 3.344114303588867 \n",
      "Epoch: 2/10:  mini-batch 802/4459:  Train loss: 3.0845932960510254  Test loss: 3.3441855907440186 \n",
      "Epoch: 2/10:  mini-batch 803/4459:  Train loss: 3.270549774169922  Test loss: 3.3439958095550537 \n",
      "Epoch: 2/10:  mini-batch 804/4459:  Train loss: 3.2332684993743896  Test loss: 3.3438282012939453 \n",
      "Epoch: 2/10:  mini-batch 805/4459:  Train loss: 3.7132680416107178  Test loss: 3.3434009552001953 \n",
      "Epoch: 2/10:  mini-batch 806/4459:  Train loss: 3.289992094039917  Test loss: 3.3428797721862793 \n",
      "Epoch: 2/10:  mini-batch 807/4459:  Train loss: 3.4697189331054688  Test loss: 3.3422720432281494 \n",
      "Epoch: 2/10:  mini-batch 808/4459:  Train loss: 3.276888370513916  Test loss: 3.3411967754364014 \n",
      "Epoch: 2/10:  mini-batch 809/4459:  Train loss: 2.959934711456299  Test loss: 3.340468406677246 \n",
      "Epoch: 2/10:  mini-batch 810/4459:  Train loss: 2.904930353164673  Test loss: 3.3403232097625732 \n",
      "Epoch: 2/10:  mini-batch 811/4459:  Train loss: 3.0325138568878174  Test loss: 3.340334415435791 \n",
      "Epoch: 2/10:  mini-batch 812/4459:  Train loss: 2.9389865398406982  Test loss: 3.3404901027679443 \n",
      "Epoch: 2/10:  mini-batch 813/4459:  Train loss: 3.0917394161224365  Test loss: 3.3404641151428223 \n",
      "Epoch: 2/10:  mini-batch 814/4459:  Train loss: 3.1785993576049805  Test loss: 3.3405628204345703 \n",
      "Epoch: 2/10:  mini-batch 815/4459:  Train loss: 2.9573395252227783  Test loss: 3.3411238193511963 \n",
      "Epoch: 2/10:  mini-batch 816/4459:  Train loss: 3.9684340953826904  Test loss: 3.341510057449341 \n",
      "Epoch: 2/10:  mini-batch 817/4459:  Train loss: 3.229656219482422  Test loss: 3.34206485748291 \n",
      "Epoch: 2/10:  mini-batch 818/4459:  Train loss: 3.5906896591186523  Test loss: 3.3426156044006348 \n",
      "Epoch: 2/10:  mini-batch 819/4459:  Train loss: 3.1242337226867676  Test loss: 3.3435163497924805 \n",
      "Epoch: 2/10:  mini-batch 820/4459:  Train loss: 3.5571389198303223  Test loss: 3.3442296981811523 \n",
      "Epoch: 2/10:  mini-batch 821/4459:  Train loss: 2.966679334640503  Test loss: 3.345217227935791 \n",
      "Epoch: 2/10:  mini-batch 822/4459:  Train loss: 3.1847195625305176  Test loss: 3.34578013420105 \n",
      "Epoch: 2/10:  mini-batch 823/4459:  Train loss: 3.2257513999938965  Test loss: 3.3463196754455566 \n",
      "Epoch: 2/10:  mini-batch 824/4459:  Train loss: 3.17626953125  Test loss: 3.3472838401794434 \n",
      "Epoch: 2/10:  mini-batch 825/4459:  Train loss: 3.3232743740081787  Test loss: 3.3482446670532227 \n",
      "Epoch: 2/10:  mini-batch 826/4459:  Train loss: 3.4780659675598145  Test loss: 3.348759174346924 \n",
      "Epoch: 2/10:  mini-batch 827/4459:  Train loss: 3.2935807704925537  Test loss: 3.348419189453125 \n",
      "Epoch: 2/10:  mini-batch 828/4459:  Train loss: 2.9615485668182373  Test loss: 3.348128318786621 \n",
      "Epoch: 2/10:  mini-batch 829/4459:  Train loss: 3.190973997116089  Test loss: 3.3478758335113525 \n",
      "Epoch: 2/10:  mini-batch 830/4459:  Train loss: 2.8036575317382812  Test loss: 3.348252296447754 \n",
      "Epoch: 2/10:  mini-batch 831/4459:  Train loss: 2.8812496662139893  Test loss: 3.3490896224975586 \n",
      "Epoch: 2/10:  mini-batch 832/4459:  Train loss: 3.554414987564087  Test loss: 3.349501848220825 \n",
      "Epoch: 2/10:  mini-batch 833/4459:  Train loss: 3.8527333736419678  Test loss: 3.3488857746124268 \n",
      "Epoch: 2/10:  mini-batch 834/4459:  Train loss: 3.4272830486297607  Test loss: 3.3480136394500732 \n",
      "Epoch: 2/10:  mini-batch 835/4459:  Train loss: 3.435100793838501  Test loss: 3.3470394611358643 \n",
      "Epoch: 2/10:  mini-batch 836/4459:  Train loss: 3.581395149230957  Test loss: 3.345914602279663 \n",
      "Epoch: 2/10:  mini-batch 837/4459:  Train loss: 3.0624518394470215  Test loss: 3.3448615074157715 \n",
      "Epoch: 2/10:  mini-batch 838/4459:  Train loss: 2.7840046882629395  Test loss: 3.3446481227874756 \n",
      "Epoch: 2/10:  mini-batch 839/4459:  Train loss: 3.6531870365142822  Test loss: 3.3441224098205566 \n",
      "Epoch: 2/10:  mini-batch 840/4459:  Train loss: 3.641148090362549  Test loss: 3.342775344848633 \n",
      "Epoch: 2/10:  mini-batch 841/4459:  Train loss: 3.7741148471832275  Test loss: 3.3416121006011963 \n",
      "Epoch: 2/10:  mini-batch 842/4459:  Train loss: 3.198230743408203  Test loss: 3.340939521789551 \n",
      "Epoch: 2/10:  mini-batch 843/4459:  Train loss: 3.5591979026794434  Test loss: 3.340054512023926 \n",
      "Epoch: 2/10:  mini-batch 844/4459:  Train loss: 3.0704805850982666  Test loss: 3.3396525382995605 \n",
      "Epoch: 2/10:  mini-batch 845/4459:  Train loss: 3.2626211643218994  Test loss: 3.339482545852661 \n",
      "Epoch: 2/10:  mini-batch 846/4459:  Train loss: 3.1029629707336426  Test loss: 3.339359760284424 \n",
      "Epoch: 2/10:  mini-batch 847/4459:  Train loss: 3.232999801635742  Test loss: 3.3393056392669678 \n",
      "Epoch: 2/10:  mini-batch 848/4459:  Train loss: 3.5091147422790527  Test loss: 3.339282512664795 \n",
      "Epoch: 2/10:  mini-batch 849/4459:  Train loss: 3.6186914443969727  Test loss: 3.3391947746276855 \n",
      "Epoch: 2/10:  mini-batch 850/4459:  Train loss: 3.5270016193389893  Test loss: 3.3392629623413086 \n",
      "Epoch: 2/10:  mini-batch 851/4459:  Train loss: 3.4392271041870117  Test loss: 3.339538097381592 \n",
      "Epoch: 2/10:  mini-batch 852/4459:  Train loss: 2.9231491088867188  Test loss: 3.339888572692871 \n",
      "Epoch: 2/10:  mini-batch 853/4459:  Train loss: 3.727342128753662  Test loss: 3.339940309524536 \n",
      "Epoch: 2/10:  mini-batch 854/4459:  Train loss: 3.4260830879211426  Test loss: 3.339588165283203 \n",
      "Epoch: 2/10:  mini-batch 855/4459:  Train loss: 3.6937460899353027  Test loss: 3.339336395263672 \n",
      "Epoch: 2/10:  mini-batch 856/4459:  Train loss: 2.8260390758514404  Test loss: 3.3393359184265137 \n",
      "Epoch: 2/10:  mini-batch 857/4459:  Train loss: 3.2805864810943604  Test loss: 3.3393096923828125 \n",
      "Epoch: 2/10:  mini-batch 858/4459:  Train loss: 3.167233943939209  Test loss: 3.3394033908843994 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 859/4459:  Train loss: 3.144550323486328  Test loss: 3.339463710784912 \n",
      "Epoch: 2/10:  mini-batch 860/4459:  Train loss: 3.5048091411590576  Test loss: 3.3395907878875732 \n",
      "Epoch: 2/10:  mini-batch 861/4459:  Train loss: 3.183669090270996  Test loss: 3.339808702468872 \n",
      "Epoch: 2/10:  mini-batch 862/4459:  Train loss: 3.5561752319335938  Test loss: 3.339765787124634 \n",
      "Epoch: 2/10:  mini-batch 863/4459:  Train loss: 3.3558573722839355  Test loss: 3.3400039672851562 \n",
      "Epoch: 2/10:  mini-batch 864/4459:  Train loss: 3.4117984771728516  Test loss: 3.340137004852295 \n",
      "Epoch: 2/10:  mini-batch 865/4459:  Train loss: 3.5540921688079834  Test loss: 3.3402442932128906 \n",
      "Epoch: 2/10:  mini-batch 866/4459:  Train loss: 3.6895623207092285  Test loss: 3.340475559234619 \n",
      "Epoch: 2/10:  mini-batch 867/4459:  Train loss: 3.150172710418701  Test loss: 3.340700626373291 \n",
      "Epoch: 2/10:  mini-batch 868/4459:  Train loss: 3.4708104133605957  Test loss: 3.34112548828125 \n",
      "Epoch: 2/10:  mini-batch 869/4459:  Train loss: 3.256675958633423  Test loss: 3.341707229614258 \n",
      "Epoch: 2/10:  mini-batch 870/4459:  Train loss: 3.251378059387207  Test loss: 3.342177629470825 \n",
      "Epoch: 2/10:  mini-batch 871/4459:  Train loss: 3.6861796379089355  Test loss: 3.342616081237793 \n",
      "Epoch: 2/10:  mini-batch 872/4459:  Train loss: 3.06673526763916  Test loss: 3.3430962562561035 \n",
      "Epoch: 2/10:  mini-batch 873/4459:  Train loss: 3.3300623893737793  Test loss: 3.3435416221618652 \n",
      "Epoch: 2/10:  mini-batch 874/4459:  Train loss: 3.147860288619995  Test loss: 3.3438267707824707 \n",
      "Epoch: 2/10:  mini-batch 875/4459:  Train loss: 2.9964630603790283  Test loss: 3.3441238403320312 \n",
      "Epoch: 2/10:  mini-batch 876/4459:  Train loss: 3.065548896789551  Test loss: 3.3443589210510254 \n",
      "Epoch: 2/10:  mini-batch 877/4459:  Train loss: 3.098834991455078  Test loss: 3.3447022438049316 \n",
      "Epoch: 2/10:  mini-batch 878/4459:  Train loss: 3.1982860565185547  Test loss: 3.3452467918395996 \n",
      "Epoch: 2/10:  mini-batch 879/4459:  Train loss: 3.1387557983398438  Test loss: 3.3459160327911377 \n",
      "Epoch: 2/10:  mini-batch 880/4459:  Train loss: 3.7894721031188965  Test loss: 3.346778154373169 \n",
      "Epoch: 2/10:  mini-batch 881/4459:  Train loss: 3.6250085830688477  Test loss: 3.347402334213257 \n",
      "Epoch: 2/10:  mini-batch 882/4459:  Train loss: 3.184356212615967  Test loss: 3.347973346710205 \n",
      "Epoch: 2/10:  mini-batch 883/4459:  Train loss: 2.980001211166382  Test loss: 3.3484785556793213 \n",
      "Epoch: 2/10:  mini-batch 884/4459:  Train loss: 3.05795955657959  Test loss: 3.348809003829956 \n",
      "Epoch: 2/10:  mini-batch 885/4459:  Train loss: 3.265712261199951  Test loss: 3.3489956855773926 \n",
      "Epoch: 2/10:  mini-batch 886/4459:  Train loss: 2.999159574508667  Test loss: 3.34944748878479 \n",
      "Epoch: 2/10:  mini-batch 887/4459:  Train loss: 3.220787763595581  Test loss: 3.350174903869629 \n",
      "Epoch: 2/10:  mini-batch 888/4459:  Train loss: 3.2656071186065674  Test loss: 3.3507044315338135 \n",
      "Epoch: 2/10:  mini-batch 889/4459:  Train loss: 3.469822406768799  Test loss: 3.350900888442993 \n",
      "Epoch: 2/10:  mini-batch 890/4459:  Train loss: 2.93808650970459  Test loss: 3.351369857788086 \n",
      "Epoch: 2/10:  mini-batch 891/4459:  Train loss: 3.1104085445404053  Test loss: 3.3517932891845703 \n",
      "Epoch: 2/10:  mini-batch 892/4459:  Train loss: 2.820847988128662  Test loss: 3.3526411056518555 \n",
      "Epoch: 2/10:  mini-batch 893/4459:  Train loss: 3.7123124599456787  Test loss: 3.3535420894622803 \n",
      "Epoch: 2/10:  mini-batch 894/4459:  Train loss: 3.1533584594726562  Test loss: 3.3545899391174316 \n",
      "Epoch: 2/10:  mini-batch 895/4459:  Train loss: 3.5361721515655518  Test loss: 3.355260133743286 \n",
      "Epoch: 2/10:  mini-batch 896/4459:  Train loss: 3.064749240875244  Test loss: 3.3559563159942627 \n",
      "Epoch: 2/10:  mini-batch 897/4459:  Train loss: 3.3141930103302  Test loss: 3.3565449714660645 \n",
      "Epoch: 2/10:  mini-batch 898/4459:  Train loss: 3.5141561031341553  Test loss: 3.356586456298828 \n",
      "Epoch: 2/10:  mini-batch 899/4459:  Train loss: 3.457210063934326  Test loss: 3.3565292358398438 \n",
      "Epoch: 2/10:  mini-batch 900/4459:  Train loss: 2.724048614501953  Test loss: 3.3572919368743896 \n",
      "Epoch: 2/10:  mini-batch 901/4459:  Train loss: 3.2774698734283447  Test loss: 3.357537269592285 \n",
      "Epoch: 2/10:  mini-batch 902/4459:  Train loss: 3.4233665466308594  Test loss: 3.3575961589813232 \n",
      "Epoch: 2/10:  mini-batch 903/4459:  Train loss: 3.1728031635284424  Test loss: 3.357546806335449 \n",
      "Epoch: 2/10:  mini-batch 904/4459:  Train loss: 2.933767080307007  Test loss: 3.35762882232666 \n",
      "Epoch: 2/10:  mini-batch 905/4459:  Train loss: 3.0952110290527344  Test loss: 3.3580050468444824 \n",
      "Epoch: 2/10:  mini-batch 906/4459:  Train loss: 3.419250726699829  Test loss: 3.3582115173339844 \n",
      "Epoch: 2/10:  mini-batch 907/4459:  Train loss: 2.9992241859436035  Test loss: 3.3586630821228027 \n",
      "Epoch: 2/10:  mini-batch 908/4459:  Train loss: 3.790459632873535  Test loss: 3.358242988586426 \n",
      "Epoch: 2/10:  mini-batch 909/4459:  Train loss: 3.7123770713806152  Test loss: 3.3572945594787598 \n",
      "Epoch: 2/10:  mini-batch 910/4459:  Train loss: 3.1354572772979736  Test loss: 3.356553316116333 \n",
      "Epoch: 2/10:  mini-batch 911/4459:  Train loss: 3.5679268836975098  Test loss: 3.3551011085510254 \n",
      "Epoch: 2/10:  mini-batch 912/4459:  Train loss: 3.677337169647217  Test loss: 3.353389263153076 \n",
      "Epoch: 2/10:  mini-batch 913/4459:  Train loss: 3.547325611114502  Test loss: 3.3516526222229004 \n",
      "Epoch: 2/10:  mini-batch 914/4459:  Train loss: 3.4617667198181152  Test loss: 3.349581003189087 \n",
      "Epoch: 2/10:  mini-batch 915/4459:  Train loss: 3.7224555015563965  Test loss: 3.347778081893921 \n",
      "Epoch: 2/10:  mini-batch 916/4459:  Train loss: 3.502030849456787  Test loss: 3.3463823795318604 \n",
      "Epoch: 2/10:  mini-batch 917/4459:  Train loss: 3.183800220489502  Test loss: 3.3452842235565186 \n",
      "Epoch: 2/10:  mini-batch 918/4459:  Train loss: 3.4826817512512207  Test loss: 3.3443894386291504 \n",
      "Epoch: 2/10:  mini-batch 919/4459:  Train loss: 3.6637284755706787  Test loss: 3.343636989593506 \n",
      "Epoch: 2/10:  mini-batch 920/4459:  Train loss: 3.096059799194336  Test loss: 3.343334913253784 \n",
      "Epoch: 2/10:  mini-batch 921/4459:  Train loss: 2.9489269256591797  Test loss: 3.343341588973999 \n",
      "Epoch: 2/10:  mini-batch 922/4459:  Train loss: 2.98701810836792  Test loss: 3.3433430194854736 \n",
      "Epoch: 2/10:  mini-batch 923/4459:  Train loss: 2.832841396331787  Test loss: 3.3437376022338867 \n",
      "Epoch: 2/10:  mini-batch 924/4459:  Train loss: 3.380966901779175  Test loss: 3.344033718109131 \n",
      "Epoch: 2/10:  mini-batch 925/4459:  Train loss: 3.178114414215088  Test loss: 3.34429931640625 \n",
      "Epoch: 2/10:  mini-batch 926/4459:  Train loss: 3.2074546813964844  Test loss: 3.3446407318115234 \n",
      "Epoch: 2/10:  mini-batch 927/4459:  Train loss: 3.811830759048462  Test loss: 3.3450381755828857 \n",
      "Epoch: 2/10:  mini-batch 928/4459:  Train loss: 3.249352216720581  Test loss: 3.345405101776123 \n",
      "Epoch: 2/10:  mini-batch 929/4459:  Train loss: 3.4361941814422607  Test loss: 3.3456108570098877 \n",
      "Epoch: 2/10:  mini-batch 930/4459:  Train loss: 3.1748242378234863  Test loss: 3.3457987308502197 \n",
      "Epoch: 2/10:  mini-batch 931/4459:  Train loss: 3.2939131259918213  Test loss: 3.3458023071289062 \n",
      "Epoch: 2/10:  mini-batch 932/4459:  Train loss: 3.316634178161621  Test loss: 3.3457345962524414 \n",
      "Epoch: 2/10:  mini-batch 933/4459:  Train loss: 3.261108636856079  Test loss: 3.345785140991211 \n",
      "Epoch: 2/10:  mini-batch 934/4459:  Train loss: 3.6389992237091064  Test loss: 3.345858573913574 \n",
      "Epoch: 2/10:  mini-batch 935/4459:  Train loss: 2.9613728523254395  Test loss: 3.3461849689483643 \n",
      "Epoch: 2/10:  mini-batch 936/4459:  Train loss: 3.1944022178649902  Test loss: 3.3465042114257812 \n",
      "Epoch: 2/10:  mini-batch 937/4459:  Train loss: 3.0228710174560547  Test loss: 3.3466978073120117 \n",
      "Epoch: 2/10:  mini-batch 938/4459:  Train loss: 3.2262802124023438  Test loss: 3.3471240997314453 \n",
      "Epoch: 2/10:  mini-batch 939/4459:  Train loss: 3.3754312992095947  Test loss: 3.3473920822143555 \n",
      "Epoch: 2/10:  mini-batch 940/4459:  Train loss: 2.9483790397644043  Test loss: 3.3479321002960205 \n",
      "Epoch: 2/10:  mini-batch 941/4459:  Train loss: 3.339545249938965  Test loss: 3.3482813835144043 \n",
      "Epoch: 2/10:  mini-batch 942/4459:  Train loss: 2.9111392498016357  Test loss: 3.348989486694336 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 943/4459:  Train loss: 3.3143482208251953  Test loss: 3.3496546745300293 \n",
      "Epoch: 2/10:  mini-batch 944/4459:  Train loss: 2.990583896636963  Test loss: 3.3505334854125977 \n",
      "Epoch: 2/10:  mini-batch 945/4459:  Train loss: 2.824977397918701  Test loss: 3.3519723415374756 \n",
      "Epoch: 2/10:  mini-batch 946/4459:  Train loss: 3.297872543334961  Test loss: 3.3533802032470703 \n",
      "Epoch: 2/10:  mini-batch 947/4459:  Train loss: 3.395092010498047  Test loss: 3.35469913482666 \n",
      "Epoch: 2/10:  mini-batch 948/4459:  Train loss: 2.872471570968628  Test loss: 3.356651544570923 \n",
      "Epoch: 2/10:  mini-batch 949/4459:  Train loss: 3.1244778633117676  Test loss: 3.3584394454956055 \n",
      "Epoch: 2/10:  mini-batch 950/4459:  Train loss: 3.216810464859009  Test loss: 3.3599109649658203 \n",
      "Epoch: 2/10:  mini-batch 951/4459:  Train loss: 3.4105405807495117  Test loss: 3.3608267307281494 \n",
      "Epoch: 2/10:  mini-batch 952/4459:  Train loss: 3.097036600112915  Test loss: 3.3621745109558105 \n",
      "Epoch: 2/10:  mini-batch 953/4459:  Train loss: 2.9129910469055176  Test loss: 3.3641762733459473 \n",
      "Epoch: 2/10:  mini-batch 954/4459:  Train loss: 3.446027994155884  Test loss: 3.365774631500244 \n",
      "Epoch: 2/10:  mini-batch 955/4459:  Train loss: 3.3415088653564453  Test loss: 3.366849660873413 \n",
      "Epoch: 2/10:  mini-batch 956/4459:  Train loss: 3.35615611076355  Test loss: 3.3677327632904053 \n",
      "Epoch: 2/10:  mini-batch 957/4459:  Train loss: 3.1956610679626465  Test loss: 3.368541717529297 \n",
      "Epoch: 2/10:  mini-batch 958/4459:  Train loss: 3.2037878036499023  Test loss: 3.3694405555725098 \n",
      "Epoch: 2/10:  mini-batch 959/4459:  Train loss: 3.1160998344421387  Test loss: 3.370361804962158 \n",
      "Epoch: 2/10:  mini-batch 960/4459:  Train loss: 3.1516313552856445  Test loss: 3.3716025352478027 \n",
      "Epoch: 2/10:  mini-batch 961/4459:  Train loss: 3.231020450592041  Test loss: 3.3723998069763184 \n",
      "Epoch: 2/10:  mini-batch 962/4459:  Train loss: 3.748316526412964  Test loss: 3.3718442916870117 \n",
      "Epoch: 2/10:  mini-batch 963/4459:  Train loss: 3.3901195526123047  Test loss: 3.3710649013519287 \n",
      "Epoch: 2/10:  mini-batch 964/4459:  Train loss: 3.3798928260803223  Test loss: 3.369375705718994 \n",
      "Epoch: 2/10:  mini-batch 965/4459:  Train loss: 3.5088424682617188  Test loss: 3.367372512817383 \n",
      "Epoch: 2/10:  mini-batch 966/4459:  Train loss: 3.0192294120788574  Test loss: 3.366180181503296 \n",
      "Epoch: 2/10:  mini-batch 967/4459:  Train loss: 3.515244960784912  Test loss: 3.364567756652832 \n",
      "Epoch: 2/10:  mini-batch 968/4459:  Train loss: 3.449845314025879  Test loss: 3.3628525733947754 \n",
      "Epoch: 2/10:  mini-batch 969/4459:  Train loss: 3.740879535675049  Test loss: 3.360386848449707 \n",
      "Epoch: 2/10:  mini-batch 970/4459:  Train loss: 4.025224208831787  Test loss: 3.357269763946533 \n",
      "Epoch: 2/10:  mini-batch 971/4459:  Train loss: 3.503694772720337  Test loss: 3.354891538619995 \n",
      "Epoch: 2/10:  mini-batch 972/4459:  Train loss: 3.3394651412963867  Test loss: 3.3529839515686035 \n",
      "Epoch: 2/10:  mini-batch 973/4459:  Train loss: 2.881908893585205  Test loss: 3.352116107940674 \n",
      "Epoch: 2/10:  mini-batch 974/4459:  Train loss: 3.7013463973999023  Test loss: 3.3510074615478516 \n",
      "Epoch: 2/10:  mini-batch 975/4459:  Train loss: 3.2653021812438965  Test loss: 3.3502578735351562 \n",
      "Epoch: 2/10:  mini-batch 976/4459:  Train loss: 3.102330446243286  Test loss: 3.349165916442871 \n",
      "Epoch: 2/10:  mini-batch 977/4459:  Train loss: 2.884392261505127  Test loss: 3.348393201828003 \n",
      "Epoch: 2/10:  mini-batch 978/4459:  Train loss: 2.951923131942749  Test loss: 3.347923755645752 \n",
      "Epoch: 2/10:  mini-batch 979/4459:  Train loss: 3.4649593830108643  Test loss: 3.3474864959716797 \n",
      "Epoch: 2/10:  mini-batch 980/4459:  Train loss: 3.2248291969299316  Test loss: 3.3472487926483154 \n",
      "Epoch: 2/10:  mini-batch 981/4459:  Train loss: 2.9007785320281982  Test loss: 3.347317695617676 \n",
      "Epoch: 2/10:  mini-batch 982/4459:  Train loss: 3.1137499809265137  Test loss: 3.347391128540039 \n",
      "Epoch: 2/10:  mini-batch 983/4459:  Train loss: 3.3861541748046875  Test loss: 3.3474068641662598 \n",
      "Epoch: 2/10:  mini-batch 984/4459:  Train loss: 3.387293815612793  Test loss: 3.3475229740142822 \n",
      "Epoch: 2/10:  mini-batch 985/4459:  Train loss: 3.4452946186065674  Test loss: 3.3475279808044434 \n",
      "Epoch: 2/10:  mini-batch 986/4459:  Train loss: 3.3799705505371094  Test loss: 3.347520589828491 \n",
      "Epoch: 2/10:  mini-batch 987/4459:  Train loss: 3.2818078994750977  Test loss: 3.3474791049957275 \n",
      "Epoch: 2/10:  mini-batch 988/4459:  Train loss: 3.648014545440674  Test loss: 3.3473739624023438 \n",
      "Epoch: 2/10:  mini-batch 989/4459:  Train loss: 3.3793811798095703  Test loss: 3.3470335006713867 \n",
      "Epoch: 2/10:  mini-batch 990/4459:  Train loss: 3.0734548568725586  Test loss: 3.3465332984924316 \n",
      "Epoch: 2/10:  mini-batch 991/4459:  Train loss: 3.4082388877868652  Test loss: 3.3460373878479004 \n",
      "Epoch: 2/10:  mini-batch 992/4459:  Train loss: 3.2070257663726807  Test loss: 3.345939874649048 \n",
      "Epoch: 2/10:  mini-batch 993/4459:  Train loss: 3.0902204513549805  Test loss: 3.34574031829834 \n",
      "Epoch: 2/10:  mini-batch 994/4459:  Train loss: 3.269378185272217  Test loss: 3.3454511165618896 \n",
      "Epoch: 2/10:  mini-batch 995/4459:  Train loss: 3.487534999847412  Test loss: 3.344926357269287 \n",
      "Epoch: 2/10:  mini-batch 996/4459:  Train loss: 3.6455953121185303  Test loss: 3.3444762229919434 \n",
      "Epoch: 2/10:  mini-batch 997/4459:  Train loss: 3.6501832008361816  Test loss: 3.3440423011779785 \n",
      "Epoch: 2/10:  mini-batch 998/4459:  Train loss: 3.2698988914489746  Test loss: 3.3434865474700928 \n",
      "Epoch: 2/10:  mini-batch 999/4459:  Train loss: 3.2831578254699707  Test loss: 3.3431148529052734 \n",
      "Epoch: 2/10:  mini-batch 1000/4459:  Train loss: 3.184112310409546  Test loss: 3.3429675102233887 \n",
      "Epoch: 2/10:  mini-batch 1001/4459:  Train loss: 3.203263282775879  Test loss: 3.3428335189819336 \n",
      "Epoch: 2/10:  mini-batch 1002/4459:  Train loss: 3.3627583980560303  Test loss: 3.342414379119873 \n",
      "Epoch: 2/10:  mini-batch 1003/4459:  Train loss: 2.9407782554626465  Test loss: 3.3421852588653564 \n",
      "Epoch: 2/10:  mini-batch 1004/4459:  Train loss: 3.1317005157470703  Test loss: 3.3423092365264893 \n",
      "Epoch: 2/10:  mini-batch 1005/4459:  Train loss: 3.3582797050476074  Test loss: 3.3420510292053223 \n",
      "Epoch: 2/10:  mini-batch 1006/4459:  Train loss: 3.4884283542633057  Test loss: 3.341771125793457 \n",
      "Epoch: 2/10:  mini-batch 1007/4459:  Train loss: 3.0206170082092285  Test loss: 3.3416566848754883 \n",
      "Epoch: 2/10:  mini-batch 1008/4459:  Train loss: 3.1309895515441895  Test loss: 3.3411197662353516 \n",
      "Epoch: 2/10:  mini-batch 1009/4459:  Train loss: 2.962693452835083  Test loss: 3.3408002853393555 \n",
      "Epoch: 2/10:  mini-batch 1010/4459:  Train loss: 3.1799418926239014  Test loss: 3.3406355381011963 \n",
      "Epoch: 2/10:  mini-batch 1011/4459:  Train loss: 3.3416805267333984  Test loss: 3.340548038482666 \n",
      "Epoch: 2/10:  mini-batch 1012/4459:  Train loss: 3.2883002758026123  Test loss: 3.3405208587646484 \n",
      "Epoch: 2/10:  mini-batch 1013/4459:  Train loss: 2.8574368953704834  Test loss: 3.3410444259643555 \n",
      "Epoch: 2/10:  mini-batch 1014/4459:  Train loss: 3.2826645374298096  Test loss: 3.3417701721191406 \n",
      "Epoch: 2/10:  mini-batch 1015/4459:  Train loss: 2.956404209136963  Test loss: 3.342729091644287 \n",
      "Epoch: 2/10:  mini-batch 1016/4459:  Train loss: 3.2223494052886963  Test loss: 3.3435556888580322 \n",
      "Epoch: 2/10:  mini-batch 1017/4459:  Train loss: 3.0952820777893066  Test loss: 3.344677448272705 \n",
      "Epoch: 2/10:  mini-batch 1018/4459:  Train loss: 2.7550864219665527  Test loss: 3.3465614318847656 \n",
      "Epoch: 2/10:  mini-batch 1019/4459:  Train loss: 3.1235597133636475  Test loss: 3.3482539653778076 \n",
      "Epoch: 2/10:  mini-batch 1020/4459:  Train loss: 3.5142526626586914  Test loss: 3.349419355392456 \n",
      "Epoch: 2/10:  mini-batch 1021/4459:  Train loss: 3.5713205337524414  Test loss: 3.3501789569854736 \n",
      "Epoch: 2/10:  mini-batch 1022/4459:  Train loss: 3.4395241737365723  Test loss: 3.3509573936462402 \n",
      "Epoch: 2/10:  mini-batch 1023/4459:  Train loss: 3.3065929412841797  Test loss: 3.351405382156372 \n",
      "Epoch: 2/10:  mini-batch 1024/4459:  Train loss: 3.204068422317505  Test loss: 3.3517425060272217 \n",
      "Epoch: 2/10:  mini-batch 1025/4459:  Train loss: 3.481998920440674  Test loss: 3.3518872261047363 \n",
      "Epoch: 2/10:  mini-batch 1026/4459:  Train loss: 3.238203525543213  Test loss: 3.351724147796631 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1027/4459:  Train loss: 3.201732635498047  Test loss: 3.351174831390381 \n",
      "Epoch: 2/10:  mini-batch 1028/4459:  Train loss: 3.0378403663635254  Test loss: 3.350417137145996 \n",
      "Epoch: 2/10:  mini-batch 1029/4459:  Train loss: 3.7768499851226807  Test loss: 3.3490729331970215 \n",
      "Epoch: 2/10:  mini-batch 1030/4459:  Train loss: 3.7201004028320312  Test loss: 3.3475499153137207 \n",
      "Epoch: 2/10:  mini-batch 1031/4459:  Train loss: 3.8582887649536133  Test loss: 3.345818042755127 \n",
      "Epoch: 2/10:  mini-batch 1032/4459:  Train loss: 3.348710060119629  Test loss: 3.344409465789795 \n",
      "Epoch: 2/10:  mini-batch 1033/4459:  Train loss: 3.135072708129883  Test loss: 3.3435122966766357 \n",
      "Epoch: 2/10:  mini-batch 1034/4459:  Train loss: 3.3411307334899902  Test loss: 3.342550277709961 \n",
      "Epoch: 2/10:  mini-batch 1035/4459:  Train loss: 3.358473300933838  Test loss: 3.3415727615356445 \n",
      "Epoch: 2/10:  mini-batch 1036/4459:  Train loss: 2.985257625579834  Test loss: 3.340855121612549 \n",
      "Epoch: 2/10:  mini-batch 1037/4459:  Train loss: 3.000385284423828  Test loss: 3.3403148651123047 \n",
      "Epoch: 2/10:  mini-batch 1038/4459:  Train loss: 3.2307074069976807  Test loss: 3.339806079864502 \n",
      "Epoch: 2/10:  mini-batch 1039/4459:  Train loss: 3.2266011238098145  Test loss: 3.339085578918457 \n",
      "Epoch: 2/10:  mini-batch 1040/4459:  Train loss: 3.560842275619507  Test loss: 3.338125705718994 \n",
      "Epoch: 2/10:  mini-batch 1041/4459:  Train loss: 3.0653443336486816  Test loss: 3.3373284339904785 \n",
      "Epoch: 2/10:  mini-batch 1042/4459:  Train loss: 3.7881362438201904  Test loss: 3.336719512939453 \n",
      "Epoch: 2/10:  mini-batch 1043/4459:  Train loss: 3.5249791145324707  Test loss: 3.3359856605529785 \n",
      "Epoch: 2/10:  mini-batch 1044/4459:  Train loss: 3.9260244369506836  Test loss: 3.3354151248931885 \n",
      "Epoch: 2/10:  mini-batch 1045/4459:  Train loss: 3.7646260261535645  Test loss: 3.334719657897949 \n",
      "Epoch: 2/10:  mini-batch 1046/4459:  Train loss: 3.0995702743530273  Test loss: 3.3341479301452637 \n",
      "Epoch: 2/10:  mini-batch 1047/4459:  Train loss: 3.412653923034668  Test loss: 3.333610773086548 \n",
      "Epoch: 2/10:  mini-batch 1048/4459:  Train loss: 3.1149215698242188  Test loss: 3.333167552947998 \n",
      "Epoch: 2/10:  mini-batch 1049/4459:  Train loss: 3.1419386863708496  Test loss: 3.3328795433044434 \n",
      "Epoch: 2/10:  mini-batch 1050/4459:  Train loss: 3.7233009338378906  Test loss: 3.3324379920959473 \n",
      "Epoch: 2/10:  mini-batch 1051/4459:  Train loss: 3.656311273574829  Test loss: 3.3324837684631348 \n",
      "Epoch: 2/10:  mini-batch 1052/4459:  Train loss: 2.7814691066741943  Test loss: 3.332793712615967 \n",
      "Epoch: 2/10:  mini-batch 1053/4459:  Train loss: 2.9488866329193115  Test loss: 3.3330588340759277 \n",
      "Epoch: 2/10:  mini-batch 1054/4459:  Train loss: 3.1339616775512695  Test loss: 3.3331170082092285 \n",
      "Epoch: 2/10:  mini-batch 1055/4459:  Train loss: 3.44852352142334  Test loss: 3.333395481109619 \n",
      "Epoch: 2/10:  mini-batch 1056/4459:  Train loss: 3.5892868041992188  Test loss: 3.3335776329040527 \n",
      "Epoch: 2/10:  mini-batch 1057/4459:  Train loss: 3.2352852821350098  Test loss: 3.333519458770752 \n",
      "Epoch: 2/10:  mini-batch 1058/4459:  Train loss: 3.3596854209899902  Test loss: 3.333526134490967 \n",
      "Epoch: 2/10:  mini-batch 1059/4459:  Train loss: 3.2713794708251953  Test loss: 3.3333499431610107 \n",
      "Epoch: 2/10:  mini-batch 1060/4459:  Train loss: 3.2051377296447754  Test loss: 3.3332295417785645 \n",
      "Epoch: 2/10:  mini-batch 1061/4459:  Train loss: 3.5915980339050293  Test loss: 3.333223581314087 \n",
      "Epoch: 2/10:  mini-batch 1062/4459:  Train loss: 3.2343695163726807  Test loss: 3.3328423500061035 \n",
      "Epoch: 2/10:  mini-batch 1063/4459:  Train loss: 3.6918938159942627  Test loss: 3.332669258117676 \n",
      "Epoch: 2/10:  mini-batch 1064/4459:  Train loss: 3.6147303581237793  Test loss: 3.3327083587646484 \n",
      "Epoch: 2/10:  mini-batch 1065/4459:  Train loss: 2.931490659713745  Test loss: 3.3328030109405518 \n",
      "Epoch: 2/10:  mini-batch 1066/4459:  Train loss: 3.6150035858154297  Test loss: 3.33315372467041 \n",
      "Epoch: 2/10:  mini-batch 1067/4459:  Train loss: 3.2786247730255127  Test loss: 3.3334243297576904 \n",
      "Epoch: 2/10:  mini-batch 1068/4459:  Train loss: 3.2595486640930176  Test loss: 3.3335225582122803 \n",
      "Epoch: 2/10:  mini-batch 1069/4459:  Train loss: 3.325632095336914  Test loss: 3.333662271499634 \n",
      "Epoch: 2/10:  mini-batch 1070/4459:  Train loss: 2.773994207382202  Test loss: 3.3337552547454834 \n",
      "Epoch: 2/10:  mini-batch 1071/4459:  Train loss: 3.2373909950256348  Test loss: 3.333740711212158 \n",
      "Epoch: 2/10:  mini-batch 1072/4459:  Train loss: 3.5493383407592773  Test loss: 3.333716630935669 \n",
      "Epoch: 2/10:  mini-batch 1073/4459:  Train loss: 3.049471378326416  Test loss: 3.333303689956665 \n",
      "Epoch: 2/10:  mini-batch 1074/4459:  Train loss: 3.251096248626709  Test loss: 3.3327064514160156 \n",
      "Epoch: 2/10:  mini-batch 1075/4459:  Train loss: 3.5096113681793213  Test loss: 3.331735372543335 \n",
      "Epoch: 2/10:  mini-batch 1076/4459:  Train loss: 3.2301249504089355  Test loss: 3.3308300971984863 \n",
      "Epoch: 2/10:  mini-batch 1077/4459:  Train loss: 2.9694459438323975  Test loss: 3.330136775970459 \n",
      "Epoch: 2/10:  mini-batch 1078/4459:  Train loss: 3.8115553855895996  Test loss: 3.329709053039551 \n",
      "Epoch: 2/10:  mini-batch 1079/4459:  Train loss: 3.587397336959839  Test loss: 3.329542636871338 \n",
      "Epoch: 2/10:  mini-batch 1080/4459:  Train loss: 3.162574291229248  Test loss: 3.329526424407959 \n",
      "Epoch: 2/10:  mini-batch 1081/4459:  Train loss: 3.197114944458008  Test loss: 3.3295202255249023 \n",
      "Epoch: 2/10:  mini-batch 1082/4459:  Train loss: 3.5444607734680176  Test loss: 3.329741954803467 \n",
      "Epoch: 2/10:  mini-batch 1083/4459:  Train loss: 3.449575424194336  Test loss: 3.3296399116516113 \n",
      "Epoch: 2/10:  mini-batch 1084/4459:  Train loss: 3.160729169845581  Test loss: 3.329404354095459 \n",
      "Epoch: 2/10:  mini-batch 1085/4459:  Train loss: 3.4492931365966797  Test loss: 3.3290350437164307 \n",
      "Epoch: 2/10:  mini-batch 1086/4459:  Train loss: 2.7684121131896973  Test loss: 3.328747272491455 \n",
      "Epoch: 2/10:  mini-batch 1087/4459:  Train loss: 3.531797170639038  Test loss: 3.328920602798462 \n",
      "Epoch: 2/10:  mini-batch 1088/4459:  Train loss: 3.7235703468322754  Test loss: 3.328949213027954 \n",
      "Epoch: 2/10:  mini-batch 1089/4459:  Train loss: 3.6493964195251465  Test loss: 3.329014778137207 \n",
      "Epoch: 2/10:  mini-batch 1090/4459:  Train loss: 3.778414249420166  Test loss: 3.3294057846069336 \n",
      "Epoch: 2/10:  mini-batch 1091/4459:  Train loss: 3.6842401027679443  Test loss: 3.33018159866333 \n",
      "Epoch: 2/10:  mini-batch 1092/4459:  Train loss: 3.5834710597991943  Test loss: 3.3311023712158203 \n",
      "Epoch: 2/10:  mini-batch 1093/4459:  Train loss: 3.482089042663574  Test loss: 3.3318257331848145 \n",
      "Epoch: 2/10:  mini-batch 1094/4459:  Train loss: 3.1869711875915527  Test loss: 3.3322367668151855 \n",
      "Epoch: 2/10:  mini-batch 1095/4459:  Train loss: 3.3700177669525146  Test loss: 3.3325226306915283 \n",
      "Epoch: 2/10:  mini-batch 1096/4459:  Train loss: 3.1307625770568848  Test loss: 3.3324036598205566 \n",
      "Epoch: 2/10:  mini-batch 1097/4459:  Train loss: 3.235114097595215  Test loss: 3.332080841064453 \n",
      "Epoch: 2/10:  mini-batch 1098/4459:  Train loss: 3.7692179679870605  Test loss: 3.332143545150757 \n",
      "Epoch: 2/10:  mini-batch 1099/4459:  Train loss: 2.9894919395446777  Test loss: 3.3320980072021484 \n",
      "Epoch: 2/10:  mini-batch 1100/4459:  Train loss: 3.6121935844421387  Test loss: 3.3323686122894287 \n",
      "Epoch: 2/10:  mini-batch 1101/4459:  Train loss: 3.178252696990967  Test loss: 3.332448959350586 \n",
      "Epoch: 2/10:  mini-batch 1102/4459:  Train loss: 3.3506531715393066  Test loss: 3.3325517177581787 \n",
      "Epoch: 2/10:  mini-batch 1103/4459:  Train loss: 3.3909575939178467  Test loss: 3.3324708938598633 \n",
      "Epoch: 2/10:  mini-batch 1104/4459:  Train loss: 3.216508626937866  Test loss: 3.3324191570281982 \n",
      "Epoch: 2/10:  mini-batch 1105/4459:  Train loss: 3.372295379638672  Test loss: 3.3325860500335693 \n",
      "Epoch: 2/10:  mini-batch 1106/4459:  Train loss: 3.6124160289764404  Test loss: 3.333082675933838 \n",
      "Epoch: 2/10:  mini-batch 1107/4459:  Train loss: 3.3798987865448  Test loss: 3.333587408065796 \n",
      "Epoch: 2/10:  mini-batch 1108/4459:  Train loss: 3.4252705574035645  Test loss: 3.3341732025146484 \n",
      "Epoch: 2/10:  mini-batch 1109/4459:  Train loss: 3.628880739212036  Test loss: 3.3347673416137695 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1110/4459:  Train loss: 3.5980215072631836  Test loss: 3.3355202674865723 \n",
      "Epoch: 2/10:  mini-batch 1111/4459:  Train loss: 3.3017702102661133  Test loss: 3.3362932205200195 \n",
      "Epoch: 2/10:  mini-batch 1112/4459:  Train loss: 3.882594347000122  Test loss: 3.3375842571258545 \n",
      "Epoch: 2/10:  mini-batch 1113/4459:  Train loss: 3.4419772624969482  Test loss: 3.3386917114257812 \n",
      "Epoch: 2/10:  mini-batch 1114/4459:  Train loss: 3.563776731491089  Test loss: 3.339663028717041 \n",
      "Epoch: 2/10:  mini-batch 1115/4459:  Train loss: 3.4199204444885254  Test loss: 3.3403663635253906 \n",
      "Epoch: 2/10:  mini-batch 1116/4459:  Train loss: 3.3774590492248535  Test loss: 3.3411107063293457 \n",
      "Epoch: 2/10:  mini-batch 1117/4459:  Train loss: 3.229036331176758  Test loss: 3.341608762741089 \n",
      "Epoch: 2/10:  mini-batch 1118/4459:  Train loss: 3.703627347946167  Test loss: 3.342083215713501 \n",
      "Epoch: 2/10:  mini-batch 1119/4459:  Train loss: 3.5293424129486084  Test loss: 3.342794418334961 \n",
      "Epoch: 2/10:  mini-batch 1120/4459:  Train loss: 3.2966153621673584  Test loss: 3.3433189392089844 \n",
      "Epoch: 2/10:  mini-batch 1121/4459:  Train loss: 3.3734958171844482  Test loss: 3.343827724456787 \n",
      "Epoch: 2/10:  mini-batch 1122/4459:  Train loss: 3.47341251373291  Test loss: 3.3444933891296387 \n",
      "Epoch: 2/10:  mini-batch 1123/4459:  Train loss: 3.500241756439209  Test loss: 3.345076560974121 \n",
      "Epoch: 2/10:  mini-batch 1124/4459:  Train loss: 3.452439308166504  Test loss: 3.3453550338745117 \n",
      "Epoch: 2/10:  mini-batch 1125/4459:  Train loss: 3.5158278942108154  Test loss: 3.3454887866973877 \n",
      "Epoch: 2/10:  mini-batch 1126/4459:  Train loss: 3.357351779937744  Test loss: 3.3454766273498535 \n",
      "Epoch: 2/10:  mini-batch 1127/4459:  Train loss: 3.594444751739502  Test loss: 3.345533847808838 \n",
      "Epoch: 2/10:  mini-batch 1128/4459:  Train loss: 3.4214844703674316  Test loss: 3.345402717590332 \n",
      "Epoch: 2/10:  mini-batch 1129/4459:  Train loss: 3.447065830230713  Test loss: 3.3454067707061768 \n",
      "Epoch: 2/10:  mini-batch 1130/4459:  Train loss: 3.2411623001098633  Test loss: 3.3452768325805664 \n",
      "Epoch: 2/10:  mini-batch 1131/4459:  Train loss: 3.2905187606811523  Test loss: 3.3449604511260986 \n",
      "Epoch: 2/10:  mini-batch 1132/4459:  Train loss: 3.2994062900543213  Test loss: 3.344576835632324 \n",
      "Epoch: 2/10:  mini-batch 1133/4459:  Train loss: 3.3516592979431152  Test loss: 3.343931198120117 \n",
      "Epoch: 2/10:  mini-batch 1134/4459:  Train loss: 3.512315034866333  Test loss: 3.3434455394744873 \n",
      "Epoch: 2/10:  mini-batch 1135/4459:  Train loss: 3.6010453701019287  Test loss: 3.343430995941162 \n",
      "Epoch: 2/10:  mini-batch 1136/4459:  Train loss: 3.5088369846343994  Test loss: 3.3434696197509766 \n",
      "Epoch: 2/10:  mini-batch 1137/4459:  Train loss: 3.596879005432129  Test loss: 3.3437960147857666 \n",
      "Epoch: 2/10:  mini-batch 1138/4459:  Train loss: 3.4494988918304443  Test loss: 3.344104528427124 \n",
      "Epoch: 2/10:  mini-batch 1139/4459:  Train loss: 3.514315366744995  Test loss: 3.344460964202881 \n",
      "Epoch: 2/10:  mini-batch 1140/4459:  Train loss: 3.530431032180786  Test loss: 3.3449347019195557 \n",
      "Epoch: 2/10:  mini-batch 1141/4459:  Train loss: 3.387439727783203  Test loss: 3.3453710079193115 \n",
      "Epoch: 2/10:  mini-batch 1142/4459:  Train loss: 3.389064073562622  Test loss: 3.3454689979553223 \n",
      "Epoch: 2/10:  mini-batch 1143/4459:  Train loss: 3.24396014213562  Test loss: 3.345285177230835 \n",
      "Epoch: 2/10:  mini-batch 1144/4459:  Train loss: 3.6261258125305176  Test loss: 3.345489025115967 \n",
      "Epoch: 2/10:  mini-batch 1145/4459:  Train loss: 3.414053440093994  Test loss: 3.3457417488098145 \n",
      "Epoch: 2/10:  mini-batch 1146/4459:  Train loss: 3.2815427780151367  Test loss: 3.345867156982422 \n",
      "Epoch: 2/10:  mini-batch 1147/4459:  Train loss: 3.299572706222534  Test loss: 3.345907211303711 \n",
      "Epoch: 2/10:  mini-batch 1148/4459:  Train loss: 3.418911933898926  Test loss: 3.3459463119506836 \n",
      "Epoch: 2/10:  mini-batch 1149/4459:  Train loss: 3.5757899284362793  Test loss: 3.345978260040283 \n",
      "Epoch: 2/10:  mini-batch 1150/4459:  Train loss: 3.5020556449890137  Test loss: 3.3460211753845215 \n",
      "Epoch: 2/10:  mini-batch 1151/4459:  Train loss: 3.267073154449463  Test loss: 3.3457658290863037 \n",
      "Epoch: 2/10:  mini-batch 1152/4459:  Train loss: 3.3320391178131104  Test loss: 3.345329999923706 \n",
      "Epoch: 2/10:  mini-batch 1153/4459:  Train loss: 3.396821975708008  Test loss: 3.344839096069336 \n",
      "Epoch: 2/10:  mini-batch 1154/4459:  Train loss: 3.48590087890625  Test loss: 3.344388961791992 \n",
      "Epoch: 2/10:  mini-batch 1155/4459:  Train loss: 3.327915668487549  Test loss: 3.343846321105957 \n",
      "Epoch: 2/10:  mini-batch 1156/4459:  Train loss: 3.4766225814819336  Test loss: 3.34344220161438 \n",
      "Epoch: 2/10:  mini-batch 1157/4459:  Train loss: 3.452901840209961  Test loss: 3.343188524246216 \n",
      "Epoch: 2/10:  mini-batch 1158/4459:  Train loss: 3.319509506225586  Test loss: 3.3426272869110107 \n",
      "Epoch: 2/10:  mini-batch 1159/4459:  Train loss: 3.55996036529541  Test loss: 3.3423445224761963 \n",
      "Epoch: 2/10:  mini-batch 1160/4459:  Train loss: 3.3814735412597656  Test loss: 3.342099189758301 \n",
      "Epoch: 2/10:  mini-batch 1161/4459:  Train loss: 3.4903242588043213  Test loss: 3.342022180557251 \n",
      "Epoch: 2/10:  mini-batch 1162/4459:  Train loss: 3.4051623344421387  Test loss: 3.342014789581299 \n",
      "Epoch: 2/10:  mini-batch 1163/4459:  Train loss: 3.3507769107818604  Test loss: 3.341947555541992 \n",
      "Epoch: 2/10:  mini-batch 1164/4459:  Train loss: 3.3108932971954346  Test loss: 3.341780424118042 \n",
      "Epoch: 2/10:  mini-batch 1165/4459:  Train loss: 3.446589469909668  Test loss: 3.3415720462799072 \n",
      "Epoch: 2/10:  mini-batch 1166/4459:  Train loss: 3.167374610900879  Test loss: 3.341305732727051 \n",
      "Epoch: 2/10:  mini-batch 1167/4459:  Train loss: 3.498110294342041  Test loss: 3.340919017791748 \n",
      "Epoch: 2/10:  mini-batch 1168/4459:  Train loss: 3.435460090637207  Test loss: 3.340505838394165 \n",
      "Epoch: 2/10:  mini-batch 1169/4459:  Train loss: 3.531808376312256  Test loss: 3.340333938598633 \n",
      "Epoch: 2/10:  mini-batch 1170/4459:  Train loss: 3.5264029502868652  Test loss: 3.340282917022705 \n",
      "Epoch: 2/10:  mini-batch 1171/4459:  Train loss: 3.52130126953125  Test loss: 3.3403944969177246 \n",
      "Epoch: 2/10:  mini-batch 1172/4459:  Train loss: 3.446429491043091  Test loss: 3.340578079223633 \n",
      "Epoch: 2/10:  mini-batch 1173/4459:  Train loss: 3.274563789367676  Test loss: 3.3407673835754395 \n",
      "Epoch: 2/10:  mini-batch 1174/4459:  Train loss: 3.560307025909424  Test loss: 3.3411359786987305 \n",
      "Epoch: 2/10:  mini-batch 1175/4459:  Train loss: 3.133810043334961  Test loss: 3.341179847717285 \n",
      "Epoch: 2/10:  mini-batch 1176/4459:  Train loss: 3.3628101348876953  Test loss: 3.3409423828125 \n",
      "Epoch: 2/10:  mini-batch 1177/4459:  Train loss: 3.6474123001098633  Test loss: 3.341055154800415 \n",
      "Epoch: 2/10:  mini-batch 1178/4459:  Train loss: 3.2105345726013184  Test loss: 3.340867042541504 \n",
      "Epoch: 2/10:  mini-batch 1179/4459:  Train loss: 3.270479202270508  Test loss: 3.340491771697998 \n",
      "Epoch: 2/10:  mini-batch 1180/4459:  Train loss: 3.2376599311828613  Test loss: 3.3399274349212646 \n",
      "Epoch: 2/10:  mini-batch 1181/4459:  Train loss: 3.2347772121429443  Test loss: 3.3394408226013184 \n",
      "Epoch: 2/10:  mini-batch 1182/4459:  Train loss: 3.4954850673675537  Test loss: 3.3390965461730957 \n",
      "Epoch: 2/10:  mini-batch 1183/4459:  Train loss: 3.394472360610962  Test loss: 3.3387861251831055 \n",
      "Epoch: 2/10:  mini-batch 1184/4459:  Train loss: 3.346503257751465  Test loss: 3.3384718894958496 \n",
      "Epoch: 2/10:  mini-batch 1185/4459:  Train loss: 3.474160671234131  Test loss: 3.338273763656616 \n",
      "Epoch: 2/10:  mini-batch 1186/4459:  Train loss: 3.57802677154541  Test loss: 3.3381924629211426 \n",
      "Epoch: 2/10:  mini-batch 1187/4459:  Train loss: 3.353409767150879  Test loss: 3.3381259441375732 \n",
      "Epoch: 2/10:  mini-batch 1188/4459:  Train loss: 3.3363637924194336  Test loss: 3.33772611618042 \n",
      "Epoch: 2/10:  mini-batch 1189/4459:  Train loss: 3.338151454925537  Test loss: 3.3374226093292236 \n",
      "Epoch: 2/10:  mini-batch 1190/4459:  Train loss: 3.3981571197509766  Test loss: 3.337139129638672 \n",
      "Epoch: 2/10:  mini-batch 1191/4459:  Train loss: 3.4222941398620605  Test loss: 3.336991310119629 \n",
      "Epoch: 2/10:  mini-batch 1192/4459:  Train loss: 3.2061243057250977  Test loss: 3.3366780281066895 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1193/4459:  Train loss: 3.412595510482788  Test loss: 3.336310386657715 \n",
      "Epoch: 2/10:  mini-batch 1194/4459:  Train loss: 3.4168589115142822  Test loss: 3.3359484672546387 \n",
      "Epoch: 2/10:  mini-batch 1195/4459:  Train loss: 3.467421531677246  Test loss: 3.3356895446777344 \n",
      "Epoch: 2/10:  mini-batch 1196/4459:  Train loss: 3.1951394081115723  Test loss: 3.3353633880615234 \n",
      "Epoch: 2/10:  mini-batch 1197/4459:  Train loss: 3.472877264022827  Test loss: 3.335205078125 \n",
      "Epoch: 2/10:  mini-batch 1198/4459:  Train loss: 3.5478761196136475  Test loss: 3.335273265838623 \n",
      "Epoch: 2/10:  mini-batch 1199/4459:  Train loss: 3.3379580974578857  Test loss: 3.3351974487304688 \n",
      "Epoch: 2/10:  mini-batch 1200/4459:  Train loss: 3.4835658073425293  Test loss: 3.3351454734802246 \n",
      "Epoch: 2/10:  mini-batch 1201/4459:  Train loss: 3.058183193206787  Test loss: 3.3349575996398926 \n",
      "Epoch: 2/10:  mini-batch 1202/4459:  Train loss: 3.5060200691223145  Test loss: 3.3349790573120117 \n",
      "Epoch: 2/10:  mini-batch 1203/4459:  Train loss: 3.252577781677246  Test loss: 3.334968090057373 \n",
      "Epoch: 2/10:  mini-batch 1204/4459:  Train loss: 3.348972797393799  Test loss: 3.3350319862365723 \n",
      "Epoch: 2/10:  mini-batch 1205/4459:  Train loss: 3.1801812648773193  Test loss: 3.335120677947998 \n",
      "Epoch: 2/10:  mini-batch 1206/4459:  Train loss: 3.2421271800994873  Test loss: 3.3352251052856445 \n",
      "Epoch: 2/10:  mini-batch 1207/4459:  Train loss: 3.5542855262756348  Test loss: 3.3355045318603516 \n",
      "Epoch: 2/10:  mini-batch 1208/4459:  Train loss: 3.38065767288208  Test loss: 3.3358747959136963 \n",
      "Epoch: 2/10:  mini-batch 1209/4459:  Train loss: 3.3317182064056396  Test loss: 3.3363001346588135 \n",
      "Epoch: 2/10:  mini-batch 1210/4459:  Train loss: 3.673722743988037  Test loss: 3.33707332611084 \n",
      "Epoch: 2/10:  mini-batch 1211/4459:  Train loss: 3.6585869789123535  Test loss: 3.3381028175354004 \n",
      "Epoch: 2/10:  mini-batch 1212/4459:  Train loss: 3.353529214859009  Test loss: 3.3390092849731445 \n",
      "Epoch: 2/10:  mini-batch 1213/4459:  Train loss: 3.427464008331299  Test loss: 3.3398795127868652 \n",
      "Epoch: 2/10:  mini-batch 1214/4459:  Train loss: 3.710378646850586  Test loss: 3.340897560119629 \n",
      "Epoch: 2/10:  mini-batch 1215/4459:  Train loss: 3.963379144668579  Test loss: 3.3423125743865967 \n",
      "Epoch: 2/10:  mini-batch 1216/4459:  Train loss: 3.406158924102783  Test loss: 3.3434994220733643 \n",
      "Epoch: 2/10:  mini-batch 1217/4459:  Train loss: 3.286851406097412  Test loss: 3.3443515300750732 \n",
      "Epoch: 2/10:  mini-batch 1218/4459:  Train loss: 3.3066439628601074  Test loss: 3.344853162765503 \n",
      "Epoch: 2/10:  mini-batch 1219/4459:  Train loss: 3.079681396484375  Test loss: 3.3446950912475586 \n",
      "Epoch: 2/10:  mini-batch 1220/4459:  Train loss: 3.2573351860046387  Test loss: 3.3441505432128906 \n",
      "Epoch: 2/10:  mini-batch 1221/4459:  Train loss: 3.0936498641967773  Test loss: 3.3435535430908203 \n",
      "Epoch: 2/10:  mini-batch 1222/4459:  Train loss: 3.4318251609802246  Test loss: 3.343417167663574 \n",
      "Epoch: 2/10:  mini-batch 1223/4459:  Train loss: 3.475496768951416  Test loss: 3.3434293270111084 \n",
      "Epoch: 2/10:  mini-batch 1224/4459:  Train loss: 3.2311315536499023  Test loss: 3.343322277069092 \n",
      "Epoch: 2/10:  mini-batch 1225/4459:  Train loss: 3.2588233947753906  Test loss: 3.3431177139282227 \n",
      "Epoch: 2/10:  mini-batch 1226/4459:  Train loss: 3.4231553077697754  Test loss: 3.34294056892395 \n",
      "Epoch: 2/10:  mini-batch 1227/4459:  Train loss: 3.337782382965088  Test loss: 3.342684268951416 \n",
      "Epoch: 2/10:  mini-batch 1228/4459:  Train loss: 3.4158830642700195  Test loss: 3.342557907104492 \n",
      "Epoch: 2/10:  mini-batch 1229/4459:  Train loss: 3.682077407836914  Test loss: 3.3427023887634277 \n",
      "Epoch: 2/10:  mini-batch 1230/4459:  Train loss: 3.377052068710327  Test loss: 3.3429837226867676 \n",
      "Epoch: 2/10:  mini-batch 1231/4459:  Train loss: 3.48030424118042  Test loss: 3.343179941177368 \n",
      "Epoch: 2/10:  mini-batch 1232/4459:  Train loss: 3.2104225158691406  Test loss: 3.3432726860046387 \n",
      "Epoch: 2/10:  mini-batch 1233/4459:  Train loss: 3.533583641052246  Test loss: 3.3435864448547363 \n",
      "Epoch: 2/10:  mini-batch 1234/4459:  Train loss: 3.4031615257263184  Test loss: 3.3440818786621094 \n",
      "Epoch: 2/10:  mini-batch 1235/4459:  Train loss: 3.298576831817627  Test loss: 3.3444581031799316 \n",
      "Epoch: 2/10:  mini-batch 1236/4459:  Train loss: 3.3248894214630127  Test loss: 3.34474515914917 \n",
      "Epoch: 2/10:  mini-batch 1237/4459:  Train loss: 3.3306283950805664  Test loss: 3.3450562953948975 \n",
      "Epoch: 2/10:  mini-batch 1238/4459:  Train loss: 2.996835231781006  Test loss: 3.345132350921631 \n",
      "Epoch: 2/10:  mini-batch 1239/4459:  Train loss: 3.386617660522461  Test loss: 3.3450028896331787 \n",
      "Epoch: 2/10:  mini-batch 1240/4459:  Train loss: 3.365447521209717  Test loss: 3.344776153564453 \n",
      "Epoch: 2/10:  mini-batch 1241/4459:  Train loss: 3.6245992183685303  Test loss: 3.344846487045288 \n",
      "Epoch: 2/10:  mini-batch 1242/4459:  Train loss: 3.423769235610962  Test loss: 3.3448259830474854 \n",
      "Epoch: 2/10:  mini-batch 1243/4459:  Train loss: 3.3575332164764404  Test loss: 3.344794988632202 \n",
      "Epoch: 2/10:  mini-batch 1244/4459:  Train loss: 3.578049659729004  Test loss: 3.3444437980651855 \n",
      "Epoch: 2/10:  mini-batch 1245/4459:  Train loss: 3.625248432159424  Test loss: 3.3445334434509277 \n",
      "Epoch: 2/10:  mini-batch 1246/4459:  Train loss: 3.3845772743225098  Test loss: 3.344822406768799 \n",
      "Epoch: 2/10:  mini-batch 1247/4459:  Train loss: 3.255516529083252  Test loss: 3.34501576423645 \n",
      "Epoch: 2/10:  mini-batch 1248/4459:  Train loss: 3.2499752044677734  Test loss: 3.3449931144714355 \n",
      "Epoch: 2/10:  mini-batch 1249/4459:  Train loss: 3.215522527694702  Test loss: 3.3449888229370117 \n",
      "Epoch: 2/10:  mini-batch 1250/4459:  Train loss: 3.390929698944092  Test loss: 3.3448007106781006 \n",
      "Epoch: 2/10:  mini-batch 1251/4459:  Train loss: 3.276491403579712  Test loss: 3.344344139099121 \n",
      "Epoch: 2/10:  mini-batch 1252/4459:  Train loss: 3.3649182319641113  Test loss: 3.3436179161071777 \n",
      "Epoch: 2/10:  mini-batch 1253/4459:  Train loss: 3.211973190307617  Test loss: 3.342954158782959 \n",
      "Epoch: 2/10:  mini-batch 1254/4459:  Train loss: 3.456627368927002  Test loss: 3.3426058292388916 \n",
      "Epoch: 2/10:  mini-batch 1255/4459:  Train loss: 3.556135654449463  Test loss: 3.3423447608947754 \n",
      "Epoch: 2/10:  mini-batch 1256/4459:  Train loss: 3.3314547538757324  Test loss: 3.342118263244629 \n",
      "Epoch: 2/10:  mini-batch 1257/4459:  Train loss: 3.3401474952697754  Test loss: 3.3418641090393066 \n",
      "Epoch: 2/10:  mini-batch 1258/4459:  Train loss: 3.235097646713257  Test loss: 3.3416972160339355 \n",
      "Epoch: 2/10:  mini-batch 1259/4459:  Train loss: 3.3803656101226807  Test loss: 3.341538190841675 \n",
      "Epoch: 2/10:  mini-batch 1260/4459:  Train loss: 3.2376046180725098  Test loss: 3.3413162231445312 \n",
      "Epoch: 2/10:  mini-batch 1261/4459:  Train loss: 3.5204715728759766  Test loss: 3.3409066200256348 \n",
      "Epoch: 2/10:  mini-batch 1262/4459:  Train loss: 3.1301846504211426  Test loss: 3.3403847217559814 \n",
      "Epoch: 2/10:  mini-batch 1263/4459:  Train loss: 3.3687171936035156  Test loss: 3.3395137786865234 \n",
      "Epoch: 2/10:  mini-batch 1264/4459:  Train loss: 3.5133066177368164  Test loss: 3.3388736248016357 \n",
      "Epoch: 2/10:  mini-batch 1265/4459:  Train loss: 3.4170479774475098  Test loss: 3.338463306427002 \n",
      "Epoch: 2/10:  mini-batch 1266/4459:  Train loss: 3.343116044998169  Test loss: 3.338146448135376 \n",
      "Epoch: 2/10:  mini-batch 1267/4459:  Train loss: 3.4491734504699707  Test loss: 3.337702751159668 \n",
      "Epoch: 2/10:  mini-batch 1268/4459:  Train loss: 3.5646636486053467  Test loss: 3.337130308151245 \n",
      "Epoch: 2/10:  mini-batch 1269/4459:  Train loss: 3.488356828689575  Test loss: 3.336726188659668 \n",
      "Epoch: 2/10:  mini-batch 1270/4459:  Train loss: 3.5949954986572266  Test loss: 3.336336612701416 \n",
      "Epoch: 2/10:  mini-batch 1271/4459:  Train loss: 3.4229178428649902  Test loss: 3.3359248638153076 \n",
      "Epoch: 2/10:  mini-batch 1272/4459:  Train loss: 3.3215062618255615  Test loss: 3.335465669631958 \n",
      "Epoch: 2/10:  mini-batch 1273/4459:  Train loss: 3.463305711746216  Test loss: 3.3351621627807617 \n",
      "Epoch: 2/10:  mini-batch 1274/4459:  Train loss: 3.2584986686706543  Test loss: 3.3348796367645264 \n",
      "Epoch: 2/10:  mini-batch 1275/4459:  Train loss: 3.341642379760742  Test loss: 3.334498882293701 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1276/4459:  Train loss: 3.348573923110962  Test loss: 3.3342294692993164 \n",
      "Epoch: 2/10:  mini-batch 1277/4459:  Train loss: 3.5527734756469727  Test loss: 3.334110975265503 \n",
      "Epoch: 2/10:  mini-batch 1278/4459:  Train loss: 3.4449548721313477  Test loss: 3.333950996398926 \n",
      "Epoch: 2/10:  mini-batch 1279/4459:  Train loss: 3.188497304916382  Test loss: 3.333512306213379 \n",
      "Epoch: 2/10:  mini-batch 1280/4459:  Train loss: 3.439006805419922  Test loss: 3.3331751823425293 \n",
      "Epoch: 2/10:  mini-batch 1281/4459:  Train loss: 3.399487257003784  Test loss: 3.3328657150268555 \n",
      "Epoch: 2/10:  mini-batch 1282/4459:  Train loss: 3.5821619033813477  Test loss: 3.333009958267212 \n",
      "Epoch: 2/10:  mini-batch 1283/4459:  Train loss: 3.1403422355651855  Test loss: 3.332824230194092 \n",
      "Epoch: 2/10:  mini-batch 1284/4459:  Train loss: 3.1642990112304688  Test loss: 3.3325295448303223 \n",
      "Epoch: 2/10:  mini-batch 1285/4459:  Train loss: 3.0980594158172607  Test loss: 3.3320586681365967 \n",
      "Epoch: 2/10:  mini-batch 1286/4459:  Train loss: 3.3007423877716064  Test loss: 3.331578254699707 \n",
      "Epoch: 2/10:  mini-batch 1287/4459:  Train loss: 3.674959182739258  Test loss: 3.331427574157715 \n",
      "Epoch: 2/10:  mini-batch 1288/4459:  Train loss: 3.6340293884277344  Test loss: 3.3317036628723145 \n",
      "Epoch: 2/10:  mini-batch 1289/4459:  Train loss: 3.275972604751587  Test loss: 3.3320212364196777 \n",
      "Epoch: 2/10:  mini-batch 1290/4459:  Train loss: 3.057237386703491  Test loss: 3.3320648670196533 \n",
      "Epoch: 2/10:  mini-batch 1291/4459:  Train loss: 3.2147343158721924  Test loss: 3.331967830657959 \n",
      "Epoch: 2/10:  mini-batch 1292/4459:  Train loss: 3.072413921356201  Test loss: 3.331883430480957 \n",
      "Epoch: 2/10:  mini-batch 1293/4459:  Train loss: 3.203151226043701  Test loss: 3.331894874572754 \n",
      "Epoch: 2/10:  mini-batch 1294/4459:  Train loss: 3.2766942977905273  Test loss: 3.3319451808929443 \n",
      "Epoch: 2/10:  mini-batch 1295/4459:  Train loss: 3.588148355484009  Test loss: 3.331738233566284 \n",
      "Epoch: 2/10:  mini-batch 1296/4459:  Train loss: 3.461850881576538  Test loss: 3.331390380859375 \n",
      "Epoch: 2/10:  mini-batch 1297/4459:  Train loss: 3.47440767288208  Test loss: 3.3310680389404297 \n",
      "Epoch: 2/10:  mini-batch 1298/4459:  Train loss: 3.3874928951263428  Test loss: 3.330430507659912 \n",
      "Epoch: 2/10:  mini-batch 1299/4459:  Train loss: 3.2398390769958496  Test loss: 3.329899787902832 \n",
      "Epoch: 2/10:  mini-batch 1300/4459:  Train loss: 3.4791324138641357  Test loss: 3.329545497894287 \n",
      "Epoch: 2/10:  mini-batch 1301/4459:  Train loss: 3.2469964027404785  Test loss: 3.329165458679199 \n",
      "Epoch: 2/10:  mini-batch 1302/4459:  Train loss: 3.0962612628936768  Test loss: 3.328864574432373 \n",
      "Epoch: 2/10:  mini-batch 1303/4459:  Train loss: 3.2747020721435547  Test loss: 3.3287816047668457 \n",
      "Epoch: 2/10:  mini-batch 1304/4459:  Train loss: 3.224123239517212  Test loss: 3.32859206199646 \n",
      "Epoch: 2/10:  mini-batch 1305/4459:  Train loss: 3.6054234504699707  Test loss: 3.328212261199951 \n",
      "Epoch: 2/10:  mini-batch 1306/4459:  Train loss: 3.319064140319824  Test loss: 3.32781982421875 \n",
      "Epoch: 2/10:  mini-batch 1307/4459:  Train loss: 3.299048900604248  Test loss: 3.326822519302368 \n",
      "Epoch: 2/10:  mini-batch 1308/4459:  Train loss: 3.3794360160827637  Test loss: 3.325887680053711 \n",
      "Epoch: 2/10:  mini-batch 1309/4459:  Train loss: 3.4317545890808105  Test loss: 3.3251490592956543 \n",
      "Epoch: 2/10:  mini-batch 1310/4459:  Train loss: 3.612316370010376  Test loss: 3.3245952129364014 \n",
      "Epoch: 2/10:  mini-batch 1311/4459:  Train loss: 3.547683000564575  Test loss: 3.3244099617004395 \n",
      "Epoch: 2/10:  mini-batch 1312/4459:  Train loss: 3.22523832321167  Test loss: 3.324211597442627 \n",
      "Epoch: 2/10:  mini-batch 1313/4459:  Train loss: 3.3559603691101074  Test loss: 3.323981523513794 \n",
      "Epoch: 2/10:  mini-batch 1314/4459:  Train loss: 3.5992257595062256  Test loss: 3.324359893798828 \n",
      "Epoch: 2/10:  mini-batch 1315/4459:  Train loss: 3.5684404373168945  Test loss: 3.324868679046631 \n",
      "Epoch: 2/10:  mini-batch 1316/4459:  Train loss: 3.6527810096740723  Test loss: 3.3254876136779785 \n",
      "Epoch: 2/10:  mini-batch 1317/4459:  Train loss: 3.562826633453369  Test loss: 3.3261055946350098 \n",
      "Epoch: 2/10:  mini-batch 1318/4459:  Train loss: 3.232921600341797  Test loss: 3.3264708518981934 \n",
      "Epoch: 2/10:  mini-batch 1319/4459:  Train loss: 3.381699562072754  Test loss: 3.326671838760376 \n",
      "Epoch: 2/10:  mini-batch 1320/4459:  Train loss: 3.449429988861084  Test loss: 3.326742172241211 \n",
      "Epoch: 2/10:  mini-batch 1321/4459:  Train loss: 3.3404412269592285  Test loss: 3.327052116394043 \n",
      "Epoch: 2/10:  mini-batch 1322/4459:  Train loss: 3.2760839462280273  Test loss: 3.3272881507873535 \n",
      "Epoch: 2/10:  mini-batch 1323/4459:  Train loss: 3.3377349376678467  Test loss: 3.3274545669555664 \n",
      "Epoch: 2/10:  mini-batch 1324/4459:  Train loss: 3.294678211212158  Test loss: 3.327493667602539 \n",
      "Epoch: 2/10:  mini-batch 1325/4459:  Train loss: 3.5039048194885254  Test loss: 3.3279337882995605 \n",
      "Epoch: 2/10:  mini-batch 1326/4459:  Train loss: 3.406921625137329  Test loss: 3.32820200920105 \n",
      "Epoch: 2/10:  mini-batch 1327/4459:  Train loss: 3.2780351638793945  Test loss: 3.328317165374756 \n",
      "Epoch: 2/10:  mini-batch 1328/4459:  Train loss: 3.300906181335449  Test loss: 3.3281047344207764 \n",
      "Epoch: 2/10:  mini-batch 1329/4459:  Train loss: 3.3621857166290283  Test loss: 3.3278849124908447 \n",
      "Epoch: 2/10:  mini-batch 1330/4459:  Train loss: 3.372302532196045  Test loss: 3.327190399169922 \n",
      "Epoch: 2/10:  mini-batch 1331/4459:  Train loss: 3.692352771759033  Test loss: 3.326352596282959 \n",
      "Epoch: 2/10:  mini-batch 1332/4459:  Train loss: 3.276299238204956  Test loss: 3.3255324363708496 \n",
      "Epoch: 2/10:  mini-batch 1333/4459:  Train loss: 3.2388181686401367  Test loss: 3.3245649337768555 \n",
      "Epoch: 2/10:  mini-batch 1334/4459:  Train loss: 3.707034111022949  Test loss: 3.323916435241699 \n",
      "Epoch: 2/10:  mini-batch 1335/4459:  Train loss: 3.552219867706299  Test loss: 3.3234262466430664 \n",
      "Epoch: 2/10:  mini-batch 1336/4459:  Train loss: 3.780183792114258  Test loss: 3.323542833328247 \n",
      "Epoch: 2/10:  mini-batch 1337/4459:  Train loss: 3.3564319610595703  Test loss: 3.3235349655151367 \n",
      "Epoch: 2/10:  mini-batch 1338/4459:  Train loss: 3.423426628112793  Test loss: 3.323442220687866 \n",
      "Epoch: 2/10:  mini-batch 1339/4459:  Train loss: 3.2268781661987305  Test loss: 3.323178768157959 \n",
      "Epoch: 2/10:  mini-batch 1340/4459:  Train loss: 3.5691585540771484  Test loss: 3.323270082473755 \n",
      "Epoch: 2/10:  mini-batch 1341/4459:  Train loss: 3.2349767684936523  Test loss: 3.3232457637786865 \n",
      "Epoch: 2/10:  mini-batch 1342/4459:  Train loss: 3.3365683555603027  Test loss: 3.3231124877929688 \n",
      "Epoch: 2/10:  mini-batch 1343/4459:  Train loss: 3.4862701892852783  Test loss: 3.3232150077819824 \n",
      "Epoch: 2/10:  mini-batch 1344/4459:  Train loss: 3.332463264465332  Test loss: 3.323211669921875 \n",
      "Epoch: 2/10:  mini-batch 1345/4459:  Train loss: 3.340696096420288  Test loss: 3.323047161102295 \n",
      "Epoch: 2/10:  mini-batch 1346/4459:  Train loss: 3.4365735054016113  Test loss: 3.3228230476379395 \n",
      "Epoch: 2/10:  mini-batch 1347/4459:  Train loss: 3.402681350708008  Test loss: 3.3227381706237793 \n",
      "Epoch: 2/10:  mini-batch 1348/4459:  Train loss: 3.4944443702697754  Test loss: 3.3227334022521973 \n",
      "Epoch: 2/10:  mini-batch 1349/4459:  Train loss: 3.2813057899475098  Test loss: 3.322645664215088 \n",
      "Epoch: 2/10:  mini-batch 1350/4459:  Train loss: 3.20048451423645  Test loss: 3.3223354816436768 \n",
      "Epoch: 2/10:  mini-batch 1351/4459:  Train loss: 3.304863929748535  Test loss: 3.3221349716186523 \n",
      "Epoch: 2/10:  mini-batch 1352/4459:  Train loss: 3.326521873474121  Test loss: 3.3216309547424316 \n",
      "Epoch: 2/10:  mini-batch 1353/4459:  Train loss: 3.4242167472839355  Test loss: 3.321122169494629 \n",
      "Epoch: 2/10:  mini-batch 1354/4459:  Train loss: 3.630188226699829  Test loss: 3.32071590423584 \n",
      "Epoch: 2/10:  mini-batch 1355/4459:  Train loss: 3.3724005222320557  Test loss: 3.320312976837158 \n",
      "Epoch: 2/10:  mini-batch 1356/4459:  Train loss: 3.2767422199249268  Test loss: 3.3198184967041016 \n",
      "Epoch: 2/10:  mini-batch 1357/4459:  Train loss: 3.147738456726074  Test loss: 3.319044351577759 \n",
      "Epoch: 2/10:  mini-batch 1358/4459:  Train loss: 3.4398117065429688  Test loss: 3.3182992935180664 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1359/4459:  Train loss: 3.2763400077819824  Test loss: 3.317350387573242 \n",
      "Epoch: 2/10:  mini-batch 1360/4459:  Train loss: 3.233907461166382  Test loss: 3.316335916519165 \n",
      "Epoch: 2/10:  mini-batch 1361/4459:  Train loss: 3.091789960861206  Test loss: 3.3150742053985596 \n",
      "Epoch: 2/10:  mini-batch 1362/4459:  Train loss: 3.3137223720550537  Test loss: 3.3138670921325684 \n",
      "Epoch: 2/10:  mini-batch 1363/4459:  Train loss: 3.286618709564209  Test loss: 3.312615394592285 \n",
      "Epoch: 2/10:  mini-batch 1364/4459:  Train loss: 3.142934799194336  Test loss: 3.3113861083984375 \n",
      "Epoch: 2/10:  mini-batch 1365/4459:  Train loss: 3.0483646392822266  Test loss: 3.310086250305176 \n",
      "Epoch: 2/10:  mini-batch 1366/4459:  Train loss: 3.2642533779144287  Test loss: 3.3086354732513428 \n",
      "Epoch: 2/10:  mini-batch 1367/4459:  Train loss: 3.5491557121276855  Test loss: 3.307509422302246 \n",
      "Epoch: 2/10:  mini-batch 1368/4459:  Train loss: 3.288273811340332  Test loss: 3.306516170501709 \n",
      "Epoch: 2/10:  mini-batch 1369/4459:  Train loss: 3.4939260482788086  Test loss: 3.3056869506835938 \n",
      "Epoch: 2/10:  mini-batch 1370/4459:  Train loss: 3.2000699043273926  Test loss: 3.3046107292175293 \n",
      "Epoch: 2/10:  mini-batch 1371/4459:  Train loss: 3.3888447284698486  Test loss: 3.3033432960510254 \n",
      "Epoch: 2/10:  mini-batch 1372/4459:  Train loss: 3.095719814300537  Test loss: 3.302008867263794 \n",
      "Epoch: 2/10:  mini-batch 1373/4459:  Train loss: 3.150393009185791  Test loss: 3.3007330894470215 \n",
      "Epoch: 2/10:  mini-batch 1374/4459:  Train loss: 3.1511693000793457  Test loss: 3.2993812561035156 \n",
      "Epoch: 2/10:  mini-batch 1375/4459:  Train loss: 3.1589584350585938  Test loss: 3.298016309738159 \n",
      "Epoch: 2/10:  mini-batch 1376/4459:  Train loss: 3.6076693534851074  Test loss: 3.296870231628418 \n",
      "Epoch: 2/10:  mini-batch 1377/4459:  Train loss: 3.389615774154663  Test loss: 3.2958617210388184 \n",
      "Epoch: 2/10:  mini-batch 1378/4459:  Train loss: 3.285078763961792  Test loss: 3.2950167655944824 \n",
      "Epoch: 2/10:  mini-batch 1379/4459:  Train loss: 3.3380322456359863  Test loss: 3.2944207191467285 \n",
      "Epoch: 2/10:  mini-batch 1380/4459:  Train loss: 3.5259366035461426  Test loss: 3.2939717769622803 \n",
      "Epoch: 2/10:  mini-batch 1381/4459:  Train loss: 3.825366258621216  Test loss: 3.2941555976867676 \n",
      "Epoch: 2/10:  mini-batch 1382/4459:  Train loss: 3.8452868461608887  Test loss: 3.294736385345459 \n",
      "Epoch: 2/10:  mini-batch 1383/4459:  Train loss: 3.4849236011505127  Test loss: 3.2952780723571777 \n",
      "Epoch: 2/10:  mini-batch 1384/4459:  Train loss: 3.343984365463257  Test loss: 3.295538902282715 \n",
      "Epoch: 2/10:  mini-batch 1385/4459:  Train loss: 3.146975517272949  Test loss: 3.2959494590759277 \n",
      "Epoch: 2/10:  mini-batch 1386/4459:  Train loss: 3.065462589263916  Test loss: 3.296349287033081 \n",
      "Epoch: 2/10:  mini-batch 1387/4459:  Train loss: 3.3216757774353027  Test loss: 3.2965357303619385 \n",
      "Epoch: 2/10:  mini-batch 1388/4459:  Train loss: 2.956864595413208  Test loss: 3.2966320514678955 \n",
      "Epoch: 2/10:  mini-batch 1389/4459:  Train loss: 3.1775262355804443  Test loss: 3.296643018722534 \n",
      "Epoch: 2/10:  mini-batch 1390/4459:  Train loss: 3.17128849029541  Test loss: 3.2965598106384277 \n",
      "Epoch: 2/10:  mini-batch 1391/4459:  Train loss: 3.5433666706085205  Test loss: 3.296600818634033 \n",
      "Epoch: 2/10:  mini-batch 1392/4459:  Train loss: 3.52433443069458  Test loss: 3.296814441680908 \n",
      "Epoch: 2/10:  mini-batch 1393/4459:  Train loss: 3.2677648067474365  Test loss: 3.2969932556152344 \n",
      "Epoch: 2/10:  mini-batch 1394/4459:  Train loss: 3.4810962677001953  Test loss: 3.2973580360412598 \n",
      "Epoch: 2/10:  mini-batch 1395/4459:  Train loss: 3.3673691749572754  Test loss: 3.2977118492126465 \n",
      "Epoch: 2/10:  mini-batch 1396/4459:  Train loss: 2.7839932441711426  Test loss: 3.297886848449707 \n",
      "Epoch: 2/10:  mini-batch 1397/4459:  Train loss: 3.1715545654296875  Test loss: 3.2980823516845703 \n",
      "Epoch: 2/10:  mini-batch 1398/4459:  Train loss: 3.5625758171081543  Test loss: 3.2984206676483154 \n",
      "Epoch: 2/10:  mini-batch 1399/4459:  Train loss: 3.5879859924316406  Test loss: 3.2988088130950928 \n",
      "Epoch: 2/10:  mini-batch 1400/4459:  Train loss: 3.4849424362182617  Test loss: 3.2992615699768066 \n",
      "Epoch: 2/10:  mini-batch 1401/4459:  Train loss: 3.24947452545166  Test loss: 3.299722194671631 \n",
      "Epoch: 2/10:  mini-batch 1402/4459:  Train loss: 3.2237625122070312  Test loss: 3.300124168395996 \n",
      "Epoch: 2/10:  mini-batch 1403/4459:  Train loss: 3.3163185119628906  Test loss: 3.3000168800354004 \n",
      "Epoch: 2/10:  mini-batch 1404/4459:  Train loss: 3.261573314666748  Test loss: 3.2995669841766357 \n",
      "Epoch: 2/10:  mini-batch 1405/4459:  Train loss: 3.4188694953918457  Test loss: 3.299377918243408 \n",
      "Epoch: 2/10:  mini-batch 1406/4459:  Train loss: 3.077461004257202  Test loss: 3.2992067337036133 \n",
      "Epoch: 2/10:  mini-batch 1407/4459:  Train loss: 3.320303440093994  Test loss: 3.29940128326416 \n",
      "Epoch: 2/10:  mini-batch 1408/4459:  Train loss: 3.456235885620117  Test loss: 3.299633741378784 \n",
      "Epoch: 2/10:  mini-batch 1409/4459:  Train loss: 3.1480135917663574  Test loss: 3.299774646759033 \n",
      "Epoch: 2/10:  mini-batch 1410/4459:  Train loss: 3.280703544616699  Test loss: 3.2999579906463623 \n",
      "Epoch: 2/10:  mini-batch 1411/4459:  Train loss: 3.4841549396514893  Test loss: 3.300389289855957 \n",
      "Epoch: 2/10:  mini-batch 1412/4459:  Train loss: 3.0429046154022217  Test loss: 3.300246477127075 \n",
      "Epoch: 2/10:  mini-batch 1413/4459:  Train loss: 3.186476707458496  Test loss: 3.300130844116211 \n",
      "Epoch: 2/10:  mini-batch 1414/4459:  Train loss: 3.2225494384765625  Test loss: 3.3000104427337646 \n",
      "Epoch: 2/10:  mini-batch 1415/4459:  Train loss: 2.877761125564575  Test loss: 3.2999608516693115 \n",
      "Epoch: 2/10:  mini-batch 1416/4459:  Train loss: 2.8922436237335205  Test loss: 3.3000855445861816 \n",
      "Epoch: 2/10:  mini-batch 1417/4459:  Train loss: 3.417647123336792  Test loss: 3.3001723289489746 \n",
      "Epoch: 2/10:  mini-batch 1418/4459:  Train loss: 3.291149854660034  Test loss: 3.3002777099609375 \n",
      "Epoch: 2/10:  mini-batch 1419/4459:  Train loss: 3.0586087703704834  Test loss: 3.3004162311553955 \n",
      "Epoch: 2/10:  mini-batch 1420/4459:  Train loss: 3.1031503677368164  Test loss: 3.3000757694244385 \n",
      "Epoch: 2/10:  mini-batch 1421/4459:  Train loss: 3.0603628158569336  Test loss: 3.299825668334961 \n",
      "Epoch: 2/10:  mini-batch 1422/4459:  Train loss: 3.4104514122009277  Test loss: 3.2997097969055176 \n",
      "Epoch: 2/10:  mini-batch 1423/4459:  Train loss: 3.4515209197998047  Test loss: 3.2995247840881348 \n",
      "Epoch: 2/10:  mini-batch 1424/4459:  Train loss: 2.878526449203491  Test loss: 3.299555540084839 \n",
      "Epoch: 2/10:  mini-batch 1425/4459:  Train loss: 3.336376667022705  Test loss: 3.299651622772217 \n",
      "Epoch: 2/10:  mini-batch 1426/4459:  Train loss: 3.375868558883667  Test loss: 3.299684762954712 \n",
      "Epoch: 2/10:  mini-batch 1427/4459:  Train loss: 3.0010902881622314  Test loss: 3.2995290756225586 \n",
      "Epoch: 2/10:  mini-batch 1428/4459:  Train loss: 2.969078779220581  Test loss: 3.2992353439331055 \n",
      "Epoch: 2/10:  mini-batch 1429/4459:  Train loss: 3.1196396350860596  Test loss: 3.298870086669922 \n",
      "Epoch: 2/10:  mini-batch 1430/4459:  Train loss: 3.395594596862793  Test loss: 3.298923969268799 \n",
      "Epoch: 2/10:  mini-batch 1431/4459:  Train loss: 3.4121296405792236  Test loss: 3.29915714263916 \n",
      "Epoch: 2/10:  mini-batch 1432/4459:  Train loss: 3.2347359657287598  Test loss: 3.299102783203125 \n",
      "Epoch: 2/10:  mini-batch 1433/4459:  Train loss: 2.8877477645874023  Test loss: 3.2987418174743652 \n",
      "Epoch: 2/10:  mini-batch 1434/4459:  Train loss: 3.1229021549224854  Test loss: 3.2979092597961426 \n",
      "Epoch: 2/10:  mini-batch 1435/4459:  Train loss: 3.505734920501709  Test loss: 3.2975287437438965 \n",
      "Epoch: 2/10:  mini-batch 1436/4459:  Train loss: 3.0332303047180176  Test loss: 3.2973132133483887 \n",
      "Epoch: 2/10:  mini-batch 1437/4459:  Train loss: 3.090498685836792  Test loss: 3.2969179153442383 \n",
      "Epoch: 2/10:  mini-batch 1438/4459:  Train loss: 3.3101322650909424  Test loss: 3.2956979274749756 \n",
      "Epoch: 2/10:  mini-batch 1439/4459:  Train loss: 3.323221206665039  Test loss: 3.2947144508361816 \n",
      "Epoch: 2/10:  mini-batch 1440/4459:  Train loss: 3.3566441535949707  Test loss: 3.2941408157348633 \n",
      "Epoch: 2/10:  mini-batch 1441/4459:  Train loss: 3.3541951179504395  Test loss: 3.293548345565796 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1442/4459:  Train loss: 3.4375486373901367  Test loss: 3.2926619052886963 \n",
      "Epoch: 2/10:  mini-batch 1443/4459:  Train loss: 3.763981342315674  Test loss: 3.2916903495788574 \n",
      "Epoch: 2/10:  mini-batch 1444/4459:  Train loss: 3.2884082794189453  Test loss: 3.2905192375183105 \n",
      "Epoch: 2/10:  mini-batch 1445/4459:  Train loss: 2.999955415725708  Test loss: 3.2895021438598633 \n",
      "Epoch: 2/10:  mini-batch 1446/4459:  Train loss: 3.167480945587158  Test loss: 3.288177251815796 \n",
      "Epoch: 2/10:  mini-batch 1447/4459:  Train loss: 3.803565502166748  Test loss: 3.287083864212036 \n",
      "Epoch: 2/10:  mini-batch 1448/4459:  Train loss: 3.335283041000366  Test loss: 3.2864935398101807 \n",
      "Epoch: 2/10:  mini-batch 1449/4459:  Train loss: 2.955944061279297  Test loss: 3.2861928939819336 \n",
      "Epoch: 2/10:  mini-batch 1450/4459:  Train loss: 3.0596206188201904  Test loss: 3.2857272624969482 \n",
      "Epoch: 2/10:  mini-batch 1451/4459:  Train loss: 3.1760880947113037  Test loss: 3.2851102352142334 \n",
      "Epoch: 2/10:  mini-batch 1452/4459:  Train loss: 3.2950029373168945  Test loss: 3.28446888923645 \n",
      "Epoch: 2/10:  mini-batch 1453/4459:  Train loss: 3.440460681915283  Test loss: 3.2830076217651367 \n",
      "Epoch: 2/10:  mini-batch 1454/4459:  Train loss: 3.705397605895996  Test loss: 3.2819178104400635 \n",
      "Epoch: 2/10:  mini-batch 1455/4459:  Train loss: 3.729804515838623  Test loss: 3.2806642055511475 \n",
      "Epoch: 2/10:  mini-batch 1456/4459:  Train loss: 3.539214849472046  Test loss: 3.280012607574463 \n",
      "Epoch: 2/10:  mini-batch 1457/4459:  Train loss: 3.296734571456909  Test loss: 3.279325246810913 \n",
      "Epoch: 2/10:  mini-batch 1458/4459:  Train loss: 3.287470817565918  Test loss: 3.2784385681152344 \n",
      "Epoch: 2/10:  mini-batch 1459/4459:  Train loss: 3.41925311088562  Test loss: 3.277951717376709 \n",
      "Epoch: 2/10:  mini-batch 1460/4459:  Train loss: 3.4262566566467285  Test loss: 3.27781081199646 \n",
      "Epoch: 2/10:  mini-batch 1461/4459:  Train loss: 3.3962488174438477  Test loss: 3.2772529125213623 \n",
      "Epoch: 2/10:  mini-batch 1462/4459:  Train loss: 3.2085471153259277  Test loss: 3.2761545181274414 \n",
      "Epoch: 2/10:  mini-batch 1463/4459:  Train loss: 3.201427459716797  Test loss: 3.2753000259399414 \n",
      "Epoch: 2/10:  mini-batch 1464/4459:  Train loss: 3.5868453979492188  Test loss: 3.2746453285217285 \n",
      "Epoch: 2/10:  mini-batch 1465/4459:  Train loss: 3.1204214096069336  Test loss: 3.273857831954956 \n",
      "Epoch: 2/10:  mini-batch 1466/4459:  Train loss: 3.199718475341797  Test loss: 3.27325177192688 \n",
      "Epoch: 2/10:  mini-batch 1467/4459:  Train loss: 3.1661579608917236  Test loss: 3.2726686000823975 \n",
      "Epoch: 2/10:  mini-batch 1468/4459:  Train loss: 3.450286865234375  Test loss: 3.2719850540161133 \n",
      "Epoch: 2/10:  mini-batch 1469/4459:  Train loss: 3.409423589706421  Test loss: 3.271383762359619 \n",
      "Epoch: 2/10:  mini-batch 1470/4459:  Train loss: 2.949467897415161  Test loss: 3.2703802585601807 \n",
      "Epoch: 2/10:  mini-batch 1471/4459:  Train loss: 3.2286171913146973  Test loss: 3.2693819999694824 \n",
      "Epoch: 2/10:  mini-batch 1472/4459:  Train loss: 3.4000613689422607  Test loss: 3.268033027648926 \n",
      "Epoch: 2/10:  mini-batch 1473/4459:  Train loss: 3.4098284244537354  Test loss: 3.26725172996521 \n",
      "Epoch: 2/10:  mini-batch 1474/4459:  Train loss: 3.3142833709716797  Test loss: 3.2665486335754395 \n",
      "Epoch: 2/10:  mini-batch 1475/4459:  Train loss: 3.1986513137817383  Test loss: 3.266166925430298 \n",
      "Epoch: 2/10:  mini-batch 1476/4459:  Train loss: 3.2021408081054688  Test loss: 3.2657124996185303 \n",
      "Epoch: 2/10:  mini-batch 1477/4459:  Train loss: 3.210326671600342  Test loss: 3.264833450317383 \n",
      "Epoch: 2/10:  mini-batch 1478/4459:  Train loss: 3.251030445098877  Test loss: 3.2640719413757324 \n",
      "Epoch: 2/10:  mini-batch 1479/4459:  Train loss: 3.1889994144439697  Test loss: 3.263303756713867 \n",
      "Epoch: 2/10:  mini-batch 1480/4459:  Train loss: 3.3260369300842285  Test loss: 3.2625253200531006 \n",
      "Epoch: 2/10:  mini-batch 1481/4459:  Train loss: 3.1734533309936523  Test loss: 3.2614829540252686 \n",
      "Epoch: 2/10:  mini-batch 1482/4459:  Train loss: 3.21413516998291  Test loss: 3.2608110904693604 \n",
      "Epoch: 2/10:  mini-batch 1483/4459:  Train loss: 3.2521722316741943  Test loss: 3.260082721710205 \n",
      "Epoch: 2/10:  mini-batch 1484/4459:  Train loss: 3.2813034057617188  Test loss: 3.2592854499816895 \n",
      "Epoch: 2/10:  mini-batch 1485/4459:  Train loss: 3.157188892364502  Test loss: 3.258474826812744 \n",
      "Epoch: 2/10:  mini-batch 1486/4459:  Train loss: 3.626391887664795  Test loss: 3.257819890975952 \n",
      "Epoch: 2/10:  mini-batch 1487/4459:  Train loss: 3.5601630210876465  Test loss: 3.2573623657226562 \n",
      "Epoch: 2/10:  mini-batch 1488/4459:  Train loss: 3.5840752124786377  Test loss: 3.2567875385284424 \n",
      "Epoch: 2/10:  mini-batch 1489/4459:  Train loss: 3.1345527172088623  Test loss: 3.2556686401367188 \n",
      "Epoch: 2/10:  mini-batch 1490/4459:  Train loss: 3.453770160675049  Test loss: 3.254879951477051 \n",
      "Epoch: 2/10:  mini-batch 1491/4459:  Train loss: 3.364729881286621  Test loss: 3.2542190551757812 \n",
      "Epoch: 2/10:  mini-batch 1492/4459:  Train loss: 3.4665539264678955  Test loss: 3.253594160079956 \n",
      "Epoch: 2/10:  mini-batch 1493/4459:  Train loss: 3.394543409347534  Test loss: 3.253150463104248 \n",
      "Epoch: 2/10:  mini-batch 1494/4459:  Train loss: 3.3210179805755615  Test loss: 3.252720355987549 \n",
      "Epoch: 2/10:  mini-batch 1495/4459:  Train loss: 3.1633565425872803  Test loss: 3.25229811668396 \n",
      "Epoch: 2/10:  mini-batch 1496/4459:  Train loss: 3.6590332984924316  Test loss: 3.2524619102478027 \n",
      "Epoch: 2/10:  mini-batch 1497/4459:  Train loss: 3.3239376544952393  Test loss: 3.2526822090148926 \n",
      "Epoch: 2/10:  mini-batch 1498/4459:  Train loss: 3.2336182594299316  Test loss: 3.2530202865600586 \n",
      "Epoch: 2/10:  mini-batch 1499/4459:  Train loss: 3.234393358230591  Test loss: 3.252981662750244 \n",
      "Epoch: 2/10:  mini-batch 1500/4459:  Train loss: 3.2622509002685547  Test loss: 3.252892017364502 \n",
      "Epoch: 2/10:  mini-batch 1501/4459:  Train loss: 3.533146381378174  Test loss: 3.252793312072754 \n",
      "Epoch: 2/10:  mini-batch 1502/4459:  Train loss: 3.1534690856933594  Test loss: 3.252289056777954 \n",
      "Epoch: 2/10:  mini-batch 1503/4459:  Train loss: 3.280200719833374  Test loss: 3.2516703605651855 \n",
      "Epoch: 2/10:  mini-batch 1504/4459:  Train loss: 3.1823668479919434  Test loss: 3.2507965564727783 \n",
      "Epoch: 2/10:  mini-batch 1505/4459:  Train loss: 3.180297374725342  Test loss: 3.2497763633728027 \n",
      "Epoch: 2/10:  mini-batch 1506/4459:  Train loss: 3.6596696376800537  Test loss: 3.2492334842681885 \n",
      "Epoch: 2/10:  mini-batch 1507/4459:  Train loss: 3.0073273181915283  Test loss: 3.248430013656616 \n",
      "Epoch: 2/10:  mini-batch 1508/4459:  Train loss: 3.3416624069213867  Test loss: 3.247946262359619 \n",
      "Epoch: 2/10:  mini-batch 1509/4459:  Train loss: 3.2900543212890625  Test loss: 3.247526168823242 \n",
      "Epoch: 2/10:  mini-batch 1510/4459:  Train loss: 3.3742799758911133  Test loss: 3.247205972671509 \n",
      "Epoch: 2/10:  mini-batch 1511/4459:  Train loss: 3.668389081954956  Test loss: 3.247265338897705 \n",
      "Epoch: 2/10:  mini-batch 1512/4459:  Train loss: 3.2091290950775146  Test loss: 3.2473015785217285 \n",
      "Epoch: 2/10:  mini-batch 1513/4459:  Train loss: 3.291797637939453  Test loss: 3.2474093437194824 \n",
      "Epoch: 2/10:  mini-batch 1514/4459:  Train loss: 3.154148817062378  Test loss: 3.2474231719970703 \n",
      "Epoch: 2/10:  mini-batch 1515/4459:  Train loss: 3.86342453956604  Test loss: 3.2479851245880127 \n",
      "Epoch: 2/10:  mini-batch 1516/4459:  Train loss: 3.372921943664551  Test loss: 3.248546600341797 \n",
      "Epoch: 2/10:  mini-batch 1517/4459:  Train loss: 3.536583185195923  Test loss: 3.2492785453796387 \n",
      "Epoch: 2/10:  mini-batch 1518/4459:  Train loss: 3.3848683834075928  Test loss: 3.2501559257507324 \n",
      "Epoch: 2/10:  mini-batch 1519/4459:  Train loss: 3.19256854057312  Test loss: 3.251102924346924 \n",
      "Epoch: 2/10:  mini-batch 1520/4459:  Train loss: 3.1521284580230713  Test loss: 3.251887798309326 \n",
      "Epoch: 2/10:  mini-batch 1521/4459:  Train loss: 3.3785037994384766  Test loss: 3.252666473388672 \n",
      "Epoch: 2/10:  mini-batch 1522/4459:  Train loss: 3.3427793979644775  Test loss: 3.2533445358276367 \n",
      "Epoch: 2/10:  mini-batch 1523/4459:  Train loss: 3.556147575378418  Test loss: 3.2543375492095947 \n",
      "Epoch: 2/10:  mini-batch 1524/4459:  Train loss: 3.41338849067688  Test loss: 3.2554616928100586 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1525/4459:  Train loss: 3.4228334426879883  Test loss: 3.2563107013702393 \n",
      "Epoch: 2/10:  mini-batch 1526/4459:  Train loss: 3.1963465213775635  Test loss: 3.2568774223327637 \n",
      "Epoch: 2/10:  mini-batch 1527/4459:  Train loss: 3.277329921722412  Test loss: 3.2573022842407227 \n",
      "Epoch: 2/10:  mini-batch 1528/4459:  Train loss: 3.570019006729126  Test loss: 3.2579054832458496 \n",
      "Epoch: 2/10:  mini-batch 1529/4459:  Train loss: 3.206493616104126  Test loss: 3.2583932876586914 \n",
      "Epoch: 2/10:  mini-batch 1530/4459:  Train loss: 3.177806854248047  Test loss: 3.258688449859619 \n",
      "Epoch: 2/10:  mini-batch 1531/4459:  Train loss: 3.435619354248047  Test loss: 3.2591962814331055 \n",
      "Epoch: 2/10:  mini-batch 1532/4459:  Train loss: 3.3855109214782715  Test loss: 3.2597508430480957 \n",
      "Epoch: 2/10:  mini-batch 1533/4459:  Train loss: 2.821455955505371  Test loss: 3.2599873542785645 \n",
      "Epoch: 2/10:  mini-batch 1534/4459:  Train loss: 3.4710776805877686  Test loss: 3.260497570037842 \n",
      "Epoch: 2/10:  mini-batch 1535/4459:  Train loss: 3.610901355743408  Test loss: 3.2612767219543457 \n",
      "Epoch: 2/10:  mini-batch 1536/4459:  Train loss: 3.479182481765747  Test loss: 3.2622885704040527 \n",
      "Epoch: 2/10:  mini-batch 1537/4459:  Train loss: 3.6614136695861816  Test loss: 3.2637453079223633 \n",
      "Epoch: 2/10:  mini-batch 1538/4459:  Train loss: 3.310781717300415  Test loss: 3.265028476715088 \n",
      "Epoch: 2/10:  mini-batch 1539/4459:  Train loss: 3.258357524871826  Test loss: 3.266202449798584 \n",
      "Epoch: 2/10:  mini-batch 1540/4459:  Train loss: 3.827629804611206  Test loss: 3.267843008041382 \n",
      "Epoch: 2/10:  mini-batch 1541/4459:  Train loss: 3.5228919982910156  Test loss: 3.269606590270996 \n",
      "Epoch: 2/10:  mini-batch 1542/4459:  Train loss: 3.2173686027526855  Test loss: 3.2710928916931152 \n",
      "Epoch: 2/10:  mini-batch 1543/4459:  Train loss: 3.495715379714966  Test loss: 3.2728850841522217 \n",
      "Epoch: 2/10:  mini-batch 1544/4459:  Train loss: 3.2008931636810303  Test loss: 3.274463653564453 \n",
      "Epoch: 2/10:  mini-batch 1545/4459:  Train loss: 3.481419563293457  Test loss: 3.27606201171875 \n",
      "Epoch: 2/10:  mini-batch 1546/4459:  Train loss: 3.027601718902588  Test loss: 3.2770605087280273 \n",
      "Epoch: 2/10:  mini-batch 1547/4459:  Train loss: 3.430088996887207  Test loss: 3.278188705444336 \n",
      "Epoch: 2/10:  mini-batch 1548/4459:  Train loss: 3.5139894485473633  Test loss: 3.2794384956359863 \n",
      "Epoch: 2/10:  mini-batch 1549/4459:  Train loss: 3.4586238861083984  Test loss: 3.2808022499084473 \n",
      "Epoch: 2/10:  mini-batch 1550/4459:  Train loss: 3.3232407569885254  Test loss: 3.2821528911590576 \n",
      "Epoch: 2/10:  mini-batch 1551/4459:  Train loss: 3.3192460536956787  Test loss: 3.2833425998687744 \n",
      "Epoch: 2/10:  mini-batch 1552/4459:  Train loss: 3.5028223991394043  Test loss: 3.284666061401367 \n",
      "Epoch: 2/10:  mini-batch 1553/4459:  Train loss: 3.586170196533203  Test loss: 3.285898447036743 \n",
      "Epoch: 2/10:  mini-batch 1554/4459:  Train loss: 3.5191571712493896  Test loss: 3.2872133255004883 \n",
      "Epoch: 2/10:  mini-batch 1555/4459:  Train loss: 3.336705207824707  Test loss: 3.288461685180664 \n",
      "Epoch: 2/10:  mini-batch 1556/4459:  Train loss: 3.569817066192627  Test loss: 3.2898166179656982 \n",
      "Epoch: 2/10:  mini-batch 1557/4459:  Train loss: 3.5871620178222656  Test loss: 3.291433572769165 \n",
      "Epoch: 2/10:  mini-batch 1558/4459:  Train loss: 3.34767746925354  Test loss: 3.2929527759552 \n",
      "Epoch: 2/10:  mini-batch 1559/4459:  Train loss: 3.0560097694396973  Test loss: 3.293588876724243 \n",
      "Epoch: 2/10:  mini-batch 1560/4459:  Train loss: 3.4079723358154297  Test loss: 3.294135093688965 \n",
      "Epoch: 2/10:  mini-batch 1561/4459:  Train loss: 3.263460874557495  Test loss: 3.294498920440674 \n",
      "Epoch: 2/10:  mini-batch 1562/4459:  Train loss: 3.1933326721191406  Test loss: 3.294675350189209 \n",
      "Epoch: 2/10:  mini-batch 1563/4459:  Train loss: 3.4659883975982666  Test loss: 3.2950143814086914 \n",
      "Epoch: 2/10:  mini-batch 1564/4459:  Train loss: 3.671499013900757  Test loss: 3.2956669330596924 \n",
      "Epoch: 2/10:  mini-batch 1565/4459:  Train loss: 3.5031347274780273  Test loss: 3.296384334564209 \n",
      "Epoch: 2/10:  mini-batch 1566/4459:  Train loss: 3.5519156455993652  Test loss: 3.297384023666382 \n",
      "Epoch: 2/10:  mini-batch 1567/4459:  Train loss: 3.2198877334594727  Test loss: 3.298064947128296 \n",
      "Epoch: 2/10:  mini-batch 1568/4459:  Train loss: 3.323573350906372  Test loss: 3.298665761947632 \n",
      "Epoch: 2/10:  mini-batch 1569/4459:  Train loss: 3.1207499504089355  Test loss: 3.299058437347412 \n",
      "Epoch: 2/10:  mini-batch 1570/4459:  Train loss: 3.04302716255188  Test loss: 3.2988626956939697 \n",
      "Epoch: 2/10:  mini-batch 1571/4459:  Train loss: 3.1991636753082275  Test loss: 3.2983434200286865 \n",
      "Epoch: 2/10:  mini-batch 1572/4459:  Train loss: 3.2707135677337646  Test loss: 3.2975540161132812 \n",
      "Epoch: 2/10:  mini-batch 1573/4459:  Train loss: 3.3674814701080322  Test loss: 3.2967066764831543 \n",
      "Epoch: 2/10:  mini-batch 1574/4459:  Train loss: 3.1871156692504883  Test loss: 3.2956697940826416 \n",
      "Epoch: 2/10:  mini-batch 1575/4459:  Train loss: 3.171658515930176  Test loss: 3.294429302215576 \n",
      "Epoch: 2/10:  mini-batch 1576/4459:  Train loss: 3.30210280418396  Test loss: 3.2933311462402344 \n",
      "Epoch: 2/10:  mini-batch 1577/4459:  Train loss: 3.371530294418335  Test loss: 3.292363405227661 \n",
      "Epoch: 2/10:  mini-batch 1578/4459:  Train loss: 2.800576686859131  Test loss: 3.2911577224731445 \n",
      "Epoch: 2/10:  mini-batch 1579/4459:  Train loss: 3.024632692337036  Test loss: 3.2899107933044434 \n",
      "Epoch: 2/10:  mini-batch 1580/4459:  Train loss: 3.1678524017333984  Test loss: 3.288637161254883 \n",
      "Epoch: 2/10:  mini-batch 1581/4459:  Train loss: 3.214674472808838  Test loss: 3.2874722480773926 \n",
      "Epoch: 2/10:  mini-batch 1582/4459:  Train loss: 3.2595157623291016  Test loss: 3.286680221557617 \n",
      "Epoch: 2/10:  mini-batch 1583/4459:  Train loss: 3.1995701789855957  Test loss: 3.2855710983276367 \n",
      "Epoch: 2/10:  mini-batch 1584/4459:  Train loss: 3.144075632095337  Test loss: 3.284611225128174 \n",
      "Epoch: 2/10:  mini-batch 1585/4459:  Train loss: 3.1388256549835205  Test loss: 3.283724308013916 \n",
      "Epoch: 2/10:  mini-batch 1586/4459:  Train loss: 2.996443748474121  Test loss: 3.282890558242798 \n",
      "Epoch: 2/10:  mini-batch 1587/4459:  Train loss: 3.2827281951904297  Test loss: 3.2822790145874023 \n",
      "Epoch: 2/10:  mini-batch 1588/4459:  Train loss: 3.5146214962005615  Test loss: 3.2818832397460938 \n",
      "Epoch: 2/10:  mini-batch 1589/4459:  Train loss: 3.2637195587158203  Test loss: 3.281865358352661 \n",
      "Epoch: 2/10:  mini-batch 1590/4459:  Train loss: 3.177295446395874  Test loss: 3.281838893890381 \n",
      "Epoch: 2/10:  mini-batch 1591/4459:  Train loss: 3.312962770462036  Test loss: 3.2818450927734375 \n",
      "Epoch: 2/10:  mini-batch 1592/4459:  Train loss: 3.275186061859131  Test loss: 3.281829357147217 \n",
      "Epoch: 2/10:  mini-batch 1593/4459:  Train loss: 3.558934211730957  Test loss: 3.2818517684936523 \n",
      "Epoch: 2/10:  mini-batch 1594/4459:  Train loss: 3.1190295219421387  Test loss: 3.2820920944213867 \n",
      "Epoch: 2/10:  mini-batch 1595/4459:  Train loss: 3.028494119644165  Test loss: 3.2824721336364746 \n",
      "Epoch: 2/10:  mini-batch 1596/4459:  Train loss: 3.0642261505126953  Test loss: 3.2832155227661133 \n",
      "Epoch: 2/10:  mini-batch 1597/4459:  Train loss: 3.6496994495391846  Test loss: 3.2839195728302 \n",
      "Epoch: 2/10:  mini-batch 1598/4459:  Train loss: 2.8786187171936035  Test loss: 3.2847390174865723 \n",
      "Epoch: 2/10:  mini-batch 1599/4459:  Train loss: 2.915337562561035  Test loss: 3.285552501678467 \n",
      "Epoch: 2/10:  mini-batch 1600/4459:  Train loss: 3.2998878955841064  Test loss: 3.2865419387817383 \n",
      "Epoch: 2/10:  mini-batch 1601/4459:  Train loss: 3.4602298736572266  Test loss: 3.288018226623535 \n",
      "Epoch: 2/10:  mini-batch 1602/4459:  Train loss: 2.803250789642334  Test loss: 3.2897233963012695 \n",
      "Epoch: 2/10:  mini-batch 1603/4459:  Train loss: 2.985365390777588  Test loss: 3.291684150695801 \n",
      "Epoch: 2/10:  mini-batch 1604/4459:  Train loss: 3.860459089279175  Test loss: 3.293461322784424 \n",
      "Epoch: 2/10:  mini-batch 1605/4459:  Train loss: 3.6481666564941406  Test loss: 3.2947566509246826 \n",
      "Epoch: 2/10:  mini-batch 1606/4459:  Train loss: 3.028085470199585  Test loss: 3.2960257530212402 \n",
      "Epoch: 2/10:  mini-batch 1607/4459:  Train loss: 2.7954158782958984  Test loss: 3.2975194454193115 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1608/4459:  Train loss: 3.3507866859436035  Test loss: 3.2984611988067627 \n",
      "Epoch: 2/10:  mini-batch 1609/4459:  Train loss: 3.141190528869629  Test loss: 3.299227714538574 \n",
      "Epoch: 2/10:  mini-batch 1610/4459:  Train loss: 3.0985422134399414  Test loss: 3.2996931076049805 \n",
      "Epoch: 2/10:  mini-batch 1611/4459:  Train loss: 3.289550304412842  Test loss: 3.3001463413238525 \n",
      "Epoch: 2/10:  mini-batch 1612/4459:  Train loss: 3.1667284965515137  Test loss: 3.3007850646972656 \n",
      "Epoch: 2/10:  mini-batch 1613/4459:  Train loss: 3.102072238922119  Test loss: 3.3010878562927246 \n",
      "Epoch: 2/10:  mini-batch 1614/4459:  Train loss: 3.280256986618042  Test loss: 3.3015053272247314 \n",
      "Epoch: 2/10:  mini-batch 1615/4459:  Train loss: 3.5495333671569824  Test loss: 3.3017988204956055 \n",
      "Epoch: 2/10:  mini-batch 1616/4459:  Train loss: 2.9755008220672607  Test loss: 3.302640438079834 \n",
      "Epoch: 2/10:  mini-batch 1617/4459:  Train loss: 3.330596446990967  Test loss: 3.3033292293548584 \n",
      "Epoch: 2/10:  mini-batch 1618/4459:  Train loss: 3.205312728881836  Test loss: 3.3041789531707764 \n",
      "Epoch: 2/10:  mini-batch 1619/4459:  Train loss: 3.6049013137817383  Test loss: 3.304471492767334 \n",
      "Epoch: 2/10:  mini-batch 1620/4459:  Train loss: 2.9844062328338623  Test loss: 3.305044651031494 \n",
      "Epoch: 2/10:  mini-batch 1621/4459:  Train loss: 3.010809898376465  Test loss: 3.305433511734009 \n",
      "Epoch: 2/10:  mini-batch 1622/4459:  Train loss: 2.8800926208496094  Test loss: 3.3063931465148926 \n",
      "Epoch: 2/10:  mini-batch 1623/4459:  Train loss: 3.5433757305145264  Test loss: 3.3074002265930176 \n",
      "Epoch: 2/10:  mini-batch 1624/4459:  Train loss: 3.1359047889709473  Test loss: 3.3084447383880615 \n",
      "Epoch: 2/10:  mini-batch 1625/4459:  Train loss: 3.195827007293701  Test loss: 3.3093011379241943 \n",
      "Epoch: 2/10:  mini-batch 1626/4459:  Train loss: 4.0173187255859375  Test loss: 3.309385299682617 \n",
      "Epoch: 2/10:  mini-batch 1627/4459:  Train loss: 3.8654489517211914  Test loss: 3.308852195739746 \n",
      "Epoch: 2/10:  mini-batch 1628/4459:  Train loss: 3.2377073764801025  Test loss: 3.308060646057129 \n",
      "Epoch: 2/10:  mini-batch 1629/4459:  Train loss: 3.269505500793457  Test loss: 3.307607650756836 \n",
      "Epoch: 2/10:  mini-batch 1630/4459:  Train loss: 3.719705581665039  Test loss: 3.3073792457580566 \n",
      "Epoch: 2/10:  mini-batch 1631/4459:  Train loss: 2.8498589992523193  Test loss: 3.3074426651000977 \n",
      "Epoch: 2/10:  mini-batch 1632/4459:  Train loss: 3.2408950328826904  Test loss: 3.307161569595337 \n",
      "Epoch: 2/10:  mini-batch 1633/4459:  Train loss: 3.7331368923187256  Test loss: 3.3065357208251953 \n",
      "Epoch: 2/10:  mini-batch 1634/4459:  Train loss: 2.833162307739258  Test loss: 3.306446075439453 \n",
      "Epoch: 2/10:  mini-batch 1635/4459:  Train loss: 3.302626371383667  Test loss: 3.3064050674438477 \n",
      "Epoch: 2/10:  mini-batch 1636/4459:  Train loss: 3.0757927894592285  Test loss: 3.3064942359924316 \n",
      "Epoch: 2/10:  mini-batch 1637/4459:  Train loss: 3.175309181213379  Test loss: 3.30674409866333 \n",
      "Epoch: 2/10:  mini-batch 1638/4459:  Train loss: 3.1585030555725098  Test loss: 3.3068907260894775 \n",
      "Epoch: 2/10:  mini-batch 1639/4459:  Train loss: 3.1951708793640137  Test loss: 3.306866407394409 \n",
      "Epoch: 2/10:  mini-batch 1640/4459:  Train loss: 3.062070369720459  Test loss: 3.306976318359375 \n",
      "Epoch: 2/10:  mini-batch 1641/4459:  Train loss: 3.1388537883758545  Test loss: 3.307405471801758 \n",
      "Epoch: 2/10:  mini-batch 1642/4459:  Train loss: 3.3722636699676514  Test loss: 3.3079161643981934 \n",
      "Epoch: 2/10:  mini-batch 1643/4459:  Train loss: 3.2110581398010254  Test loss: 3.307905912399292 \n",
      "Epoch: 2/10:  mini-batch 1644/4459:  Train loss: 3.3003592491149902  Test loss: 3.308000087738037 \n",
      "Epoch: 2/10:  mini-batch 1645/4459:  Train loss: 3.5746283531188965  Test loss: 3.308183193206787 \n",
      "Epoch: 2/10:  mini-batch 1646/4459:  Train loss: 3.5588674545288086  Test loss: 3.308337926864624 \n",
      "Epoch: 2/10:  mini-batch 1647/4459:  Train loss: 3.5020570755004883  Test loss: 3.3088746070861816 \n",
      "Epoch: 2/10:  mini-batch 1648/4459:  Train loss: 3.3780529499053955  Test loss: 3.3093020915985107 \n",
      "Epoch: 2/10:  mini-batch 1649/4459:  Train loss: 3.4898054599761963  Test loss: 3.3095836639404297 \n",
      "Epoch: 2/10:  mini-batch 1650/4459:  Train loss: 3.4701924324035645  Test loss: 3.30997371673584 \n",
      "Epoch: 2/10:  mini-batch 1651/4459:  Train loss: 2.9950034618377686  Test loss: 3.3103485107421875 \n",
      "Epoch: 2/10:  mini-batch 1652/4459:  Train loss: 3.1512441635131836  Test loss: 3.31050443649292 \n",
      "Epoch: 2/10:  mini-batch 1653/4459:  Train loss: 3.5723049640655518  Test loss: 3.3107962608337402 \n",
      "Epoch: 2/10:  mini-batch 1654/4459:  Train loss: 2.964700222015381  Test loss: 3.311178684234619 \n",
      "Epoch: 2/10:  mini-batch 1655/4459:  Train loss: 2.831177234649658  Test loss: 3.311556339263916 \n",
      "Epoch: 2/10:  mini-batch 1656/4459:  Train loss: 3.3080785274505615  Test loss: 3.3119516372680664 \n",
      "Epoch: 2/10:  mini-batch 1657/4459:  Train loss: 3.536803960800171  Test loss: 3.312474489212036 \n",
      "Epoch: 2/10:  mini-batch 1658/4459:  Train loss: 3.443715810775757  Test loss: 3.3130412101745605 \n",
      "Epoch: 2/10:  mini-batch 1659/4459:  Train loss: 2.8882033824920654  Test loss: 3.3136484622955322 \n",
      "Epoch: 2/10:  mini-batch 1660/4459:  Train loss: 3.066600799560547  Test loss: 3.3142900466918945 \n",
      "Epoch: 2/10:  mini-batch 1661/4459:  Train loss: 3.2298970222473145  Test loss: 3.314913272857666 \n",
      "Epoch: 2/10:  mini-batch 1662/4459:  Train loss: 3.179943323135376  Test loss: 3.315495491027832 \n",
      "Epoch: 2/10:  mini-batch 1663/4459:  Train loss: 3.2124783992767334  Test loss: 3.315932512283325 \n",
      "Epoch: 2/10:  mini-batch 1664/4459:  Train loss: 3.3172144889831543  Test loss: 3.316467761993408 \n",
      "Epoch: 2/10:  mini-batch 1665/4459:  Train loss: 2.9887890815734863  Test loss: 3.3172812461853027 \n",
      "Epoch: 2/10:  mini-batch 1666/4459:  Train loss: 3.094013214111328  Test loss: 3.3180787563323975 \n",
      "Epoch: 2/10:  mini-batch 1667/4459:  Train loss: 3.6354782581329346  Test loss: 3.318389892578125 \n",
      "Epoch: 2/10:  mini-batch 1668/4459:  Train loss: 3.188035488128662  Test loss: 3.3185887336730957 \n",
      "Epoch: 2/10:  mini-batch 1669/4459:  Train loss: 3.2481706142425537  Test loss: 3.318850517272949 \n",
      "Epoch: 2/10:  mini-batch 1670/4459:  Train loss: 3.4426403045654297  Test loss: 3.3192780017852783 \n",
      "Epoch: 2/10:  mini-batch 1671/4459:  Train loss: 3.1345386505126953  Test loss: 3.3198046684265137 \n",
      "Epoch: 2/10:  mini-batch 1672/4459:  Train loss: 3.23270320892334  Test loss: 3.320186138153076 \n",
      "Epoch: 2/10:  mini-batch 1673/4459:  Train loss: 2.9006505012512207  Test loss: 3.3206710815429688 \n",
      "Epoch: 2/10:  mini-batch 1674/4459:  Train loss: 3.09529709815979  Test loss: 3.3212082386016846 \n",
      "Epoch: 2/10:  mini-batch 1675/4459:  Train loss: 3.5039021968841553  Test loss: 3.3213560581207275 \n",
      "Epoch: 2/10:  mini-batch 1676/4459:  Train loss: 2.9965972900390625  Test loss: 3.321730613708496 \n",
      "Epoch: 2/10:  mini-batch 1677/4459:  Train loss: 3.1573705673217773  Test loss: 3.3217878341674805 \n",
      "Epoch: 2/10:  mini-batch 1678/4459:  Train loss: 2.902688503265381  Test loss: 3.322024345397949 \n",
      "Epoch: 2/10:  mini-batch 1679/4459:  Train loss: 3.3394908905029297  Test loss: 3.32210636138916 \n",
      "Epoch: 2/10:  mini-batch 1680/4459:  Train loss: 3.407775640487671  Test loss: 3.3220949172973633 \n",
      "Epoch: 2/10:  mini-batch 1681/4459:  Train loss: 3.0329699516296387  Test loss: 3.3220441341400146 \n",
      "Epoch: 2/10:  mini-batch 1682/4459:  Train loss: 3.1590628623962402  Test loss: 3.322129249572754 \n",
      "Epoch: 2/10:  mini-batch 1683/4459:  Train loss: 3.1286230087280273  Test loss: 3.3222429752349854 \n",
      "Epoch: 2/10:  mini-batch 1684/4459:  Train loss: 3.0727972984313965  Test loss: 3.3224101066589355 \n",
      "Epoch: 2/10:  mini-batch 1685/4459:  Train loss: 3.105175495147705  Test loss: 3.322722911834717 \n",
      "Epoch: 2/10:  mini-batch 1686/4459:  Train loss: 3.3832027912139893  Test loss: 3.3230080604553223 \n",
      "Epoch: 2/10:  mini-batch 1687/4459:  Train loss: 3.341245174407959  Test loss: 3.3229918479919434 \n",
      "Epoch: 2/10:  mini-batch 1688/4459:  Train loss: 3.2622787952423096  Test loss: 3.3233861923217773 \n",
      "Epoch: 2/10:  mini-batch 1689/4459:  Train loss: 3.5136075019836426  Test loss: 3.3234949111938477 \n",
      "Epoch: 2/10:  mini-batch 1690/4459:  Train loss: 3.6912899017333984  Test loss: 3.3231992721557617 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1691/4459:  Train loss: 3.1438913345336914  Test loss: 3.3230693340301514 \n",
      "Epoch: 2/10:  mini-batch 1692/4459:  Train loss: 3.233442783355713  Test loss: 3.322683811187744 \n",
      "Epoch: 2/10:  mini-batch 1693/4459:  Train loss: 3.6280436515808105  Test loss: 3.3221163749694824 \n",
      "Epoch: 2/10:  mini-batch 1694/4459:  Train loss: 3.0729284286499023  Test loss: 3.3220279216766357 \n",
      "Epoch: 2/10:  mini-batch 1695/4459:  Train loss: 3.213728904724121  Test loss: 3.3218913078308105 \n",
      "Epoch: 2/10:  mini-batch 1696/4459:  Train loss: 3.13726806640625  Test loss: 3.3219358921051025 \n",
      "Epoch: 2/10:  mini-batch 1697/4459:  Train loss: 2.8376054763793945  Test loss: 3.322309970855713 \n",
      "Epoch: 2/10:  mini-batch 1698/4459:  Train loss: 3.121074676513672  Test loss: 3.322673797607422 \n",
      "Epoch: 2/10:  mini-batch 1699/4459:  Train loss: 3.345803737640381  Test loss: 3.322843551635742 \n",
      "Epoch: 2/10:  mini-batch 1700/4459:  Train loss: 3.65364933013916  Test loss: 3.3227648735046387 \n",
      "Epoch: 2/10:  mini-batch 1701/4459:  Train loss: 2.677030324935913  Test loss: 3.3233237266540527 \n",
      "Epoch: 2/10:  mini-batch 1702/4459:  Train loss: 3.181072950363159  Test loss: 3.323573112487793 \n",
      "Epoch: 2/10:  mini-batch 1703/4459:  Train loss: 3.3541741371154785  Test loss: 3.323953628540039 \n",
      "Epoch: 2/10:  mini-batch 1704/4459:  Train loss: 3.896218776702881  Test loss: 3.324127197265625 \n",
      "Epoch: 2/10:  mini-batch 1705/4459:  Train loss: 3.0182859897613525  Test loss: 3.324402332305908 \n",
      "Epoch: 2/10:  mini-batch 1706/4459:  Train loss: 2.9086146354675293  Test loss: 3.32509446144104 \n",
      "Epoch: 2/10:  mini-batch 1707/4459:  Train loss: 3.4152135848999023  Test loss: 3.325626850128174 \n",
      "Epoch: 2/10:  mini-batch 1708/4459:  Train loss: 2.971114158630371  Test loss: 3.326357841491699 \n",
      "Epoch: 2/10:  mini-batch 1709/4459:  Train loss: 3.1754608154296875  Test loss: 3.327056884765625 \n",
      "Epoch: 2/10:  mini-batch 1710/4459:  Train loss: 3.827664375305176  Test loss: 3.3275513648986816 \n",
      "Epoch: 2/10:  mini-batch 1711/4459:  Train loss: 3.3154120445251465  Test loss: 3.3280293941497803 \n",
      "Epoch: 2/10:  mini-batch 1712/4459:  Train loss: 2.8693978786468506  Test loss: 3.3287854194641113 \n",
      "Epoch: 2/10:  mini-batch 1713/4459:  Train loss: 2.7720632553100586  Test loss: 3.3299965858459473 \n",
      "Epoch: 2/10:  mini-batch 1714/4459:  Train loss: 2.9546573162078857  Test loss: 3.331437587738037 \n",
      "Epoch: 2/10:  mini-batch 1715/4459:  Train loss: 3.278627872467041  Test loss: 3.332669734954834 \n",
      "Epoch: 2/10:  mini-batch 1716/4459:  Train loss: 3.121096611022949  Test loss: 3.333609104156494 \n",
      "Epoch: 2/10:  mini-batch 1717/4459:  Train loss: 3.472553253173828  Test loss: 3.334566593170166 \n",
      "Epoch: 2/10:  mini-batch 1718/4459:  Train loss: 3.6602344512939453  Test loss: 3.33522629737854 \n",
      "Epoch: 2/10:  mini-batch 1719/4459:  Train loss: 3.932624578475952  Test loss: 3.3351335525512695 \n",
      "Epoch: 2/10:  mini-batch 1720/4459:  Train loss: 3.644153356552124  Test loss: 3.3345513343811035 \n",
      "Epoch: 2/10:  mini-batch 1721/4459:  Train loss: 3.2530314922332764  Test loss: 3.33378267288208 \n",
      "Epoch: 2/10:  mini-batch 1722/4459:  Train loss: 2.974062204360962  Test loss: 3.3333773612976074 \n",
      "Epoch: 2/10:  mini-batch 1723/4459:  Train loss: 3.1497626304626465  Test loss: 3.3325114250183105 \n",
      "Epoch: 2/10:  mini-batch 1724/4459:  Train loss: 3.406507968902588  Test loss: 3.3316540718078613 \n",
      "Epoch: 2/10:  mini-batch 1725/4459:  Train loss: 3.4161648750305176  Test loss: 3.330794334411621 \n",
      "Epoch: 2/10:  mini-batch 1726/4459:  Train loss: 3.220203399658203  Test loss: 3.330000877380371 \n",
      "Epoch: 2/10:  mini-batch 1727/4459:  Train loss: 3.1294631958007812  Test loss: 3.3292956352233887 \n",
      "Epoch: 2/10:  mini-batch 1728/4459:  Train loss: 3.2345728874206543  Test loss: 3.3286094665527344 \n",
      "Epoch: 2/10:  mini-batch 1729/4459:  Train loss: 3.4702444076538086  Test loss: 3.3278121948242188 \n",
      "Epoch: 2/10:  mini-batch 1730/4459:  Train loss: 3.3418025970458984  Test loss: 3.326934814453125 \n",
      "Epoch: 2/10:  mini-batch 1731/4459:  Train loss: 3.5621840953826904  Test loss: 3.325906276702881 \n",
      "Epoch: 2/10:  mini-batch 1732/4459:  Train loss: 3.194002628326416  Test loss: 3.3251070976257324 \n",
      "Epoch: 2/10:  mini-batch 1733/4459:  Train loss: 3.3986942768096924  Test loss: 3.32454776763916 \n",
      "Epoch: 2/10:  mini-batch 1734/4459:  Train loss: 3.3709661960601807  Test loss: 3.324324607849121 \n",
      "Epoch: 2/10:  mini-batch 1735/4459:  Train loss: 3.0390713214874268  Test loss: 3.3242647647857666 \n",
      "Epoch: 2/10:  mini-batch 1736/4459:  Train loss: 3.302368640899658  Test loss: 3.3241395950317383 \n",
      "Epoch: 2/10:  mini-batch 1737/4459:  Train loss: 3.2386667728424072  Test loss: 3.324054718017578 \n",
      "Epoch: 2/10:  mini-batch 1738/4459:  Train loss: 3.167954444885254  Test loss: 3.3244149684906006 \n",
      "Epoch: 2/10:  mini-batch 1739/4459:  Train loss: 3.3359322547912598  Test loss: 3.324561595916748 \n",
      "Epoch: 2/10:  mini-batch 1740/4459:  Train loss: 3.4355766773223877  Test loss: 3.3243913650512695 \n",
      "Epoch: 2/10:  mini-batch 1741/4459:  Train loss: 3.0905447006225586  Test loss: 3.32452392578125 \n",
      "Epoch: 2/10:  mini-batch 1742/4459:  Train loss: 3.4392194747924805  Test loss: 3.3247811794281006 \n",
      "Epoch: 2/10:  mini-batch 1743/4459:  Train loss: 3.6977498531341553  Test loss: 3.3251192569732666 \n",
      "Epoch: 2/10:  mini-batch 1744/4459:  Train loss: 3.6263184547424316  Test loss: 3.325409173965454 \n",
      "Epoch: 2/10:  mini-batch 1745/4459:  Train loss: 3.6484622955322266  Test loss: 3.3253045082092285 \n",
      "Epoch: 2/10:  mini-batch 1746/4459:  Train loss: 3.338454246520996  Test loss: 3.3257641792297363 \n",
      "Epoch: 2/10:  mini-batch 1747/4459:  Train loss: 3.0499911308288574  Test loss: 3.326049566268921 \n",
      "Epoch: 2/10:  mini-batch 1748/4459:  Train loss: 3.219841241836548  Test loss: 3.3262407779693604 \n",
      "Epoch: 2/10:  mini-batch 1749/4459:  Train loss: 3.4297218322753906  Test loss: 3.3265128135681152 \n",
      "Epoch: 2/10:  mini-batch 1750/4459:  Train loss: 3.428900718688965  Test loss: 3.3268868923187256 \n",
      "Epoch: 2/10:  mini-batch 1751/4459:  Train loss: 2.9395463466644287  Test loss: 3.3271431922912598 \n",
      "Epoch: 2/10:  mini-batch 1752/4459:  Train loss: 3.5500311851501465  Test loss: 3.327523708343506 \n",
      "Epoch: 2/10:  mini-batch 1753/4459:  Train loss: 3.908688545227051  Test loss: 3.328261375427246 \n",
      "Epoch: 2/10:  mini-batch 1754/4459:  Train loss: 3.6542606353759766  Test loss: 3.3294413089752197 \n",
      "Epoch: 2/10:  mini-batch 1755/4459:  Train loss: 3.202200174331665  Test loss: 3.3307344913482666 \n",
      "Epoch: 2/10:  mini-batch 1756/4459:  Train loss: 3.5964927673339844  Test loss: 3.3326737880706787 \n",
      "Epoch: 2/10:  mini-batch 1757/4459:  Train loss: 3.515904426574707  Test loss: 3.3351712226867676 \n",
      "Epoch: 2/10:  mini-batch 1758/4459:  Train loss: 3.183176040649414  Test loss: 3.336883544921875 \n",
      "Epoch: 2/10:  mini-batch 1759/4459:  Train loss: 2.916656017303467  Test loss: 3.3365368843078613 \n",
      "Epoch: 2/10:  mini-batch 1760/4459:  Train loss: 3.343158721923828  Test loss: 3.3361997604370117 \n",
      "Epoch: 2/10:  mini-batch 1761/4459:  Train loss: 3.01507830619812  Test loss: 3.335106134414673 \n",
      "Epoch: 2/10:  mini-batch 1762/4459:  Train loss: 3.626809597015381  Test loss: 3.3349568843841553 \n",
      "Epoch: 2/10:  mini-batch 1763/4459:  Train loss: 3.6002566814422607  Test loss: 3.3353071212768555 \n",
      "Epoch: 2/10:  mini-batch 1764/4459:  Train loss: 3.309380054473877  Test loss: 3.3353946208953857 \n",
      "Epoch: 2/10:  mini-batch 1765/4459:  Train loss: 3.4562478065490723  Test loss: 3.335305690765381 \n",
      "Epoch: 2/10:  mini-batch 1766/4459:  Train loss: 3.3045589923858643  Test loss: 3.3353443145751953 \n",
      "Epoch: 2/10:  mini-batch 1767/4459:  Train loss: 2.938512086868286  Test loss: 3.334867477416992 \n",
      "Epoch: 2/10:  mini-batch 1768/4459:  Train loss: 3.468153953552246  Test loss: 3.3344945907592773 \n",
      "Epoch: 2/10:  mini-batch 1769/4459:  Train loss: 3.503948211669922  Test loss: 3.334360122680664 \n",
      "Epoch: 2/10:  mini-batch 1770/4459:  Train loss: 3.5608880519866943  Test loss: 3.334472179412842 \n",
      "Epoch: 2/10:  mini-batch 1771/4459:  Train loss: 3.346261978149414  Test loss: 3.3346524238586426 \n",
      "Epoch: 2/10:  mini-batch 1772/4459:  Train loss: 3.7779717445373535  Test loss: 3.335304021835327 \n",
      "Epoch: 2/10:  mini-batch 1773/4459:  Train loss: 3.2246735095977783  Test loss: 3.3357224464416504 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1774/4459:  Train loss: 3.1680054664611816  Test loss: 3.3360719680786133 \n",
      "Epoch: 2/10:  mini-batch 1775/4459:  Train loss: 3.143526315689087  Test loss: 3.3361878395080566 \n",
      "Epoch: 2/10:  mini-batch 1776/4459:  Train loss: 3.3655147552490234  Test loss: 3.3365845680236816 \n",
      "Epoch: 2/10:  mini-batch 1777/4459:  Train loss: 3.2314915657043457  Test loss: 3.3369669914245605 \n",
      "Epoch: 2/10:  mini-batch 1778/4459:  Train loss: 2.9342403411865234  Test loss: 3.3372578620910645 \n",
      "Epoch: 2/10:  mini-batch 1779/4459:  Train loss: 2.9234821796417236  Test loss: 3.3376121520996094 \n",
      "Epoch: 2/10:  mini-batch 1780/4459:  Train loss: 2.9961752891540527  Test loss: 3.3380417823791504 \n",
      "Epoch: 2/10:  mini-batch 1781/4459:  Train loss: 3.6323533058166504  Test loss: 3.3386149406433105 \n",
      "Epoch: 2/10:  mini-batch 1782/4459:  Train loss: 3.3401575088500977  Test loss: 3.339142322540283 \n",
      "Epoch: 2/10:  mini-batch 1783/4459:  Train loss: 3.430330276489258  Test loss: 3.3401119709014893 \n",
      "Epoch: 2/10:  mini-batch 1784/4459:  Train loss: 3.6491565704345703  Test loss: 3.3413143157958984 \n",
      "Epoch: 2/10:  mini-batch 1785/4459:  Train loss: 3.664376735687256  Test loss: 3.342635154724121 \n",
      "Epoch: 2/10:  mini-batch 1786/4459:  Train loss: 3.3570218086242676  Test loss: 3.343798875808716 \n",
      "Epoch: 2/10:  mini-batch 1787/4459:  Train loss: 3.217634677886963  Test loss: 3.344758987426758 \n",
      "Epoch: 2/10:  mini-batch 1788/4459:  Train loss: 3.516491413116455  Test loss: 3.345480442047119 \n",
      "Epoch: 2/10:  mini-batch 1789/4459:  Train loss: 3.205237865447998  Test loss: 3.3458480834960938 \n",
      "Epoch: 2/10:  mini-batch 1790/4459:  Train loss: 3.28975510597229  Test loss: 3.3461053371429443 \n",
      "Epoch: 2/10:  mini-batch 1791/4459:  Train loss: 3.627383232116699  Test loss: 3.346559762954712 \n",
      "Epoch: 2/10:  mini-batch 1792/4459:  Train loss: 3.4380252361297607  Test loss: 3.3467540740966797 \n",
      "Epoch: 2/10:  mini-batch 1793/4459:  Train loss: 3.566710948944092  Test loss: 3.347127914428711 \n",
      "Epoch: 2/10:  mini-batch 1794/4459:  Train loss: 3.5013434886932373  Test loss: 3.3474931716918945 \n",
      "Epoch: 2/10:  mini-batch 1795/4459:  Train loss: 3.326904773712158  Test loss: 3.3477864265441895 \n",
      "Epoch: 2/10:  mini-batch 1796/4459:  Train loss: 3.0625085830688477  Test loss: 3.3482449054718018 \n",
      "Epoch: 2/10:  mini-batch 1797/4459:  Train loss: 3.4457762241363525  Test loss: 3.3487606048583984 \n",
      "Epoch: 2/10:  mini-batch 1798/4459:  Train loss: 3.6150219440460205  Test loss: 3.349712371826172 \n",
      "Epoch: 2/10:  mini-batch 1799/4459:  Train loss: 3.4715821743011475  Test loss: 3.3506550788879395 \n",
      "Epoch: 2/10:  mini-batch 1800/4459:  Train loss: 3.834199905395508  Test loss: 3.3515634536743164 \n",
      "Epoch: 2/10:  mini-batch 1801/4459:  Train loss: 3.9110352993011475  Test loss: 3.352659225463867 \n",
      "Epoch: 2/10:  mini-batch 1802/4459:  Train loss: 3.3091938495635986  Test loss: 3.353818655014038 \n",
      "Epoch: 2/10:  mini-batch 1803/4459:  Train loss: 3.422002077102661  Test loss: 3.3549818992614746 \n",
      "Epoch: 2/10:  mini-batch 1804/4459:  Train loss: 3.1736645698547363  Test loss: 3.3561434745788574 \n",
      "Epoch: 2/10:  mini-batch 1805/4459:  Train loss: 3.4243080615997314  Test loss: 3.357624053955078 \n",
      "Epoch: 2/10:  mini-batch 1806/4459:  Train loss: 3.5190625190734863  Test loss: 3.3592729568481445 \n",
      "Epoch: 2/10:  mini-batch 1807/4459:  Train loss: 3.089412212371826  Test loss: 3.3605284690856934 \n",
      "Epoch: 2/10:  mini-batch 1808/4459:  Train loss: 3.2718865871429443  Test loss: 3.3617215156555176 \n",
      "Epoch: 2/10:  mini-batch 1809/4459:  Train loss: 3.2630863189697266  Test loss: 3.362985610961914 \n",
      "Epoch: 2/10:  mini-batch 1810/4459:  Train loss: 3.0530383586883545  Test loss: 3.3640596866607666 \n",
      "Epoch: 2/10:  mini-batch 1811/4459:  Train loss: 3.3193726539611816  Test loss: 3.3651952743530273 \n",
      "Epoch: 2/10:  mini-batch 1812/4459:  Train loss: 3.183560848236084  Test loss: 3.3660895824432373 \n",
      "Epoch: 2/10:  mini-batch 1813/4459:  Train loss: 3.343968391418457  Test loss: 3.366945266723633 \n",
      "Epoch: 2/10:  mini-batch 1814/4459:  Train loss: 3.106905460357666  Test loss: 3.367361068725586 \n",
      "Epoch: 2/10:  mini-batch 1815/4459:  Train loss: 3.2020015716552734  Test loss: 3.3677189350128174 \n",
      "Epoch: 2/10:  mini-batch 1816/4459:  Train loss: 3.2418265342712402  Test loss: 3.3683230876922607 \n",
      "Epoch: 2/10:  mini-batch 1817/4459:  Train loss: 3.667632579803467  Test loss: 3.368783950805664 \n",
      "Epoch: 2/10:  mini-batch 1818/4459:  Train loss: 3.2836132049560547  Test loss: 3.369309902191162 \n",
      "Epoch: 2/10:  mini-batch 1819/4459:  Train loss: 3.33884859085083  Test loss: 3.369290828704834 \n",
      "Epoch: 2/10:  mini-batch 1820/4459:  Train loss: 3.2788071632385254  Test loss: 3.369126796722412 \n",
      "Epoch: 2/10:  mini-batch 1821/4459:  Train loss: 3.436298370361328  Test loss: 3.3692209720611572 \n",
      "Epoch: 2/10:  mini-batch 1822/4459:  Train loss: 3.4293737411499023  Test loss: 3.369204044342041 \n",
      "Epoch: 2/10:  mini-batch 1823/4459:  Train loss: 3.562392234802246  Test loss: 3.36922550201416 \n",
      "Epoch: 2/10:  mini-batch 1824/4459:  Train loss: 3.133942127227783  Test loss: 3.3692245483398438 \n",
      "Epoch: 2/10:  mini-batch 1825/4459:  Train loss: 3.123960018157959  Test loss: 3.3692097663879395 \n",
      "Epoch: 2/10:  mini-batch 1826/4459:  Train loss: 3.3023953437805176  Test loss: 3.368974447250366 \n",
      "Epoch: 2/10:  mini-batch 1827/4459:  Train loss: 3.7158138751983643  Test loss: 3.3687961101531982 \n",
      "Epoch: 2/10:  mini-batch 1828/4459:  Train loss: 3.1228108406066895  Test loss: 3.3685946464538574 \n",
      "Epoch: 2/10:  mini-batch 1829/4459:  Train loss: 3.198254108428955  Test loss: 3.36838436126709 \n",
      "Epoch: 2/10:  mini-batch 1830/4459:  Train loss: 3.4728167057037354  Test loss: 3.367987871170044 \n",
      "Epoch: 2/10:  mini-batch 1831/4459:  Train loss: 3.1672658920288086  Test loss: 3.3676228523254395 \n",
      "Epoch: 2/10:  mini-batch 1832/4459:  Train loss: 3.525407075881958  Test loss: 3.367553234100342 \n",
      "Epoch: 2/10:  mini-batch 1833/4459:  Train loss: 3.5572588443756104  Test loss: 3.367436408996582 \n",
      "Epoch: 2/10:  mini-batch 1834/4459:  Train loss: 3.6335058212280273  Test loss: 3.3673031330108643 \n",
      "Epoch: 2/10:  mini-batch 1835/4459:  Train loss: 3.253361225128174  Test loss: 3.3673410415649414 \n",
      "Epoch: 2/10:  mini-batch 1836/4459:  Train loss: 3.594287872314453  Test loss: 3.367410182952881 \n",
      "Epoch: 2/10:  mini-batch 1837/4459:  Train loss: 3.2616684436798096  Test loss: 3.367225170135498 \n",
      "Epoch: 2/10:  mini-batch 1838/4459:  Train loss: 3.358022928237915  Test loss: 3.367316722869873 \n",
      "Epoch: 2/10:  mini-batch 1839/4459:  Train loss: 3.1221418380737305  Test loss: 3.3671669960021973 \n",
      "Epoch: 2/10:  mini-batch 1840/4459:  Train loss: 3.3738784790039062  Test loss: 3.367194890975952 \n",
      "Epoch: 2/10:  mini-batch 1841/4459:  Train loss: 3.444720506668091  Test loss: 3.3670008182525635 \n",
      "Epoch: 2/10:  mini-batch 1842/4459:  Train loss: 3.211352825164795  Test loss: 3.3670787811279297 \n",
      "Epoch: 2/10:  mini-batch 1843/4459:  Train loss: 3.3019232749938965  Test loss: 3.3670706748962402 \n",
      "Epoch: 2/10:  mini-batch 1844/4459:  Train loss: 3.5179595947265625  Test loss: 3.366868257522583 \n",
      "Epoch: 2/10:  mini-batch 1845/4459:  Train loss: 3.70328426361084  Test loss: 3.366604804992676 \n",
      "Epoch: 2/10:  mini-batch 1846/4459:  Train loss: 3.100882053375244  Test loss: 3.366549253463745 \n",
      "Epoch: 2/10:  mini-batch 1847/4459:  Train loss: 3.2405834197998047  Test loss: 3.3665385246276855 \n",
      "Epoch: 2/10:  mini-batch 1848/4459:  Train loss: 3.1709530353546143  Test loss: 3.3665130138397217 \n",
      "Epoch: 2/10:  mini-batch 1849/4459:  Train loss: 3.1944358348846436  Test loss: 3.366429328918457 \n",
      "Epoch: 2/10:  mini-batch 1850/4459:  Train loss: 3.1686174869537354  Test loss: 3.3664989471435547 \n",
      "Epoch: 2/10:  mini-batch 1851/4459:  Train loss: 3.228273391723633  Test loss: 3.3668782711029053 \n",
      "Epoch: 2/10:  mini-batch 1852/4459:  Train loss: 3.467578172683716  Test loss: 3.367199182510376 \n",
      "Epoch: 2/10:  mini-batch 1853/4459:  Train loss: 3.393584728240967  Test loss: 3.367764472961426 \n",
      "Epoch: 2/10:  mini-batch 1854/4459:  Train loss: 3.6541190147399902  Test loss: 3.36820650100708 \n",
      "Epoch: 2/10:  mini-batch 1855/4459:  Train loss: 3.6488895416259766  Test loss: 3.3687901496887207 \n",
      "Epoch: 2/10:  mini-batch 1856/4459:  Train loss: 3.5193634033203125  Test loss: 3.36917781829834 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1857/4459:  Train loss: 3.3471245765686035  Test loss: 3.369520664215088 \n",
      "Epoch: 2/10:  mini-batch 1858/4459:  Train loss: 3.4733433723449707  Test loss: 3.369843006134033 \n",
      "Epoch: 2/10:  mini-batch 1859/4459:  Train loss: 3.4076244831085205  Test loss: 3.370347738265991 \n",
      "Epoch: 2/10:  mini-batch 1860/4459:  Train loss: 3.4754018783569336  Test loss: 3.370495319366455 \n",
      "Epoch: 2/10:  mini-batch 1861/4459:  Train loss: 3.2805774211883545  Test loss: 3.370849847793579 \n",
      "Epoch: 2/10:  mini-batch 1862/4459:  Train loss: 3.3091306686401367  Test loss: 3.371244430541992 \n",
      "Epoch: 2/10:  mini-batch 1863/4459:  Train loss: 3.337725877761841  Test loss: 3.3713560104370117 \n",
      "Epoch: 2/10:  mini-batch 1864/4459:  Train loss: 3.286978006362915  Test loss: 3.371645450592041 \n",
      "Epoch: 2/10:  mini-batch 1865/4459:  Train loss: 3.0136477947235107  Test loss: 3.3718104362487793 \n",
      "Epoch: 2/10:  mini-batch 1866/4459:  Train loss: 3.181356430053711  Test loss: 3.3719019889831543 \n",
      "Epoch: 2/10:  mini-batch 1867/4459:  Train loss: 3.31355619430542  Test loss: 3.372114896774292 \n",
      "Epoch: 2/10:  mini-batch 1868/4459:  Train loss: 3.452653169631958  Test loss: 3.3720688819885254 \n",
      "Epoch: 2/10:  mini-batch 1869/4459:  Train loss: 3.3696062564849854  Test loss: 3.371886730194092 \n",
      "Epoch: 2/10:  mini-batch 1870/4459:  Train loss: 3.281754970550537  Test loss: 3.3716723918914795 \n",
      "Epoch: 2/10:  mini-batch 1871/4459:  Train loss: 3.156757354736328  Test loss: 3.3714466094970703 \n",
      "Epoch: 2/10:  mini-batch 1872/4459:  Train loss: 3.256190776824951  Test loss: 3.371443271636963 \n",
      "Epoch: 2/10:  mini-batch 1873/4459:  Train loss: 3.5205512046813965  Test loss: 3.371593475341797 \n",
      "Epoch: 2/10:  mini-batch 1874/4459:  Train loss: 3.655869245529175  Test loss: 3.3718786239624023 \n",
      "Epoch: 2/10:  mini-batch 1875/4459:  Train loss: 3.1940581798553467  Test loss: 3.3719208240509033 \n",
      "Epoch: 2/10:  mini-batch 1876/4459:  Train loss: 3.282897472381592  Test loss: 3.3717713356018066 \n",
      "Epoch: 2/10:  mini-batch 1877/4459:  Train loss: 3.641303300857544  Test loss: 3.3715381622314453 \n",
      "Epoch: 2/10:  mini-batch 1878/4459:  Train loss: 3.9682350158691406  Test loss: 3.3712825775146484 \n",
      "Epoch: 2/10:  mini-batch 1879/4459:  Train loss: 3.510924816131592  Test loss: 3.3709752559661865 \n",
      "Epoch: 2/10:  mini-batch 1880/4459:  Train loss: 3.3225278854370117  Test loss: 3.370939016342163 \n",
      "Epoch: 2/10:  mini-batch 1881/4459:  Train loss: 3.3645219802856445  Test loss: 3.371103525161743 \n",
      "Epoch: 2/10:  mini-batch 1882/4459:  Train loss: 3.289593458175659  Test loss: 3.371262550354004 \n",
      "Epoch: 2/10:  mini-batch 1883/4459:  Train loss: 3.1438121795654297  Test loss: 3.371184825897217 \n",
      "Epoch: 2/10:  mini-batch 1884/4459:  Train loss: 3.3025119304656982  Test loss: 3.3711445331573486 \n",
      "Epoch: 2/10:  mini-batch 1885/4459:  Train loss: 3.456317186355591  Test loss: 3.3712334632873535 \n",
      "Epoch: 2/10:  mini-batch 1886/4459:  Train loss: 3.4087343215942383  Test loss: 3.371253252029419 \n",
      "Epoch: 2/10:  mini-batch 1887/4459:  Train loss: 3.412325143814087  Test loss: 3.371342182159424 \n",
      "Epoch: 2/10:  mini-batch 1888/4459:  Train loss: 3.3589179515838623  Test loss: 3.3712801933288574 \n",
      "Epoch: 2/10:  mini-batch 1889/4459:  Train loss: 3.603294849395752  Test loss: 3.371432304382324 \n",
      "Epoch: 2/10:  mini-batch 1890/4459:  Train loss: 3.386309862136841  Test loss: 3.3720829486846924 \n",
      "Epoch: 2/10:  mini-batch 1891/4459:  Train loss: 3.249113082885742  Test loss: 3.3725733757019043 \n",
      "Epoch: 2/10:  mini-batch 1892/4459:  Train loss: 3.319199800491333  Test loss: 3.3729684352874756 \n",
      "Epoch: 2/10:  mini-batch 1893/4459:  Train loss: 3.4378156661987305  Test loss: 3.373478889465332 \n",
      "Epoch: 2/10:  mini-batch 1894/4459:  Train loss: 3.442990303039551  Test loss: 3.373931407928467 \n",
      "Epoch: 2/10:  mini-batch 1895/4459:  Train loss: 3.653615951538086  Test loss: 3.374356269836426 \n",
      "Epoch: 2/10:  mini-batch 1896/4459:  Train loss: 3.389333963394165  Test loss: 3.3746955394744873 \n",
      "Epoch: 2/10:  mini-batch 1897/4459:  Train loss: 3.8442816734313965  Test loss: 3.3752493858337402 \n",
      "Epoch: 2/10:  mini-batch 1898/4459:  Train loss: 3.8411998748779297  Test loss: 3.3761157989501953 \n",
      "Epoch: 2/10:  mini-batch 1899/4459:  Train loss: 3.463369846343994  Test loss: 3.377072811126709 \n",
      "Epoch: 2/10:  mini-batch 1900/4459:  Train loss: 3.39030122756958  Test loss: 3.3779571056365967 \n",
      "Epoch: 2/10:  mini-batch 1901/4459:  Train loss: 3.2752833366394043  Test loss: 3.3788399696350098 \n",
      "Epoch: 2/10:  mini-batch 1902/4459:  Train loss: 3.3259308338165283  Test loss: 3.379730224609375 \n",
      "Epoch: 2/10:  mini-batch 1903/4459:  Train loss: 3.4415011405944824  Test loss: 3.380744457244873 \n",
      "Epoch: 2/10:  mini-batch 1904/4459:  Train loss: 3.4603726863861084  Test loss: 3.381875514984131 \n",
      "Epoch: 2/10:  mini-batch 1905/4459:  Train loss: 3.517805337905884  Test loss: 3.3830978870391846 \n",
      "Epoch: 2/10:  mini-batch 1906/4459:  Train loss: 3.3624634742736816  Test loss: 3.384199619293213 \n",
      "Epoch: 2/10:  mini-batch 1907/4459:  Train loss: 3.1720497608184814  Test loss: 3.3850111961364746 \n",
      "Epoch: 2/10:  mini-batch 1908/4459:  Train loss: 3.068690538406372  Test loss: 3.3855695724487305 \n",
      "Epoch: 2/10:  mini-batch 1909/4459:  Train loss: 3.4306676387786865  Test loss: 3.386350154876709 \n",
      "Epoch: 2/10:  mini-batch 1910/4459:  Train loss: 3.670128107070923  Test loss: 3.387446165084839 \n",
      "Epoch: 2/10:  mini-batch 1911/4459:  Train loss: 3.420090675354004  Test loss: 3.38832950592041 \n",
      "Epoch: 2/10:  mini-batch 1912/4459:  Train loss: 3.7537598609924316  Test loss: 3.3894729614257812 \n",
      "Epoch: 2/10:  mini-batch 1913/4459:  Train loss: 3.421595811843872  Test loss: 3.3908491134643555 \n",
      "Epoch: 2/10:  mini-batch 1914/4459:  Train loss: 3.3954741954803467  Test loss: 3.392392635345459 \n",
      "Epoch: 2/10:  mini-batch 1915/4459:  Train loss: 3.2484240531921387  Test loss: 3.393354654312134 \n",
      "Epoch: 2/10:  mini-batch 1916/4459:  Train loss: 3.334258794784546  Test loss: 3.3947315216064453 \n",
      "Epoch: 2/10:  mini-batch 1917/4459:  Train loss: 3.330906391143799  Test loss: 3.39669132232666 \n",
      "Epoch: 2/10:  mini-batch 1918/4459:  Train loss: 3.677367687225342  Test loss: 3.399111747741699 \n",
      "Epoch: 2/10:  mini-batch 1919/4459:  Train loss: 3.290870428085327  Test loss: 3.402522563934326 \n",
      "Epoch: 2/10:  mini-batch 1920/4459:  Train loss: 3.215224504470825  Test loss: 3.4069552421569824 \n",
      "Epoch: 2/10:  mini-batch 1921/4459:  Train loss: 3.1315996646881104  Test loss: 3.412126064300537 \n",
      "Epoch: 2/10:  mini-batch 1922/4459:  Train loss: 3.185076951980591  Test loss: 3.4148616790771484 \n",
      "Epoch: 2/10:  mini-batch 1923/4459:  Train loss: 3.2710883617401123  Test loss: 3.416313648223877 \n",
      "Epoch: 2/10:  mini-batch 1924/4459:  Train loss: 3.3862898349761963  Test loss: 3.416494846343994 \n",
      "Epoch: 2/10:  mini-batch 1925/4459:  Train loss: 3.172027111053467  Test loss: 3.416931629180908 \n",
      "Epoch: 2/10:  mini-batch 1926/4459:  Train loss: 3.519223928451538  Test loss: 3.4177005290985107 \n",
      "Epoch: 2/10:  mini-batch 1927/4459:  Train loss: 3.7148704528808594  Test loss: 3.416131019592285 \n",
      "Epoch: 2/10:  mini-batch 1928/4459:  Train loss: 3.4058635234832764  Test loss: 3.4139931201934814 \n",
      "Epoch: 2/10:  mini-batch 1929/4459:  Train loss: 3.108126401901245  Test loss: 3.4124464988708496 \n",
      "Epoch: 2/10:  mini-batch 1930/4459:  Train loss: 3.3586363792419434  Test loss: 3.4100496768951416 \n",
      "Epoch: 2/10:  mini-batch 1931/4459:  Train loss: 3.358555555343628  Test loss: 3.408442497253418 \n",
      "Epoch: 2/10:  mini-batch 1932/4459:  Train loss: 3.684675693511963  Test loss: 3.406156539916992 \n",
      "Epoch: 2/10:  mini-batch 1933/4459:  Train loss: 3.4092702865600586  Test loss: 3.4048285484313965 \n",
      "Epoch: 2/10:  mini-batch 1934/4459:  Train loss: 3.342721939086914  Test loss: 3.4033865928649902 \n",
      "Epoch: 2/10:  mini-batch 1935/4459:  Train loss: 3.7048511505126953  Test loss: 3.401930809020996 \n",
      "Epoch: 2/10:  mini-batch 1936/4459:  Train loss: 3.066593885421753  Test loss: 3.4008841514587402 \n",
      "Epoch: 2/10:  mini-batch 1937/4459:  Train loss: 3.0733675956726074  Test loss: 3.399667501449585 \n",
      "Epoch: 2/10:  mini-batch 1938/4459:  Train loss: 3.3069231510162354  Test loss: 3.3994245529174805 \n",
      "Epoch: 2/10:  mini-batch 1939/4459:  Train loss: 3.008192539215088  Test loss: 3.399252414703369 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 1940/4459:  Train loss: 3.14365816116333  Test loss: 3.3993797302246094 \n",
      "Epoch: 2/10:  mini-batch 1941/4459:  Train loss: 3.197895050048828  Test loss: 3.399432420730591 \n",
      "Epoch: 2/10:  mini-batch 1942/4459:  Train loss: 3.396986246109009  Test loss: 3.39955997467041 \n",
      "Epoch: 2/10:  mini-batch 1943/4459:  Train loss: 3.2438645362854004  Test loss: 3.4007959365844727 \n",
      "Epoch: 2/10:  mini-batch 1944/4459:  Train loss: 3.3907294273376465  Test loss: 3.403146266937256 \n",
      "Epoch: 2/10:  mini-batch 1945/4459:  Train loss: 3.369664430618286  Test loss: 3.4045910835266113 \n",
      "Epoch: 2/10:  mini-batch 1946/4459:  Train loss: 3.1921966075897217  Test loss: 3.4064855575561523 \n",
      "Epoch: 2/10:  mini-batch 1947/4459:  Train loss: 3.420464515686035  Test loss: 3.406790256500244 \n",
      "Epoch: 2/10:  mini-batch 1948/4459:  Train loss: 3.3309597969055176  Test loss: 3.407956123352051 \n",
      "Epoch: 2/10:  mini-batch 1949/4459:  Train loss: 3.4746289253234863  Test loss: 3.407832145690918 \n",
      "Epoch: 2/10:  mini-batch 1950/4459:  Train loss: 3.252869129180908  Test loss: 3.4070143699645996 \n",
      "Epoch: 2/10:  mini-batch 1951/4459:  Train loss: 3.7918152809143066  Test loss: 3.404383897781372 \n",
      "Epoch: 2/10:  mini-batch 1952/4459:  Train loss: 3.161886215209961  Test loss: 3.4028851985931396 \n",
      "Epoch: 2/10:  mini-batch 1953/4459:  Train loss: 3.1005353927612305  Test loss: 3.4019405841827393 \n",
      "Epoch: 2/10:  mini-batch 1954/4459:  Train loss: 3.071835994720459  Test loss: 3.402076244354248 \n",
      "Epoch: 2/10:  mini-batch 1955/4459:  Train loss: 3.115157127380371  Test loss: 3.4021501541137695 \n",
      "Epoch: 2/10:  mini-batch 1956/4459:  Train loss: 3.264270067214966  Test loss: 3.4026687145233154 \n",
      "Epoch: 2/10:  mini-batch 1957/4459:  Train loss: 3.526388645172119  Test loss: 3.40195369720459 \n",
      "Epoch: 2/10:  mini-batch 1958/4459:  Train loss: 3.3511505126953125  Test loss: 3.400634765625 \n",
      "Epoch: 2/10:  mini-batch 1959/4459:  Train loss: 3.683506965637207  Test loss: 3.3982760906219482 \n",
      "Epoch: 2/10:  mini-batch 1960/4459:  Train loss: 3.5378737449645996  Test loss: 3.396157741546631 \n",
      "Epoch: 2/10:  mini-batch 1961/4459:  Train loss: 3.4452295303344727  Test loss: 3.395185947418213 \n",
      "Epoch: 2/10:  mini-batch 1962/4459:  Train loss: 3.1801400184631348  Test loss: 3.395063877105713 \n",
      "Epoch: 2/10:  mini-batch 1963/4459:  Train loss: 3.3719582557678223  Test loss: 3.395106554031372 \n",
      "Epoch: 2/10:  mini-batch 1964/4459:  Train loss: 3.0932905673980713  Test loss: 3.395768165588379 \n",
      "Epoch: 2/10:  mini-batch 1965/4459:  Train loss: 3.2002835273742676  Test loss: 3.3969790935516357 \n",
      "Epoch: 2/10:  mini-batch 1966/4459:  Train loss: 3.192070722579956  Test loss: 3.398470163345337 \n",
      "Epoch: 2/10:  mini-batch 1967/4459:  Train loss: 3.4425787925720215  Test loss: 3.3999152183532715 \n",
      "Epoch: 2/10:  mini-batch 1968/4459:  Train loss: 3.775376796722412  Test loss: 3.400763988494873 \n",
      "Epoch: 2/10:  mini-batch 1969/4459:  Train loss: 3.1813108921051025  Test loss: 3.401900291442871 \n",
      "Epoch: 2/10:  mini-batch 1970/4459:  Train loss: 3.2552595138549805  Test loss: 3.4029414653778076 \n",
      "Epoch: 2/10:  mini-batch 1971/4459:  Train loss: 3.5990896224975586  Test loss: 3.4040160179138184 \n",
      "Epoch: 2/10:  mini-batch 1972/4459:  Train loss: 3.387176752090454  Test loss: 3.4046573638916016 \n",
      "Epoch: 2/10:  mini-batch 1973/4459:  Train loss: 3.3042259216308594  Test loss: 3.4050469398498535 \n",
      "Epoch: 2/10:  mini-batch 1974/4459:  Train loss: 3.412815570831299  Test loss: 3.405379295349121 \n",
      "Epoch: 2/10:  mini-batch 1975/4459:  Train loss: 3.4355082511901855  Test loss: 3.4055590629577637 \n",
      "Epoch: 2/10:  mini-batch 1976/4459:  Train loss: 3.4388656616210938  Test loss: 3.4054741859436035 \n",
      "Epoch: 2/10:  mini-batch 1977/4459:  Train loss: 3.4849705696105957  Test loss: 3.4053449630737305 \n",
      "Epoch: 2/10:  mini-batch 1978/4459:  Train loss: 3.1370086669921875  Test loss: 3.4056589603424072 \n",
      "Epoch: 2/10:  mini-batch 1979/4459:  Train loss: 3.139392852783203  Test loss: 3.4065518379211426 \n",
      "Epoch: 2/10:  mini-batch 1980/4459:  Train loss: 3.43456768989563  Test loss: 3.4074881076812744 \n",
      "Epoch: 2/10:  mini-batch 1981/4459:  Train loss: 3.0890932083129883  Test loss: 3.4089457988739014 \n",
      "Epoch: 2/10:  mini-batch 1982/4459:  Train loss: 3.618900775909424  Test loss: 3.410109281539917 \n",
      "Epoch: 2/10:  mini-batch 1983/4459:  Train loss: 3.6228127479553223  Test loss: 3.4114534854888916 \n",
      "Epoch: 2/10:  mini-batch 1984/4459:  Train loss: 3.303118944168091  Test loss: 3.41272234916687 \n",
      "Epoch: 2/10:  mini-batch 1985/4459:  Train loss: 3.1650233268737793  Test loss: 3.4142231941223145 \n",
      "Epoch: 2/10:  mini-batch 1986/4459:  Train loss: 3.2549376487731934  Test loss: 3.4160850048065186 \n",
      "Epoch: 2/10:  mini-batch 1987/4459:  Train loss: 3.5122361183166504  Test loss: 3.4179162979125977 \n",
      "Epoch: 2/10:  mini-batch 1988/4459:  Train loss: 3.284299612045288  Test loss: 3.4197487831115723 \n",
      "Epoch: 2/10:  mini-batch 1989/4459:  Train loss: 3.3394722938537598  Test loss: 3.4210853576660156 \n",
      "Epoch: 2/10:  mini-batch 1990/4459:  Train loss: 3.231550931930542  Test loss: 3.421929359436035 \n",
      "Epoch: 2/10:  mini-batch 1991/4459:  Train loss: 3.2395877838134766  Test loss: 3.4224328994750977 \n",
      "Epoch: 2/10:  mini-batch 1992/4459:  Train loss: 3.281442642211914  Test loss: 3.422968864440918 \n",
      "Epoch: 2/10:  mini-batch 1993/4459:  Train loss: 3.1768007278442383  Test loss: 3.4234492778778076 \n",
      "Epoch: 2/10:  mini-batch 1994/4459:  Train loss: 3.128016233444214  Test loss: 3.4239320755004883 \n",
      "Epoch: 2/10:  mini-batch 1995/4459:  Train loss: 3.368928909301758  Test loss: 3.4242420196533203 \n",
      "Epoch: 2/10:  mini-batch 1996/4459:  Train loss: 3.223076820373535  Test loss: 3.4248530864715576 \n",
      "Epoch: 2/10:  mini-batch 1997/4459:  Train loss: 3.1839914321899414  Test loss: 3.4257869720458984 \n",
      "Epoch: 2/10:  mini-batch 1998/4459:  Train loss: 3.492842674255371  Test loss: 3.426669120788574 \n",
      "Epoch: 2/10:  mini-batch 1999/4459:  Train loss: 3.5030431747436523  Test loss: 3.4275736808776855 \n",
      "Epoch: 2/10:  mini-batch 2000/4459:  Train loss: 3.683163642883301  Test loss: 3.4285099506378174 \n",
      "Epoch: 2/10:  mini-batch 2001/4459:  Train loss: 3.216008424758911  Test loss: 3.429870128631592 \n",
      "Epoch: 2/10:  mini-batch 2002/4459:  Train loss: 3.485790491104126  Test loss: 3.431398391723633 \n",
      "Epoch: 2/10:  mini-batch 2003/4459:  Train loss: 3.466613292694092  Test loss: 3.432377576828003 \n",
      "Epoch: 2/10:  mini-batch 2004/4459:  Train loss: 3.4594554901123047  Test loss: 3.4333043098449707 \n",
      "Epoch: 2/10:  mini-batch 2005/4459:  Train loss: 3.345189094543457  Test loss: 3.434342861175537 \n",
      "Epoch: 2/10:  mini-batch 2006/4459:  Train loss: 3.32568097114563  Test loss: 3.435112953186035 \n",
      "Epoch: 2/10:  mini-batch 2007/4459:  Train loss: 3.614879846572876  Test loss: 3.4361023902893066 \n",
      "Epoch: 2/10:  mini-batch 2008/4459:  Train loss: 3.20369291305542  Test loss: 3.437269449234009 \n",
      "Epoch: 2/10:  mini-batch 2009/4459:  Train loss: 3.1218700408935547  Test loss: 3.438389778137207 \n",
      "Epoch: 2/10:  mini-batch 2010/4459:  Train loss: 3.2665090560913086  Test loss: 3.4392945766448975 \n",
      "Epoch: 2/10:  mini-batch 2011/4459:  Train loss: 3.257385730743408  Test loss: 3.440847635269165 \n",
      "Epoch: 2/10:  mini-batch 2012/4459:  Train loss: 3.1383466720581055  Test loss: 3.4425244331359863 \n",
      "Epoch: 2/10:  mini-batch 2013/4459:  Train loss: 2.977435350418091  Test loss: 3.4448063373565674 \n",
      "Epoch: 2/10:  mini-batch 2014/4459:  Train loss: 3.1774635314941406  Test loss: 3.4467310905456543 \n",
      "Epoch: 2/10:  mini-batch 2015/4459:  Train loss: 3.2175490856170654  Test loss: 3.4485440254211426 \n",
      "Epoch: 2/10:  mini-batch 2016/4459:  Train loss: 3.262913703918457  Test loss: 3.4500908851623535 \n",
      "Epoch: 2/10:  mini-batch 2017/4459:  Train loss: 3.4468517303466797  Test loss: 3.4511942863464355 \n",
      "Epoch: 2/10:  mini-batch 2018/4459:  Train loss: 3.452117919921875  Test loss: 3.452563524246216 \n",
      "Epoch: 2/10:  mini-batch 2019/4459:  Train loss: 3.3151512145996094  Test loss: 3.4539849758148193 \n",
      "Epoch: 2/10:  mini-batch 2020/4459:  Train loss: 3.0879900455474854  Test loss: 3.4555516242980957 \n",
      "Epoch: 2/10:  mini-batch 2021/4459:  Train loss: 3.0312373638153076  Test loss: 3.457536220550537 \n",
      "Epoch: 2/10:  mini-batch 2022/4459:  Train loss: 3.3916103839874268  Test loss: 3.45859956741333 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2/10:  mini-batch 2023/4459:  Train loss: 3.4150609970092773  Test loss: 3.4588847160339355 \n",
      "Epoch: 2/10:  mini-batch 2024/4459:  Train loss: 3.5448923110961914  Test loss: 3.458601951599121 \n",
      "Epoch: 2/10:  mini-batch 2025/4459:  Train loss: 3.265249729156494  Test loss: 3.458801746368408 \n",
      "Epoch: 2/10:  mini-batch 2026/4459:  Train loss: 3.7945070266723633  Test loss: 3.45780611038208 \n",
      "Epoch: 2/10:  mini-batch 2027/4459:  Train loss: 3.521066188812256  Test loss: 3.456003427505493 \n",
      "Epoch: 2/10:  mini-batch 2028/4459:  Train loss: 3.3252363204956055  Test loss: 3.4550416469573975 \n",
      "Epoch: 2/10:  mini-batch 2029/4459:  Train loss: 3.401651382446289  Test loss: 3.4544293880462646 \n",
      "Epoch: 2/10:  mini-batch 2030/4459:  Train loss: 3.1628284454345703  Test loss: 3.4543709754943848 \n",
      "Epoch: 2/10:  mini-batch 2031/4459:  Train loss: 3.3156471252441406  Test loss: 3.4534733295440674 \n",
      "Epoch: 2/10:  mini-batch 2032/4459:  Train loss: 3.443488359451294  Test loss: 3.4514565467834473 \n",
      "Epoch: 2/10:  mini-batch 2033/4459:  Train loss: 3.389613628387451  Test loss: 3.4490113258361816 \n",
      "Epoch: 2/10:  mini-batch 2034/4459:  Train loss: 3.5699424743652344  Test loss: 3.4461591243743896 \n",
      "Epoch: 2/10:  mini-batch 2035/4459:  Train loss: 3.0259175300598145  Test loss: 3.443978786468506 \n",
      "Epoch: 2/10:  mini-batch 2036/4459:  Train loss: 3.409651756286621  Test loss: 3.441621780395508 \n",
      "Epoch: 2/10:  mini-batch 2037/4459:  Train loss: 3.720029354095459  Test loss: 3.438861846923828 \n",
      "Epoch: 2/10:  mini-batch 2038/4459:  Train loss: 3.553621292114258  Test loss: 3.436490297317505 \n",
      "Epoch: 2/10:  mini-batch 2039/4459:  Train loss: 3.0262575149536133  Test loss: 3.4343526363372803 \n",
      "Epoch: 2/10:  mini-batch 2040/4459:  Train loss: 3.2939000129699707  Test loss: 3.4328651428222656 \n",
      "Epoch: 2/10:  mini-batch 2041/4459:  Train loss: 2.942612648010254  Test loss: 3.4323604106903076 \n",
      "Epoch: 2/10:  mini-batch 2042/4459:  Train loss: 3.056519031524658  Test loss: 3.432316303253174 \n",
      "Epoch: 2/10:  mini-batch 2043/4459:  Train loss: 3.1733670234680176  Test loss: 3.432093381881714 \n",
      "Epoch: 2/10:  mini-batch 2044/4459:  Train loss: 3.218430519104004  Test loss: 3.4314167499542236 \n",
      "Epoch: 2/10:  mini-batch 2045/4459:  Train loss: 3.1819138526916504  Test loss: 3.4311153888702393 \n",
      "Epoch: 2/10:  mini-batch 2046/4459:  Train loss: 3.481438636779785  Test loss: 3.431149959564209 \n",
      "Epoch: 2/10:  mini-batch 2047/4459:  Train loss: 3.1689836978912354  Test loss: 3.4312753677368164 \n",
      "Epoch: 2/10:  mini-batch 2048/4459:  Train loss: 3.092046022415161  Test loss: 3.4315426349639893 \n",
      "Epoch: 2/10:  mini-batch 2049/4459:  Train loss: 3.6540560722351074  Test loss: 3.431117534637451 \n",
      "Epoch: 2/10:  mini-batch 2050/4459:  Train loss: 3.0280981063842773  Test loss: 3.431190013885498 \n",
      "Epoch: 2/10:  mini-batch 2051/4459:  Train loss: 3.1290082931518555  Test loss: 3.431443452835083 \n",
      "Epoch: 2/10:  mini-batch 2052/4459:  Train loss: 3.041865825653076  Test loss: 3.4317944049835205 \n",
      "Epoch: 2/10:  mini-batch 2053/4459:  Train loss: 3.0772910118103027  Test loss: 3.432588577270508 \n",
      "Epoch: 2/10:  mini-batch 2054/4459:  Train loss: 3.179605722427368  Test loss: 3.4334299564361572 \n",
      "Epoch: 2/10:  mini-batch 2055/4459:  Train loss: 3.3268065452575684  Test loss: 3.4346578121185303 \n",
      "Epoch: 2/10:  mini-batch 2056/4459:  Train loss: 3.181713819503784  Test loss: 3.4364113807678223 \n",
      "Epoch: 2/10:  mini-batch 2057/4459:  Train loss: 3.7521066665649414  Test loss: 3.4358654022216797 \n",
      "Epoch: 2/10:  mini-batch 2058/4459:  Train loss: 3.434178113937378  Test loss: 3.435838460922241 \n",
      "Epoch: 2/10:  mini-batch 2059/4459:  Train loss: 3.378868341445923  Test loss: 3.4351913928985596 \n",
      "Epoch: 2/10:  mini-batch 2060/4459:  Train loss: 3.1519079208374023  Test loss: 3.434892177581787 \n",
      "Epoch: 2/10:  mini-batch 2061/4459:  Train loss: 3.348234176635742  Test loss: 3.4347751140594482 \n",
      "Epoch: 2/10:  mini-batch 2062/4459:  Train loss: 3.552008628845215  Test loss: 3.433802843093872 \n",
      "Epoch: 2/10:  mini-batch 2063/4459:  Train loss: 3.266871452331543  Test loss: 3.4324588775634766 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "completed = 0\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "ended = False\n",
    "epoch = 0\n",
    "sess = tf.Session()\n",
    "# Initialize variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "while (epoch < epochs) and not(ended):\n",
    "    completed = int(epoch * 100/epochs)\n",
    "    if completed >= perc:\n",
    "        logging.info(str(perc) + \" % completed\")\n",
    "        perc = int(epoch * 100/epochs)\n",
    "\n",
    "    batch_completed = 0\n",
    "    mini_batch = 1\n",
    "    mini_batch_train_losses = []\n",
    "    mini_batch_test_losses = []\n",
    "    while batch_completed < (len(X_train) - batch_size):\n",
    "        train_X = X_train[batch_completed:(batch_completed + batch_size)]\n",
    "        train_Y = Y_train[batch_completed:(batch_completed + batch_size)]\n",
    "        loss = train(sess, cnn, train_X, train_Y)\n",
    "        # Runs out of memory while evaluating on the complete test set. Only batch_size used for evaluating\n",
    "        # This step should be randomized\n",
    "        test_loss = get_loss(sess, cnn, X_test[0:batch_size], Y_test[0:batch_size])\n",
    "        print('Epoch: {}/{}: '.format(epoch+1, epochs), 'mini-batch {}/{}: '.format(mini_batch, num_batches), \"Train loss: {} \".format(loss), \"Test loss: {} \".format(test_loss))\n",
    "        batch_completed += batch_size\n",
    "        mini_batch += 1\n",
    "        mini_batch_train_losses.append(loss)\n",
    "        mini_batch_test_losses.append(test_loss)\n",
    "    epoch_train_losses.append(sum(mini_batch_train_losses)/len(mini_batch_train_losses))\n",
    "    epoch_test_losses.append(sum(mini_batch_test_losses)/len(mini_batch_test_losses))\n",
    "    # Early stopping check (callback should be used instead):\n",
    "    if (epoch > 0) and (epoch_test_losses[epoch] > epoch_test_losses[epoch - 1]):\n",
    "        saver.save(sess, \"supervised_CNN_full3d_multiclass\")\n",
    "        ended = True\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for random guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.58351893845611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 30471,\n",
       " 4: 46873,\n",
       " 3: 37407,\n",
       " 19: 26342,\n",
       " 35: 11897,\n",
       " 28: 9840,\n",
       " 22: 36674,\n",
       " 34: 8350,\n",
       " 33: 2056,\n",
       " 32: 10720,\n",
       " 31: 13937,\n",
       " 25: 33460,\n",
       " 13: 16128,\n",
       " 10: 10893,\n",
       " 16: 8483,\n",
       " 12: 20853,\n",
       " 14: 4373,\n",
       " 7: 33647,\n",
       " 9: 19946,\n",
       " 15: 5855,\n",
       " 11: 1845,\n",
       " 23: 23448,\n",
       " 29: 4070,\n",
       " 5: 6840,\n",
       " 0: 22925,\n",
       " 6: 7982,\n",
       " 21: 14095,\n",
       " 18: 23582,\n",
       " 24: 6591,\n",
       " 2: 5448,\n",
       " 26: 22685,\n",
       " 17: 7270,\n",
       " 20: 6991,\n",
       " 27: 7433,\n",
       " 30: 6368,\n",
       " 8: 15015}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = {}\n",
    "for y in Y_train:\n",
    "    try:\n",
    "        tbl[y] += 1\n",
    "    except:\n",
    "        tbl[y] = 1\n",
    "\n",
    "tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section on testing the prediction the whole history of the first test aircraft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_aircraft_data = X_test[0:test_ac_index[0]]\n",
    "test_class_predictions = get_predicted_class(sess, cnn, X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment failed for 2 reasons:\n",
    "\n",
    "- if current location of aircraft is coded as -1, then fraction of windows for which the convolution picks the controlled aircraft is quite small\n",
    "- convolution uses shared weights between 2 layers. These weights are independent of the location of the controlled aircraft"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
