{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "import sys\n",
    "sys.path.append('../gym-flight/')\n",
    "import gym_flight\n",
    "from gym_flight.utils.geo import destination\n",
    "from util import *\n",
    "import multiprocessing as mp\n",
    "from full3d_util import get_range_df, bind, unlist\n",
    "splits = mp.cpu_count() - 1\n",
    "from aircraft_center_3d_util import train_test_split, fix_XY\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_x_length = 5\n",
    "half_y_length = 7\n",
    "half_z_length = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"processed_flight_data.bin\"):\n",
    "    flight_data = pickle.load(open(\"processed_flight_data.bin\", \"rb\"))\n",
    "else:\n",
    "    if os.path.isfile(\"raw_flight_data.bin\"):\n",
    "        flight_data = pickle.load(open(\"raw_flight_data.bin\", \"rb\"))\n",
    "    else:\n",
    "        flight_data_path = \"../../JFKSamples.csv\"\n",
    "        dtype_dict = {\"id\": str, \"raw_ts\": np.int64, \"ts\": np.int64, \"lat\": np.float32, \"lon\": np.float32, \"altitude\": np.float32, \"speed\": np.float32, \"x\": np.int16, \"y\": np.int16, \"z\": np.int16, \"is_landing\": np.int8}\n",
    "        flight_data = pd.read_csv(flight_data_path, dtype = dtype_dict)\n",
    "        flight_data = flight_data.sort_values(by = ['id', 'ts']).reset_index(drop = True)\n",
    "        flight_data = flight_data.drop_duplicates(subset = ['id', 'ts'], keep = 'first')\n",
    "        flight_data = flight_data.sort_values(by = ['id', 'ts']).reset_index(drop = True)\n",
    "        temp_ts = (flight_data['ts']/1000).apply(math.floor)\n",
    "        temp_ts = temp_ts - temp_ts.apply(lambda x: x % 60)\n",
    "        flight_data['round_min_ts'] = pd.to_datetime(temp_ts, unit = 's')\n",
    "        flight_data = flight_data.drop_duplicates(subset = ['id', 'ts'], keep = 'first')\n",
    "        flight_data['raw_ts'] = pd.to_datetime(flight_data['ts']/1000, unit = 's')\n",
    "        flight_data['prev_ts'] = flight_data[['id', 'raw_ts']].set_index(['id']).groupby(level=\"id\").shift(1).reset_index(drop = True)\n",
    "        flight_data['prev_time_diff_s'] = flight_data['raw_ts'] - flight_data['prev_ts']\n",
    "        flight_data['prev_time_diff_s'] = flight_data['prev_time_diff_s'].apply(lambda x: x.delta if type(x) != pd._libs.tslibs.nattype.NaTType else 0)/1000000000\n",
    "        flight_data = get_new_id_flight_data(flight_data)\n",
    "        flight_data['time'] = flight_data['raw_ts'].apply(lambda x: x.time())\n",
    "        flight_data['date'] = flight_data['raw_ts'].apply(lambda x: x.date())\n",
    "        flight_data['time_diff'] = flight_data['raw_ts'] - flight_data['round_min_ts']\n",
    "        flight_data['time_diff_s'] = flight_data['time_diff'].apply(lambda x: x.delta)/1000000000\n",
    "        flight_data['ground_distance'] = flight_data['time_diff_s'] * flight_data['ground_speed'] * 4.63/9\n",
    "        # Extrapolating (backwards in time) using the first known speed, heading at a given minute\n",
    "        flight_data['min_start_lat_lon'] = flight_data.apply(lambda row: destination(row['lat'], row['lon'], row['azimuth'], -row['ground_distance']), axis = 1)\n",
    "        flight_data['min_start_lat'] = flight_data['min_start_lat_lon'].apply(lambda x: x[0])\n",
    "        flight_data['min_start_lon'] = flight_data['min_start_lat_lon'].apply(lambda x: x[1])\n",
    "        pickle.dump(flight_data, open(\"raw_flight_data.bin\", \"wb\"))\n",
    "    \n",
    "    flight_data = flight_data.sort_values(by = ['id', 'round_min_ts']).reset_index(drop = False)\n",
    "    flight_data = flight_data.drop_duplicates(subset = ['id', 'round_min_ts'], keep = 'first')\n",
    "    flight_data = flight_data.reset_index(drop = True)\n",
    "    flight_data['next_round_min_ts'] = flight_data[['id', 'round_min_ts']].set_index(['id']).groupby(level=\"id\").shift(-1).reset_index(drop = True)\n",
    "    flight_data1 = get_timestamps_df(flight_data)\n",
    "    flight_data1.columns = ['round_min_ts', 'id']\n",
    "    flight_data = pd.merge(flight_data, flight_data1, how = 'outer')\n",
    "    flight_data = flight_data.sort_values(by = ['id', 'round_min_ts']).reset_index(drop = True)\n",
    "    flight_data = flight_data.drop(['index'], axis = 1)\n",
    "    flight_data = flight_data[['id', 'round_min_ts', 'ground_speed', 'altitude', 'azimuth', 'min_start_lat', 'min_start_lon']]\n",
    "    flight_data.columns = ['id', 'ts', 'ground_speed', 'altitude', 'azimuth', 'lat', 'lon']\n",
    "    speeds = flight_data['ground_speed'].tolist()\n",
    "    alts = flight_data['altitude'].tolist()\n",
    "    azis = flight_data['azimuth'].tolist()\n",
    "    lats = flight_data['lat'].tolist()\n",
    "    lons = flight_data['lon'].tolist()\n",
    "\n",
    "    for i in range(len(speeds)):\n",
    "        if np.isnan(speeds[i]):\n",
    "            speeds[i] = speeds[i-1]\n",
    "            alts[i] = alts[i-1]\n",
    "            azis[i] = azis[i-1]\n",
    "            dist = speeds[i] * 60 * 4.63 / 9\n",
    "            lat_lon = destination(lats[i-1], lons[i-1], azis[i-1], dist)\n",
    "            lats[i] = lat_lon[0]\n",
    "            lons[i] = lat_lon[1]\n",
    "    \n",
    "    flight_data['ground_speed'] = speeds\n",
    "    flight_data['altitude'] = alts\n",
    "    flight_data['azimuth'] = azis\n",
    "    flight_data['lon'] = lons\n",
    "    flight_data['lat'] = lats\n",
    "    shifted = flight_data.drop('ts', axis = 1).set_index('id').groupby(level = 'id').shift(-1).reset_index(drop = True)\n",
    "    shifted.columns = ['next_' + col for col in shifted.columns]\n",
    "    flight_data = pd.concat([flight_data, shifted], axis = 1)\n",
    "    flight_data['d_speed'] = flight_data['next_ground_speed'] - flight_data['ground_speed']\n",
    "    flight_data['d_altitude'] = flight_data['next_altitude'] - flight_data['altitude']\n",
    "    flight_data['d_azimuth'] = flight_data['next_azimuth'] - flight_data['azimuth']\n",
    "    pickle.dump(flight_data, open(\"processed_flight_data.bin\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data = remove_aircraft_with_anomalous_data(flight_data, speed_at_ground_altitude = True, estimated_speed = False, ground_speed = True, altitude = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_data['altitude'] = flight_data['altitude'].clip(lower = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352985, 18)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important\n",
    "Do not remove anomalous aircraft. Position estimate can be changed to weighted average of last and next known positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-81.6034924593011\n",
      "-64.07259139124781\n",
      "33.42332498145621\n",
      "46.41334811983426\n"
     ]
    }
   ],
   "source": [
    "lon_min = flight_data['lon'].min()\n",
    "lon_max = flight_data['lon'].max()\n",
    "lat_min = flight_data['lat'].min()\n",
    "lat_max = flight_data['lat'].max()\n",
    "print(lon_min)\n",
    "print(lon_max)\n",
    "print(lat_min)\n",
    "print(lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-78.0\n",
      "-68.0\n",
      "35.0\n",
      "45.0\n"
     ]
    }
   ],
   "source": [
    "flight_data['lat'] = flight_data['lat'].clip(lower = 35)\n",
    "flight_data['lat'] = flight_data['lat'].clip(upper = 45)\n",
    "flight_data['lon'] = flight_data['lon'].clip(lower = -78)\n",
    "flight_data['lon'] = flight_data['lon'].clip(upper = -68)\n",
    "lon_min = flight_data['lon'].min()\n",
    "lon_max = flight_data['lon'].max()\n",
    "lat_min = flight_data['lat'].min()\n",
    "lat_max = flight_data['lat'].max()\n",
    "print(lon_min)\n",
    "print(lon_max)\n",
    "print(lat_min)\n",
    "print(lat_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_length = 100\n",
    "y_length = 100\n",
    "z_length = 100\n",
    "alt_bucket_range = 500 # One bucket every 500 ft\n",
    "\n",
    "# Matching the schema with the previous preprocessed data - for reusability\n",
    "flight_data['x'] = (x_length * (flight_data['lat'] - lat_min - 0.0001)/(lat_max - lat_min)).apply(math.ceil)\n",
    "flight_data['y'] = (y_length * (flight_data['lon'] - lon_min - 0.0001)/(lon_max - lon_min)).apply(math.ceil)\n",
    "flight_data['z'] = (flight_data['altitude']/alt_bucket_range).apply(math.floor)\n",
    "flight_data['x'] = flight_data['x'].clip(upper = x_length - 1)\n",
    "flight_data['y'] = flight_data['y'].clip(upper = y_length - 1)\n",
    "flight_data['z'] = flight_data['z'].clip(upper = z_length - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretizing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ((flight_data['ts'] - flight_data['ts'].min()).apply(lambda x: x.delta/1000000000)/60).apply(math.floor)\n",
    "flight_data['ts'] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making each aircraft as center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_mp(uniq_ts):\n",
    "        return get_X_Y(flight_data, uniq_ts)\n",
    "\n",
    "\n",
    "def get_X_Y(flight_data, uniq_ts, include_controlled_ac = True):\n",
    "    X = {}\n",
    "    Y = {}\n",
    "    for k, ts in enumerate(uniq_ts):\n",
    "        ts_df = flight_data[flight_data['ts'] == ts]\n",
    "        if ts_df.shape[0] > 0:\n",
    "            for i in range(ts_df.shape[0]):\n",
    "                controlled_ac_row = ts_df.iloc[i]\n",
    "                controlled_ac_df = np.zeros(tuple(state_dim) + (2, ))\n",
    "                aircraft_x = controlled_ac_row['x']\n",
    "                aircraft_y = controlled_ac_row['y']\n",
    "                aircraft_z = controlled_ac_row['z']\n",
    "                lower_x = aircraft_x - half_x_length\n",
    "                upper_x = aircraft_x + half_x_length\n",
    "                lower_y = aircraft_y - half_y_length\n",
    "                upper_y = aircraft_y + half_y_length\n",
    "                lower_z = aircraft_z - half_z_length\n",
    "                upper_z = aircraft_z + half_z_length\n",
    "                for j in range(ts_df.shape[0]):\n",
    "                    if j != i:\n",
    "                        other_ac_x = ts_df['x'].iloc[j]\n",
    "                        other_ac_y = ts_df['y'].iloc[j]\n",
    "                        other_ac_z = ts_df['z'].iloc[j]\n",
    "                        if other_ac_x >= lower_x and other_ac_x <= upper_x and other_ac_y >= lower_y and other_ac_y <= upper_y and other_ac_z >= lower_z and other_ac_z <= upper_z:\n",
    "                            controlled_ac_df[other_ac_x - lower_x, other_ac_y - lower_y, other_ac_z - lower_z, :] = [ts_df['ground_speed'].iloc[j], ts_df['azimuth'].iloc[j]]\n",
    "\n",
    "                if include_controlled_ac:\n",
    "                    controlled_ac_df[half_x_length, half_y_length, half_z_length, :] = [controlled_ac_row['ground_speed'], controlled_ac_row['azimuth']]\n",
    "\n",
    "                X[(ts, controlled_ac_row['id'])] = controlled_ac_df\n",
    "                Y[(ts, controlled_ac_row['id'])] = np.array([controlled_ac_row['next_ground_speed'] - controlled_ac_row['ground_speed'], controlled_ac_row['next_altitude'] - controlled_ac_row['altitude'], controlled_ac_row['next_azimuth'] - controlled_ac_row['azimuth']])\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isfile(\"Y_train_aircraft_center_space_continuous_action_new_preprocessing.npy\"):\n",
    "    X_train = np.load(\"X_train_aircraft_center_space_multiclass_action_new_preprocessing.npy\")\n",
    "    Y_train = np.load(\"Y_train_aircraft_center_space_continuous_action_new_preprocessing.npy\")\n",
    "    X_test = np.load(\"X_test_aircraft_center_space_multiclass_action_new_preprocessing.npy\")\n",
    "    Y_test = np.load(\"Y_test_aircraft_center_space_continuous_action_new_preprocessing.npy\")\n",
    "    y_min = np.load(\"y_min.npy\")\n",
    "    y_max = np.load(\"y_max.npy\")\n",
    "else:\n",
    "    uniq_ts = flight_data['ts'].unique()\n",
    "    uniq_ts.sort()\n",
    "    state_dim = [2 * half_x_length + 1, 2 * half_y_length + 1, 2 * half_z_length + 1]\n",
    "    p = mp.Pool(processes = splits)\n",
    "    \n",
    "    # Arranging in reverse order to speed up computation\n",
    "    rng = list(uniq_ts)\n",
    "    # Multiprocessing the generation of examples from raw data\n",
    "    split_rng = np.array_split(rng, splits)\n",
    "    pool_results = p.map(get_X_Y_mp, split_rng)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    XY = np.concatenate(pool_results, axis = 0)\n",
    "    \n",
    "    # Arranging X and Y\n",
    "    X = [XY[i] for i in range(len(XY)) if i % 2 == 0]\n",
    "    Y = [XY[i] for i in range(len(XY)) if i % 2 == 1]\n",
    "    X = [np.array([item.reshape(8250) for item in list(x.values())]) for x in X]\n",
    "    Y = [np.array(list(y.values())) for y in Y]\n",
    "    X = np.concatenate(X, axis = 0)\n",
    "    Y = np.concatenate(Y, axis = 0)\n",
    "    X_train, Y_train, X_test, Y_test = train_test_split(X, Y)\n",
    "    \n",
    "    # Removing rows with nan values\n",
    "    Y_train_nan = np.isnan(Y_train[:,0])\n",
    "    Y_train = Y_train[~Y_train_nan, :]\n",
    "    X_train = X_train[~Y_train_nan, :]\n",
    "    Y_test_nan = np.isnan(Y_test[:,0])\n",
    "    Y_test = Y_test[~Y_test_nan, :]\n",
    "    X_test = X_test[~Y_test_nan, :]\n",
    "    \n",
    "    # Normalizing output\n",
    "    y_train_min = np.min(Y_train, axis = 0)\n",
    "    y_train_max = np.max(Y_train, axis = 0)\n",
    "    y_test_min = np.min(Y_test, axis = 0)\n",
    "    y_test_max = np.max(Y_test, axis = 0)\n",
    "\n",
    "    y_min = np.array([min(y_train_min[i], y_test_min[i]) for i in range(3)])\n",
    "    y_max = np.array([max(y_train_max[i], y_test_max[i]) for i in range(3)])\n",
    "\n",
    "    Y_train = (Y_train - y_min)/(y_max - y_min)\n",
    "    Y_test = (Y_test - y_min)/(y_max - y_min)\n",
    "    \n",
    "    # Saving\n",
    "    np.save(\"X_train_aircraft_center_space_multiclass_action_new_preprocessing\", X_train)\n",
    "    np.save(\"Y_train_aircraft_center_space_continuous_action_new_preprocessing\", Y_train)\n",
    "    np.save(\"X_test_aircraft_center_space_multiclass_action_new_preprocessing\", X_test)\n",
    "    np.save(\"Y_test_aircraft_center_space_continuous_action_new_preprocessing\", Y_test)\n",
    "    np.save(\"y_min\", y_min)\n",
    "    np.save(\"y_max\", y_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_continuous:\n",
    "    def __init__(self, state_dim, learning_rate):\n",
    "        self.target_ = tf.placeholder(tf.float32, [None, 3], name='target')\n",
    "        self.state_ = tf.placeholder(tf.float32, [None, state_dim], name='state')\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # fc1: (, 1024)\n",
    "        self.fc1 = tf.layers.dense(self.state_, 1024, activation=tf.nn.sigmoid, name='fc1')\n",
    "        \n",
    "        # Fc2 (b, 128) - may be bottleneck layer because action size is larger\n",
    "        self.fc2 = tf.layers.dense(self.fc1, 128, activation=tf.nn.sigmoid, name='fc2')\n",
    "        \n",
    "        # Split into 3 linear output units for:\n",
    "        # - change in speed\n",
    "        # - change in altitude\n",
    "        # - change in heading\n",
    "        # This should probably be changed to separate dense 'heads' that connect to individual outputs\n",
    "        # If we maintain different dense heads, if one output is noisy, then the weights of that dense head will be random (with the hope that weights of other heads are not affected)\n",
    "        self.fc3_d_speed = tf.layers.dense(self.fc2, 32, activation=tf.nn.sigmoid, name='fc3_d_speed')\n",
    "        self.fc3_d_altitude = tf.layers.dense(self.fc2, 32, activation=tf.nn.sigmoid, name='fc3_d_altitude')\n",
    "        self.fc3_d_azimuth = tf.layers.dense(self.fc2, 32, activation=tf.nn.sigmoid, name='fc3_d_azimuth')\n",
    "        self.output_d_speed = tf.layers.dense(self.fc3_d_speed, 1, name = 'output_d_speed')\n",
    "        self.output_d_altitude = tf.layers.dense(self.fc3_d_altitude, 1, name = 'output_d_altitude')\n",
    "        self.output_d_azimuth = tf.layers.dense(self.fc3_d_azimuth, 1, name = 'output_d_azimuth')\n",
    "        self.output = tf.concat([self.output_d_speed, self.output_d_altitude, self.output_d_azimuth], 1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.reduce_sum(tf.square(self.target_ - self.output), axis = 1))\n",
    "        \n",
    "        # Optimizer: Adam\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        \n",
    "        # Train Op\n",
    "        self.train = self.optimizer.minimize(self.loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function for training the network\n",
    "def train(sess, mlp, state, action):\n",
    "    feed_dict = {mlp.state_: state, mlp.target_: action}\n",
    "    loss, _ = sess.run([mlp.loss, mlp.train], feed_dict=feed_dict)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def get_predicted_action(sess, mlp, state):\n",
    "    output = sess.run(mlp.output, feed_dict={mlp.state_: state})\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "def get_loss(sess, mlp, state, action):\n",
    "    output = sess.run(mlp.loss, feed_dict={mlp.state_: state, mlp.target_: action})\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-6ab9a3161270>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "state_dim = 2 * (2 * half_x_length + 1) * (2 * half_y_length + 1) * (2 * half_z_length + 1)\n",
    "learning_rate = 0.025\n",
    "mlp = MLP_continuous(state_dim, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "perc = 0\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "num_batches = len(X_train)//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % completed\n",
      "Epoch: 1/1:  mini-batch 1/1896:  Train loss: 1.498777151107788  Test loss: 17.26111602783203 \n",
      "Epoch: 1/1:  mini-batch 2/1896:  Train loss: 17.24936866760254  Test loss: 6.103260517120361 \n",
      "Epoch: 1/1:  mini-batch 3/1896:  Train loss: 6.091188430786133  Test loss: 2.5224039554595947 \n",
      "Epoch: 1/1:  mini-batch 4/1896:  Train loss: 2.536949396133423  Test loss: 3.3160345554351807 \n",
      "Epoch: 1/1:  mini-batch 5/1896:  Train loss: 3.3259871006011963  Test loss: 2.957406520843506 \n",
      "Epoch: 1/1:  mini-batch 6/1896:  Train loss: 2.950155258178711  Test loss: 1.8146696090698242 \n",
      "Epoch: 1/1:  mini-batch 7/1896:  Train loss: 1.8184826374053955  Test loss: 1.0785149335861206 \n",
      "Epoch: 1/1:  mini-batch 8/1896:  Train loss: 1.082886815071106  Test loss: 0.801626443862915 \n",
      "Epoch: 1/1:  mini-batch 9/1896:  Train loss: 0.803265392780304  Test loss: 0.6569745540618896 \n",
      "Epoch: 1/1:  mini-batch 10/1896:  Train loss: 0.651308000087738  Test loss: 0.5008301138877869 \n",
      "Epoch: 1/1:  mini-batch 11/1896:  Train loss: 0.5000676512718201  Test loss: 0.3415175676345825 \n",
      "Epoch: 1/1:  mini-batch 12/1896:  Train loss: 0.33950209617614746  Test loss: 0.22507396340370178 \n",
      "Epoch: 1/1:  mini-batch 13/1896:  Train loss: 0.2183598279953003  Test loss: 0.17320439219474792 \n",
      "Epoch: 1/1:  mini-batch 14/1896:  Train loss: 0.17090797424316406  Test loss: 0.17553189396858215 \n",
      "Epoch: 1/1:  mini-batch 15/1896:  Train loss: 0.16998964548110962  Test loss: 0.20520317554473877 \n",
      "Epoch: 1/1:  mini-batch 16/1896:  Train loss: 0.20158690214157104  Test loss: 0.23422974348068237 \n",
      "Epoch: 1/1:  mini-batch 17/1896:  Train loss: 0.2297263741493225  Test loss: 0.24497680366039276 \n",
      "Epoch: 1/1:  mini-batch 18/1896:  Train loss: 0.24187354743480682  Test loss: 0.23128540813922882 \n",
      "Epoch: 1/1:  mini-batch 19/1896:  Train loss: 0.22647540271282196  Test loss: 0.19798681139945984 \n",
      "Epoch: 1/1:  mini-batch 20/1896:  Train loss: 0.19677940011024475  Test loss: 0.15491965413093567 \n",
      "Epoch: 1/1:  mini-batch 21/1896:  Train loss: 0.15286701917648315  Test loss: 0.1121104508638382 \n",
      "Epoch: 1/1:  mini-batch 22/1896:  Train loss: 0.10610859096050262  Test loss: 0.0766700729727745 \n",
      "Epoch: 1/1:  mini-batch 23/1896:  Train loss: 0.07355158776044846  Test loss: 0.051958389580249786 \n",
      "Epoch: 1/1:  mini-batch 24/1896:  Train loss: 0.05084788426756859  Test loss: 0.03867005556821823 \n",
      "Epoch: 1/1:  mini-batch 25/1896:  Train loss: 0.03769196569919586  Test loss: 0.03547259047627449 \n",
      "Epoch: 1/1:  mini-batch 26/1896:  Train loss: 0.032185688614845276  Test loss: 0.03966075927019119 \n",
      "Epoch: 1/1:  mini-batch 27/1896:  Train loss: 0.03891065716743469  Test loss: 0.04776393249630928 \n",
      "Epoch: 1/1:  mini-batch 28/1896:  Train loss: 0.04523848742246628  Test loss: 0.056383099406957626 \n",
      "Epoch: 1/1:  mini-batch 29/1896:  Train loss: 0.05421365052461624  Test loss: 0.06271873414516449 \n",
      "Epoch: 1/1:  mini-batch 30/1896:  Train loss: 0.06025948375463486  Test loss: 0.0650189071893692 \n",
      "Epoch: 1/1:  mini-batch 31/1896:  Train loss: 0.06115458905696869  Test loss: 0.062423981726169586 \n",
      "Epoch: 1/1:  mini-batch 32/1896:  Train loss: 0.05877178907394409  Test loss: 0.05536536127328873 \n",
      "Epoch: 1/1:  mini-batch 33/1896:  Train loss: 0.05135169252753258  Test loss: 0.045292504131793976 \n",
      "Epoch: 1/1:  mini-batch 34/1896:  Train loss: 0.04464314505457878  Test loss: 0.03404401242733002 \n",
      "Epoch: 1/1:  mini-batch 35/1896:  Train loss: 0.02994772233068943  Test loss: 0.023659128695726395 \n",
      "Epoch: 1/1:  mini-batch 36/1896:  Train loss: 0.019957169890403748  Test loss: 0.015811534598469734 \n",
      "Epoch: 1/1:  mini-batch 37/1896:  Train loss: 0.011807220987975597  Test loss: 0.011420311406254768 \n",
      "Epoch: 1/1:  mini-batch 38/1896:  Train loss: 0.007540800143033266  Test loss: 0.010564040392637253 \n",
      "Epoch: 1/1:  mini-batch 39/1896:  Train loss: 0.008695682510733604  Test loss: 0.012618648819625378 \n",
      "Epoch: 1/1:  mini-batch 40/1896:  Train loss: 0.01185577642172575  Test loss: 0.01627192273736 \n",
      "Epoch: 1/1:  mini-batch 41/1896:  Train loss: 0.012685173191130161  Test loss: 0.02010919153690338 \n",
      "Epoch: 1/1:  mini-batch 42/1896:  Train loss: 0.015930775552988052  Test loss: 0.022846000269055367 \n",
      "Epoch: 1/1:  mini-batch 43/1896:  Train loss: 0.022150296717882156  Test loss: 0.023666327819228172 \n",
      "Epoch: 1/1:  mini-batch 44/1896:  Train loss: 0.021418705582618713  Test loss: 0.022289367392659187 \n",
      "Epoch: 1/1:  mini-batch 45/1896:  Train loss: 0.024046553298830986  Test loss: 0.01911253109574318 \n",
      "Epoch: 1/1:  mini-batch 46/1896:  Train loss: 0.015617908909916878  Test loss: 0.01499021053314209 \n",
      "Epoch: 1/1:  mini-batch 47/1896:  Train loss: 0.013272232376039028  Test loss: 0.01087074726819992 \n",
      "Epoch: 1/1:  mini-batch 48/1896:  Train loss: 0.00861124973744154  Test loss: 0.0076151276007294655 \n",
      "Epoch: 1/1:  mini-batch 49/1896:  Train loss: 0.005708424374461174  Test loss: 0.005757666192948818 \n",
      "Epoch: 1/1:  mini-batch 50/1896:  Train loss: 0.0017782571958377957  Test loss: 0.005430888384580612 \n",
      "Epoch: 1/1:  mini-batch 51/1896:  Train loss: 0.006091808434575796  Test loss: 0.006332630757242441 \n",
      "Epoch: 1/1:  mini-batch 52/1896:  Train loss: 0.00742102786898613  Test loss: 0.00788754690438509 \n",
      "Epoch: 1/1:  mini-batch 53/1896:  Train loss: 0.005487412214279175  Test loss: 0.009443772956728935 \n",
      "Epoch: 1/1:  mini-batch 54/1896:  Train loss: 0.005991918034851551  Test loss: 0.010456073097884655 \n",
      "Epoch: 1/1:  mini-batch 55/1896:  Train loss: 0.00692300871014595  Test loss: 0.010640406981110573 \n",
      "Epoch: 1/1:  mini-batch 56/1896:  Train loss: 0.008636808022856712  Test loss: 0.010018550790846348 \n",
      "Epoch: 1/1:  mini-batch 57/1896:  Train loss: 0.008410734124481678  Test loss: 0.00882289744913578 \n",
      "Epoch: 1/1:  mini-batch 58/1896:  Train loss: 0.0047278087586164474  Test loss: 0.00737940426915884 \n",
      "Epoch: 1/1:  mini-batch 59/1896:  Train loss: 0.005335933528840542  Test loss: 0.006088082678616047 \n",
      "Epoch: 1/1:  mini-batch 60/1896:  Train loss: 0.006951966322958469  Test loss: 0.005211156792938709 \n",
      "Epoch: 1/1:  mini-batch 61/1896:  Train loss: 0.002802498871460557  Test loss: 0.004885597620159388 \n",
      "Epoch: 1/1:  mini-batch 62/1896:  Train loss: 0.004586785566061735  Test loss: 0.005045841448009014 \n",
      "Epoch: 1/1:  mini-batch 63/1896:  Train loss: 0.004501583985984325  Test loss: 0.005514455959200859 \n",
      "Epoch: 1/1:  mini-batch 64/1896:  Train loss: 0.005948031321167946  Test loss: 0.0060783326625823975 \n",
      "Epoch: 1/1:  mini-batch 65/1896:  Train loss: 0.008302757516503334  Test loss: 0.006534312851727009 \n",
      "Epoch: 1/1:  mini-batch 66/1896:  Train loss: 0.002717862371355295  Test loss: 0.006723692640662193 \n",
      "Epoch: 1/1:  mini-batch 67/1896:  Train loss: 0.004859509412199259  Test loss: 0.00661186408251524 \n",
      "Epoch: 1/1:  mini-batch 68/1896:  Train loss: 0.005128873512148857  Test loss: 0.006264182738959789 \n",
      "Epoch: 1/1:  mini-batch 69/1896:  Train loss: 0.005846403539180756  Test loss: 0.005786397494375706 \n",
      "Epoch: 1/1:  mini-batch 70/1896:  Train loss: 0.004171461798250675  Test loss: 0.005354113876819611 \n",
      "Epoch: 1/1:  mini-batch 71/1896:  Train loss: 0.001565437880344689  Test loss: 0.005074827931821346 \n",
      "Epoch: 1/1:  mini-batch 72/1896:  Train loss: 0.007410109508782625  Test loss: 0.004975521005690098 \n",
      "Epoch: 1/1:  mini-batch 73/1896:  Train loss: 0.0007554180338047445  Test loss: 0.00503423810005188 \n",
      "Epoch: 1/1:  mini-batch 74/1896:  Train loss: 0.002579914638772607  Test loss: 0.005172205623239279 \n",
      "Epoch: 1/1:  mini-batch 75/1896:  Train loss: 0.005228028632700443  Test loss: 0.005320128984749317 \n",
      "Epoch: 1/1:  mini-batch 76/1896:  Train loss: 0.002129148691892624  Test loss: 0.005419247318059206 \n",
      "Epoch: 1/1:  mini-batch 77/1896:  Train loss: 0.004570617340505123  Test loss: 0.005417490843683481 \n",
      "Epoch: 1/1:  mini-batch 78/1896:  Train loss: 0.005827785469591618  Test loss: 0.005305719096213579 \n",
      "Epoch: 1/1:  mini-batch 79/1896:  Train loss: 0.003371856175363064  Test loss: 0.005127694457769394 \n",
      "Epoch: 1/1:  mini-batch 80/1896:  Train loss: 0.006422819569706917  Test loss: 0.004951466340571642 \n",
      "Epoch: 1/1:  mini-batch 81/1896:  Train loss: 0.003510192036628723  Test loss: 0.00482089351862669 \n",
      "Epoch: 1/1:  mini-batch 82/1896:  Train loss: 0.0026166599709540606  Test loss: 0.004770657047629356 \n",
      "Epoch: 1/1:  mini-batch 83/1896:  Train loss: 0.0003634992172010243  Test loss: 0.004798398353159428 \n",
      "Epoch: 1/1:  mini-batch 84/1896:  Train loss: 0.004206063225865364  Test loss: 0.00486388523131609 \n",
      "Epoch: 1/1:  mini-batch 85/1896:  Train loss: 0.0023572195786982775  Test loss: 0.004929511807858944 \n",
      "Epoch: 1/1:  mini-batch 86/1896:  Train loss: 0.0031403936445713043  Test loss: 0.004972665570676327 \n",
      "Epoch: 1/1:  mini-batch 87/1896:  Train loss: 0.003983660601079464  Test loss: 0.0049844225868582726 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 88/1896:  Train loss: 0.004781574010848999  Test loss: 0.0049590147100389 \n",
      "Epoch: 1/1:  mini-batch 89/1896:  Train loss: 0.0040688044391572475  Test loss: 0.004922373685985804 \n",
      "Epoch: 1/1:  mini-batch 90/1896:  Train loss: 0.0038349712267518044  Test loss: 0.0048955525271594524 \n",
      "Epoch: 1/1:  mini-batch 91/1896:  Train loss: 0.0005355254979804158  Test loss: 0.004877286497503519 \n",
      "Epoch: 1/1:  mini-batch 92/1896:  Train loss: 0.005137941334396601  Test loss: 0.004862913861870766 \n",
      "Epoch: 1/1:  mini-batch 93/1896:  Train loss: 0.0024041980504989624  Test loss: 0.004858310334384441 \n",
      "Epoch: 1/1:  mini-batch 94/1896:  Train loss: 0.0020963959395885468  Test loss: 0.004856602288782597 \n",
      "Epoch: 1/1:  mini-batch 95/1896:  Train loss: 0.0021240797359496355  Test loss: 0.004853284917771816 \n",
      "Epoch: 1/1:  mini-batch 96/1896:  Train loss: 0.0007244420703500509  Test loss: 0.004843240603804588 \n",
      "Epoch: 1/1:  mini-batch 97/1896:  Train loss: 0.0024232130963355303  Test loss: 0.0048284390941262245 \n",
      "Epoch: 1/1:  mini-batch 98/1896:  Train loss: 0.006891561206430197  Test loss: 0.004818268585950136 \n",
      "Epoch: 1/1:  mini-batch 99/1896:  Train loss: 0.0013041733764111996  Test loss: 0.004810007754713297 \n",
      "Epoch: 1/1:  mini-batch 100/1896:  Train loss: 0.002517193555831909  Test loss: 0.004800706170499325 \n",
      "Epoch: 1/1:  mini-batch 101/1896:  Train loss: 0.0039185043424367905  Test loss: 0.004791751503944397 \n",
      "Epoch: 1/1:  mini-batch 102/1896:  Train loss: 0.0018501138547435403  Test loss: 0.004785936791449785 \n",
      "Epoch: 1/1:  mini-batch 103/1896:  Train loss: 0.00418703630566597  Test loss: 0.004776668734848499 \n",
      "Epoch: 1/1:  mini-batch 104/1896:  Train loss: 0.0006696767522953451  Test loss: 0.004770961590111256 \n",
      "Epoch: 1/1:  mini-batch 105/1896:  Train loss: 0.0041153524070978165  Test loss: 0.004770796746015549 \n",
      "Epoch: 1/1:  mini-batch 106/1896:  Train loss: 0.0016312195220962167  Test loss: 0.004780204966664314 \n",
      "Epoch: 1/1:  mini-batch 107/1896:  Train loss: 0.0055711595341563225  Test loss: 0.004802636336535215 \n",
      "Epoch: 1/1:  mini-batch 108/1896:  Train loss: 0.0016464018262922764  Test loss: 0.004827135242521763 \n",
      "Epoch: 1/1:  mini-batch 109/1896:  Train loss: 0.0010322430171072483  Test loss: 0.00484924390912056 \n",
      "Epoch: 1/1:  mini-batch 110/1896:  Train loss: 0.0021778217051178217  Test loss: 0.004862535744905472 \n",
      "Epoch: 1/1:  mini-batch 111/1896:  Train loss: 0.00221415888518095  Test loss: 0.00487098004668951 \n",
      "Epoch: 1/1:  mini-batch 112/1896:  Train loss: 0.0015315077034756541  Test loss: 0.004863354843109846 \n",
      "Epoch: 1/1:  mini-batch 113/1896:  Train loss: 0.0026082475669682026  Test loss: 0.004843720234930515 \n",
      "Epoch: 1/1:  mini-batch 114/1896:  Train loss: 0.003317613620311022  Test loss: 0.004820074886083603 \n",
      "Epoch: 1/1:  mini-batch 115/1896:  Train loss: 0.0032144251745194197  Test loss: 0.004792722873389721 \n",
      "Epoch: 1/1:  mini-batch 116/1896:  Train loss: 0.003842741483822465  Test loss: 0.004771510139107704 \n",
      "Epoch: 1/1:  mini-batch 117/1896:  Train loss: 0.001024587545543909  Test loss: 0.0047658029943704605 \n",
      "Epoch: 1/1:  mini-batch 118/1896:  Train loss: 0.004555664025247097  Test loss: 0.004769166465848684 \n",
      "Epoch: 1/1:  mini-batch 119/1896:  Train loss: 0.004592915531247854  Test loss: 0.00477078091353178 \n",
      "Epoch: 1/1:  mini-batch 120/1896:  Train loss: 0.0013632766203954816  Test loss: 0.00476796505972743 \n",
      "Epoch: 1/1:  mini-batch 121/1896:  Train loss: 0.008053463883697987  Test loss: 0.004768339917063713 \n",
      "Epoch: 1/1:  mini-batch 122/1896:  Train loss: 0.0041617159731686115  Test loss: 0.004767741076648235 \n",
      "Epoch: 1/1:  mini-batch 123/1896:  Train loss: 0.003794028889387846  Test loss: 0.004765267483890057 \n",
      "Epoch: 1/1:  mini-batch 124/1896:  Train loss: 0.00379391061142087  Test loss: 0.004764962010085583 \n",
      "Epoch: 1/1:  mini-batch 125/1896:  Train loss: 0.0009922727476805449  Test loss: 0.004766692407429218 \n",
      "Epoch: 1/1:  mini-batch 126/1896:  Train loss: 0.0018483992898836732  Test loss: 0.004769602324813604 \n",
      "Epoch: 1/1:  mini-batch 127/1896:  Train loss: 0.003150360658764839  Test loss: 0.004773151129484177 \n",
      "Epoch: 1/1:  mini-batch 128/1896:  Train loss: 0.0015492616221308708  Test loss: 0.0047789765521883965 \n",
      "Epoch: 1/1:  mini-batch 129/1896:  Train loss: 0.006495009642094374  Test loss: 0.004779034294188023 \n",
      "Epoch: 1/1:  mini-batch 130/1896:  Train loss: 0.0032201858703047037  Test loss: 0.004773090593516827 \n",
      "Epoch: 1/1:  mini-batch 131/1896:  Train loss: 0.0023281516041606665  Test loss: 0.00476630125194788 \n",
      "Epoch: 1/1:  mini-batch 132/1896:  Train loss: 0.004891619551926851  Test loss: 0.0047651613131165504 \n",
      "Epoch: 1/1:  mini-batch 133/1896:  Train loss: 0.0008652711403556168  Test loss: 0.004763729404658079 \n",
      "Epoch: 1/1:  mini-batch 134/1896:  Train loss: 0.0033274434972554445  Test loss: 0.004764462821185589 \n",
      "Epoch: 1/1:  mini-batch 135/1896:  Train loss: 0.00324062816798687  Test loss: 0.0047647105529904366 \n",
      "Epoch: 1/1:  mini-batch 136/1896:  Train loss: 0.005055580288171768  Test loss: 0.004763227887451649 \n",
      "Epoch: 1/1:  mini-batch 137/1896:  Train loss: 0.006385656539350748  Test loss: 0.004764328245073557 \n",
      "Epoch: 1/1:  mini-batch 138/1896:  Train loss: 0.0050307027995586395  Test loss: 0.004765895660966635 \n",
      "Epoch: 1/1:  mini-batch 139/1896:  Train loss: 0.0021134221460670233  Test loss: 0.004769747145473957 \n",
      "Epoch: 1/1:  mini-batch 140/1896:  Train loss: 0.0036694426089525223  Test loss: 0.004779387265443802 \n",
      "Epoch: 1/1:  mini-batch 141/1896:  Train loss: 0.00042013972415588796  Test loss: 0.004788233432918787 \n",
      "Epoch: 1/1:  mini-batch 142/1896:  Train loss: 0.0034134341403841972  Test loss: 0.004789996892213821 \n",
      "Epoch: 1/1:  mini-batch 143/1896:  Train loss: 0.001187945599667728  Test loss: 0.004787726327776909 \n",
      "Epoch: 1/1:  mini-batch 144/1896:  Train loss: 0.0006834398955106735  Test loss: 0.00478306133300066 \n",
      "Epoch: 1/1:  mini-batch 145/1896:  Train loss: 0.004922944121062756  Test loss: 0.004774612374603748 \n",
      "Epoch: 1/1:  mini-batch 146/1896:  Train loss: 0.0023641143925487995  Test loss: 0.0047718314453959465 \n",
      "Epoch: 1/1:  mini-batch 147/1896:  Train loss: 0.0023468974977731705  Test loss: 0.004770911298692226 \n",
      "Epoch: 1/1:  mini-batch 148/1896:  Train loss: 0.001831081579439342  Test loss: 0.004769666586071253 \n",
      "Epoch: 1/1:  mini-batch 149/1896:  Train loss: 0.0013780128210783005  Test loss: 0.0047673932276666164 \n",
      "Epoch: 1/1:  mini-batch 150/1896:  Train loss: 0.004085024353116751  Test loss: 0.004767649807035923 \n",
      "Epoch: 1/1:  mini-batch 151/1896:  Train loss: 0.004592780955135822  Test loss: 0.004776667803525925 \n",
      "Epoch: 1/1:  mini-batch 152/1896:  Train loss: 0.006270904093980789  Test loss: 0.00478363037109375 \n",
      "Epoch: 1/1:  mini-batch 153/1896:  Train loss: 0.002273445250466466  Test loss: 0.004787152633070946 \n",
      "Epoch: 1/1:  mini-batch 154/1896:  Train loss: 0.0036050421185791492  Test loss: 0.004788745194673538 \n",
      "Epoch: 1/1:  mini-batch 155/1896:  Train loss: 0.005456810817122459  Test loss: 0.004780794493854046 \n",
      "Epoch: 1/1:  mini-batch 156/1896:  Train loss: 0.006604189984500408  Test loss: 0.004770981147885323 \n",
      "Epoch: 1/1:  mini-batch 157/1896:  Train loss: 0.005902749951928854  Test loss: 0.00477449269965291 \n",
      "Epoch: 1/1:  mini-batch 158/1896:  Train loss: 0.004391442518681288  Test loss: 0.004783401265740395 \n",
      "Epoch: 1/1:  mini-batch 159/1896:  Train loss: 0.0021948269568383694  Test loss: 0.00478767417371273 \n",
      "Epoch: 1/1:  mini-batch 160/1896:  Train loss: 0.0042274221777915955  Test loss: 0.004784585442394018 \n",
      "Epoch: 1/1:  mini-batch 161/1896:  Train loss: 0.0026090042665600777  Test loss: 0.0047788782976567745 \n",
      "Epoch: 1/1:  mini-batch 162/1896:  Train loss: 0.0038124779239296913  Test loss: 0.004771938547492027 \n",
      "Epoch: 1/1:  mini-batch 163/1896:  Train loss: 0.004124554339796305  Test loss: 0.004766867496073246 \n",
      "Epoch: 1/1:  mini-batch 164/1896:  Train loss: 0.005314844194799662  Test loss: 0.004764658398926258 \n",
      "Epoch: 1/1:  mini-batch 165/1896:  Train loss: 0.006821170449256897  Test loss: 0.004762163385748863 \n",
      "Epoch: 1/1:  mini-batch 166/1896:  Train loss: 0.0018060797592625022  Test loss: 0.004764292389154434 \n",
      "Epoch: 1/1:  mini-batch 167/1896:  Train loss: 0.002121338155120611  Test loss: 0.004772915504872799 \n",
      "Epoch: 1/1:  mini-batch 168/1896:  Train loss: 0.008798862807452679  Test loss: 0.004774455912411213 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 169/1896:  Train loss: 0.005212303716689348  Test loss: 0.004776829853653908 \n",
      "Epoch: 1/1:  mini-batch 170/1896:  Train loss: 0.003751009237021208  Test loss: 0.0047722505405545235 \n",
      "Epoch: 1/1:  mini-batch 171/1896:  Train loss: 0.006293397396802902  Test loss: 0.004769080318510532 \n",
      "Epoch: 1/1:  mini-batch 172/1896:  Train loss: 0.0016735261306166649  Test loss: 0.004771741572767496 \n",
      "Epoch: 1/1:  mini-batch 173/1896:  Train loss: 0.0030675134621560574  Test loss: 0.0047778235748410225 \n",
      "Epoch: 1/1:  mini-batch 174/1896:  Train loss: 0.003145731519907713  Test loss: 0.0047758580185472965 \n",
      "Epoch: 1/1:  mini-batch 175/1896:  Train loss: 0.003999690525233746  Test loss: 0.004768643528223038 \n",
      "Epoch: 1/1:  mini-batch 176/1896:  Train loss: 0.0006447620107792318  Test loss: 0.004764653742313385 \n",
      "Epoch: 1/1:  mini-batch 177/1896:  Train loss: 0.0046361396089196205  Test loss: 0.004761896561831236 \n",
      "Epoch: 1/1:  mini-batch 178/1896:  Train loss: 0.0030003783758729696  Test loss: 0.00476197712123394 \n",
      "Epoch: 1/1:  mini-batch 179/1896:  Train loss: 0.00303416745737195  Test loss: 0.004762203898280859 \n",
      "Epoch: 1/1:  mini-batch 180/1896:  Train loss: 0.0036073133815079927  Test loss: 0.0047628749161958694 \n",
      "Epoch: 1/1:  mini-batch 181/1896:  Train loss: 0.003310388419777155  Test loss: 0.004762809257954359 \n",
      "Epoch: 1/1:  mini-batch 182/1896:  Train loss: 0.0018182178027927876  Test loss: 0.004766201600432396 \n",
      "Epoch: 1/1:  mini-batch 183/1896:  Train loss: 0.001970902783796191  Test loss: 0.0047720689326524734 \n",
      "Epoch: 1/1:  mini-batch 184/1896:  Train loss: 0.001514982432126999  Test loss: 0.004780886694788933 \n",
      "Epoch: 1/1:  mini-batch 185/1896:  Train loss: 0.0026179745327681303  Test loss: 0.004785667173564434 \n",
      "Epoch: 1/1:  mini-batch 186/1896:  Train loss: 0.004118910990655422  Test loss: 0.004790714476257563 \n",
      "Epoch: 1/1:  mini-batch 187/1896:  Train loss: 0.005149492993950844  Test loss: 0.004792273510247469 \n",
      "Epoch: 1/1:  mini-batch 188/1896:  Train loss: 0.002427092520520091  Test loss: 0.004791917745023966 \n",
      "Epoch: 1/1:  mini-batch 189/1896:  Train loss: 0.0026924696285277605  Test loss: 0.004784942604601383 \n",
      "Epoch: 1/1:  mini-batch 190/1896:  Train loss: 0.0011704973876476288  Test loss: 0.004778849892318249 \n",
      "Epoch: 1/1:  mini-batch 191/1896:  Train loss: 0.005812321789562702  Test loss: 0.004775422625243664 \n",
      "Epoch: 1/1:  mini-batch 192/1896:  Train loss: 0.002315070480108261  Test loss: 0.004771560896188021 \n",
      "Epoch: 1/1:  mini-batch 193/1896:  Train loss: 0.0029164489824324846  Test loss: 0.004767159000039101 \n",
      "Epoch: 1/1:  mini-batch 194/1896:  Train loss: 0.0070259468629956245  Test loss: 0.00476608332246542 \n",
      "Epoch: 1/1:  mini-batch 195/1896:  Train loss: 0.0045700217597186565  Test loss: 0.004762930795550346 \n",
      "Epoch: 1/1:  mini-batch 196/1896:  Train loss: 0.003115276340395212  Test loss: 0.004762161523103714 \n",
      "Epoch: 1/1:  mini-batch 197/1896:  Train loss: 0.004048584960401058  Test loss: 0.004763122648000717 \n",
      "Epoch: 1/1:  mini-batch 198/1896:  Train loss: 0.00434392224997282  Test loss: 0.004765875171869993 \n",
      "Epoch: 1/1:  mini-batch 199/1896:  Train loss: 0.0023846083786338568  Test loss: 0.004767449107021093 \n",
      "Epoch: 1/1:  mini-batch 200/1896:  Train loss: 0.0036035426892340183  Test loss: 0.004766927100718021 \n",
      "Epoch: 1/1:  mini-batch 201/1896:  Train loss: 0.002268930897116661  Test loss: 0.004767668433487415 \n",
      "Epoch: 1/1:  mini-batch 202/1896:  Train loss: 0.001816769945435226  Test loss: 0.004770693369209766 \n",
      "Epoch: 1/1:  mini-batch 203/1896:  Train loss: 0.0006289828452281654  Test loss: 0.004774001426994801 \n",
      "Epoch: 1/1:  mini-batch 204/1896:  Train loss: 0.005751938559114933  Test loss: 0.004769655875861645 \n",
      "Epoch: 1/1:  mini-batch 205/1896:  Train loss: 0.0005446106661111116  Test loss: 0.00476748775690794 \n",
      "Epoch: 1/1:  mini-batch 206/1896:  Train loss: 0.0026895913761109114  Test loss: 0.004767272155731916 \n",
      "Epoch: 1/1:  mini-batch 207/1896:  Train loss: 0.0010514562018215656  Test loss: 0.004766670987010002 \n",
      "Epoch: 1/1:  mini-batch 208/1896:  Train loss: 0.0005451911129057407  Test loss: 0.00476792361587286 \n",
      "Epoch: 1/1:  mini-batch 209/1896:  Train loss: 0.003432666417211294  Test loss: 0.004772708285599947 \n",
      "Epoch: 1/1:  mini-batch 210/1896:  Train loss: 0.004373833537101746  Test loss: 0.004767635837197304 \n",
      "Epoch: 1/1:  mini-batch 211/1896:  Train loss: 0.005486822687089443  Test loss: 0.00476545374840498 \n",
      "Epoch: 1/1:  mini-batch 212/1896:  Train loss: 0.004970908164978027  Test loss: 0.004767803940922022 \n",
      "Epoch: 1/1:  mini-batch 213/1896:  Train loss: 0.005886457394808531  Test loss: 0.004771051928400993 \n",
      "Epoch: 1/1:  mini-batch 214/1896:  Train loss: 0.008501391857862473  Test loss: 0.004773655906319618 \n",
      "Epoch: 1/1:  mini-batch 215/1896:  Train loss: 0.0017208204371854663  Test loss: 0.004773193970322609 \n",
      "Epoch: 1/1:  mini-batch 216/1896:  Train loss: 0.003528646659106016  Test loss: 0.004769554361701012 \n",
      "Epoch: 1/1:  mini-batch 217/1896:  Train loss: 0.00268531683832407  Test loss: 0.004765625577419996 \n",
      "Epoch: 1/1:  mini-batch 218/1896:  Train loss: 0.0021422849968075752  Test loss: 0.00476484838873148 \n",
      "Epoch: 1/1:  mini-batch 219/1896:  Train loss: 0.003625208046287298  Test loss: 0.004767953883856535 \n",
      "Epoch: 1/1:  mini-batch 220/1896:  Train loss: 0.004734600894153118  Test loss: 0.0047753434628248215 \n",
      "Epoch: 1/1:  mini-batch 221/1896:  Train loss: 0.001008597551845014  Test loss: 0.004789021797478199 \n",
      "Epoch: 1/1:  mini-batch 222/1896:  Train loss: 0.0024275328032672405  Test loss: 0.004803410731256008 \n",
      "Epoch: 1/1:  mini-batch 223/1896:  Train loss: 0.007691548205912113  Test loss: 0.004803486168384552 \n",
      "Epoch: 1/1:  mini-batch 224/1896:  Train loss: 0.0023525666911154985  Test loss: 0.004803801886737347 \n",
      "Epoch: 1/1:  mini-batch 225/1896:  Train loss: 0.003919568378478289  Test loss: 0.004801634233444929 \n",
      "Epoch: 1/1:  mini-batch 226/1896:  Train loss: 0.0034611618611961603  Test loss: 0.0047850655391812325 \n",
      "Epoch: 1/1:  mini-batch 227/1896:  Train loss: 0.004777435213327408  Test loss: 0.004775729961693287 \n",
      "Epoch: 1/1:  mini-batch 228/1896:  Train loss: 0.004247851669788361  Test loss: 0.0047681815922260284 \n",
      "Epoch: 1/1:  mini-batch 229/1896:  Train loss: 0.008706241846084595  Test loss: 0.004770603496581316 \n",
      "Epoch: 1/1:  mini-batch 230/1896:  Train loss: 0.003508392022922635  Test loss: 0.0047776903957128525 \n",
      "Epoch: 1/1:  mini-batch 231/1896:  Train loss: 0.00038070770096965134  Test loss: 0.0047823176719248295 \n",
      "Epoch: 1/1:  mini-batch 232/1896:  Train loss: 0.00210779532790184  Test loss: 0.004778735339641571 \n",
      "Epoch: 1/1:  mini-batch 233/1896:  Train loss: 0.003465446410700679  Test loss: 0.0047683073207736015 \n",
      "Epoch: 1/1:  mini-batch 234/1896:  Train loss: 0.004052635282278061  Test loss: 0.004765263758599758 \n",
      "Epoch: 1/1:  mini-batch 235/1896:  Train loss: 0.004110333509743214  Test loss: 0.004766006954014301 \n",
      "Epoch: 1/1:  mini-batch 236/1896:  Train loss: 0.0026023315731436014  Test loss: 0.004773844964802265 \n",
      "Epoch: 1/1:  mini-batch 237/1896:  Train loss: 0.0030825778376311064  Test loss: 0.004792291671037674 \n",
      "Epoch: 1/1:  mini-batch 238/1896:  Train loss: 0.004814412444829941  Test loss: 0.004813264589756727 \n",
      "Epoch: 1/1:  mini-batch 239/1896:  Train loss: 0.007371789310127497  Test loss: 0.00481790117919445 \n",
      "Epoch: 1/1:  mini-batch 240/1896:  Train loss: 0.0028335233218967915  Test loss: 0.004806138575077057 \n",
      "Epoch: 1/1:  mini-batch 241/1896:  Train loss: 0.005098286084830761  Test loss: 0.0047780731692910194 \n",
      "Epoch: 1/1:  mini-batch 242/1896:  Train loss: 0.003664339892566204  Test loss: 0.004765911027789116 \n",
      "Epoch: 1/1:  mini-batch 243/1896:  Train loss: 0.0075599392876029015  Test loss: 0.004763664212077856 \n",
      "Epoch: 1/1:  mini-batch 244/1896:  Train loss: 0.004575318191200495  Test loss: 0.004763820208609104 \n",
      "Epoch: 1/1:  mini-batch 245/1896:  Train loss: 0.001798804383724928  Test loss: 0.004764394834637642 \n",
      "Epoch: 1/1:  mini-batch 246/1896:  Train loss: 0.00320938928052783  Test loss: 0.004765060730278492 \n",
      "Epoch: 1/1:  mini-batch 247/1896:  Train loss: 0.005248415283858776  Test loss: 0.00476575642824173 \n",
      "Epoch: 1/1:  mini-batch 248/1896:  Train loss: 0.0034553413279354572  Test loss: 0.004764767363667488 \n",
      "Epoch: 1/1:  mini-batch 249/1896:  Train loss: 0.008242478594183922  Test loss: 0.004767104517668486 \n",
      "Epoch: 1/1:  mini-batch 250/1896:  Train loss: 0.003032189793884754  Test loss: 0.004774151835590601 \n",
      "Epoch: 1/1:  mini-batch 251/1896:  Train loss: 0.004561593756079674  Test loss: 0.004774353001266718 \n",
      "Epoch: 1/1:  mini-batch 252/1896:  Train loss: 0.004020715598016977  Test loss: 0.004770107567310333 \n",
      "Epoch: 1/1:  mini-batch 253/1896:  Train loss: 0.0028046858496963978  Test loss: 0.004770083352923393 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 254/1896:  Train loss: 0.0047527882270514965  Test loss: 0.004773398861289024 \n",
      "Epoch: 1/1:  mini-batch 255/1896:  Train loss: 0.004548111464828253  Test loss: 0.004767788574099541 \n",
      "Epoch: 1/1:  mini-batch 256/1896:  Train loss: 0.0009892042726278305  Test loss: 0.004764219745993614 \n",
      "Epoch: 1/1:  mini-batch 257/1896:  Train loss: 0.0023975237272679806  Test loss: 0.004765274468809366 \n",
      "Epoch: 1/1:  mini-batch 258/1896:  Train loss: 0.0031180037185549736  Test loss: 0.004765615798532963 \n",
      "Epoch: 1/1:  mini-batch 259/1896:  Train loss: 0.0022506786044687033  Test loss: 0.004767714999616146 \n",
      "Epoch: 1/1:  mini-batch 260/1896:  Train loss: 0.004654823802411556  Test loss: 0.004772697575390339 \n",
      "Epoch: 1/1:  mini-batch 261/1896:  Train loss: 0.005107037723064423  Test loss: 0.004783780314028263 \n",
      "Epoch: 1/1:  mini-batch 262/1896:  Train loss: 0.005556602030992508  Test loss: 0.0047904751263558865 \n",
      "Epoch: 1/1:  mini-batch 263/1896:  Train loss: 0.007550537586212158  Test loss: 0.004790673963725567 \n",
      "Epoch: 1/1:  mini-batch 264/1896:  Train loss: 0.005523091182112694  Test loss: 0.004791226238012314 \n",
      "Epoch: 1/1:  mini-batch 265/1896:  Train loss: 0.0018486022017896175  Test loss: 0.004793528467416763 \n",
      "Epoch: 1/1:  mini-batch 266/1896:  Train loss: 0.0038477929774671793  Test loss: 0.0047943186946213245 \n",
      "Epoch: 1/1:  mini-batch 267/1896:  Train loss: 0.004655461758375168  Test loss: 0.00478233490139246 \n",
      "Epoch: 1/1:  mini-batch 268/1896:  Train loss: 0.0020536414813250303  Test loss: 0.0047755055129528046 \n",
      "Epoch: 1/1:  mini-batch 269/1896:  Train loss: 0.003890363499522209  Test loss: 0.004767527803778648 \n",
      "Epoch: 1/1:  mini-batch 270/1896:  Train loss: 0.0012557061854749918  Test loss: 0.004764087498188019 \n",
      "Epoch: 1/1:  mini-batch 271/1896:  Train loss: 0.005536637734621763  Test loss: 0.004763283766806126 \n",
      "Epoch: 1/1:  mini-batch 272/1896:  Train loss: 0.0035073731560260057  Test loss: 0.004763520322740078 \n",
      "Epoch: 1/1:  mini-batch 273/1896:  Train loss: 0.006062758155167103  Test loss: 0.004766181111335754 \n",
      "Epoch: 1/1:  mini-batch 274/1896:  Train loss: 0.003612989094108343  Test loss: 0.00477018766105175 \n",
      "Epoch: 1/1:  mini-batch 275/1896:  Train loss: 0.0007352321408689022  Test loss: 0.004771344363689423 \n",
      "Epoch: 1/1:  mini-batch 276/1896:  Train loss: 0.006048429757356644  Test loss: 0.004762335680425167 \n",
      "Epoch: 1/1:  mini-batch 277/1896:  Train loss: 0.007182839326560497  Test loss: 0.004764946177601814 \n",
      "Epoch: 1/1:  mini-batch 278/1896:  Train loss: 0.002450729487463832  Test loss: 0.004790858365595341 \n",
      "Epoch: 1/1:  mini-batch 279/1896:  Train loss: 0.0021854101214557886  Test loss: 0.004837514366954565 \n",
      "Epoch: 1/1:  mini-batch 280/1896:  Train loss: 0.0054487427696585655  Test loss: 0.004851080942898989 \n",
      "Epoch: 1/1:  mini-batch 281/1896:  Train loss: 0.0034077134914696217  Test loss: 0.004836586304008961 \n",
      "Epoch: 1/1:  mini-batch 282/1896:  Train loss: 0.0013195148203521967  Test loss: 0.004813392646610737 \n",
      "Epoch: 1/1:  mini-batch 283/1896:  Train loss: 0.0034483913332223892  Test loss: 0.00479096919298172 \n",
      "Epoch: 1/1:  mini-batch 284/1896:  Train loss: 0.00524489302188158  Test loss: 0.004775750916451216 \n",
      "Epoch: 1/1:  mini-batch 285/1896:  Train loss: 0.001917171524837613  Test loss: 0.004771830514073372 \n",
      "Epoch: 1/1:  mini-batch 286/1896:  Train loss: 0.003446013666689396  Test loss: 0.004771352279931307 \n",
      "Epoch: 1/1:  mini-batch 287/1896:  Train loss: 0.0020474593620747328  Test loss: 0.004775314591825008 \n",
      "Epoch: 1/1:  mini-batch 288/1896:  Train loss: 0.0008975947857834399  Test loss: 0.004776584915816784 \n",
      "Epoch: 1/1:  mini-batch 289/1896:  Train loss: 0.0059187444858253  Test loss: 0.004766359459608793 \n",
      "Epoch: 1/1:  mini-batch 290/1896:  Train loss: 0.0003790096379816532  Test loss: 0.0047662765718996525 \n",
      "Epoch: 1/1:  mini-batch 291/1896:  Train loss: 0.004479559138417244  Test loss: 0.0047749802470207214 \n",
      "Epoch: 1/1:  mini-batch 292/1896:  Train loss: 0.0014284384669736028  Test loss: 0.004790349863469601 \n",
      "Epoch: 1/1:  mini-batch 293/1896:  Train loss: 0.008127043023705482  Test loss: 0.00478520430624485 \n",
      "Epoch: 1/1:  mini-batch 294/1896:  Train loss: 0.00394268287345767  Test loss: 0.0047902436926960945 \n",
      "Epoch: 1/1:  mini-batch 295/1896:  Train loss: 0.005403456278145313  Test loss: 0.004795054905116558 \n",
      "Epoch: 1/1:  mini-batch 296/1896:  Train loss: 0.0040557109750807285  Test loss: 0.0047844089567661285 \n",
      "Epoch: 1/1:  mini-batch 297/1896:  Train loss: 0.006477358750998974  Test loss: 0.004774854518473148 \n",
      "Epoch: 1/1:  mini-batch 298/1896:  Train loss: 0.00419491995126009  Test loss: 0.004771324805915356 \n",
      "Epoch: 1/1:  mini-batch 299/1896:  Train loss: 0.001649550162255764  Test loss: 0.004771079868078232 \n",
      "Epoch: 1/1:  mini-batch 300/1896:  Train loss: 0.007601547054946423  Test loss: 0.004769104532897472 \n",
      "Epoch: 1/1:  mini-batch 301/1896:  Train loss: 0.006806866731494665  Test loss: 0.004768675658851862 \n",
      "Epoch: 1/1:  mini-batch 302/1896:  Train loss: 0.0017649162327870727  Test loss: 0.00477201584726572 \n",
      "Epoch: 1/1:  mini-batch 303/1896:  Train loss: 0.0025526827666908503  Test loss: 0.004781241528689861 \n",
      "Epoch: 1/1:  mini-batch 304/1896:  Train loss: 0.0006842211587354541  Test loss: 0.004794175736606121 \n",
      "Epoch: 1/1:  mini-batch 305/1896:  Train loss: 0.0020731238182634115  Test loss: 0.004806382115930319 \n",
      "Epoch: 1/1:  mini-batch 306/1896:  Train loss: 0.0013946234248578548  Test loss: 0.004806548357009888 \n",
      "Epoch: 1/1:  mini-batch 307/1896:  Train loss: 0.002498362213373184  Test loss: 0.00480637326836586 \n",
      "Epoch: 1/1:  mini-batch 308/1896:  Train loss: 0.004168600309640169  Test loss: 0.004797089844942093 \n",
      "Epoch: 1/1:  mini-batch 309/1896:  Train loss: 0.0006166341481730342  Test loss: 0.004782622680068016 \n",
      "Epoch: 1/1:  mini-batch 310/1896:  Train loss: 0.0018975723069161177  Test loss: 0.004769858438521624 \n",
      "Epoch: 1/1:  mini-batch 311/1896:  Train loss: 0.002391186309978366  Test loss: 0.0047643366269767284 \n",
      "Epoch: 1/1:  mini-batch 312/1896:  Train loss: 0.0006648186827078462  Test loss: 0.004766486585140228 \n",
      "Epoch: 1/1:  mini-batch 313/1896:  Train loss: 0.000741206924431026  Test loss: 0.004771268926560879 \n",
      "Epoch: 1/1:  mini-batch 314/1896:  Train loss: 0.0016282422002404928  Test loss: 0.004769900348037481 \n",
      "Epoch: 1/1:  mini-batch 315/1896:  Train loss: 0.002016756683588028  Test loss: 0.004768584854900837 \n",
      "Epoch: 1/1:  mini-batch 316/1896:  Train loss: 0.00311851454898715  Test loss: 0.004765376448631287 \n",
      "Epoch: 1/1:  mini-batch 317/1896:  Train loss: 0.0009748521260917187  Test loss: 0.004764729645103216 \n",
      "Epoch: 1/1:  mini-batch 318/1896:  Train loss: 0.00830894522368908  Test loss: 0.004766396712511778 \n",
      "Epoch: 1/1:  mini-batch 319/1896:  Train loss: 0.0023636810947209597  Test loss: 0.0047697345726192 \n",
      "Epoch: 1/1:  mini-batch 320/1896:  Train loss: 0.0009954595007002354  Test loss: 0.004772700369358063 \n",
      "Epoch: 1/1:  mini-batch 321/1896:  Train loss: 0.006152980960905552  Test loss: 0.004790248349308968 \n",
      "Epoch: 1/1:  mini-batch 322/1896:  Train loss: 0.0036568250507116318  Test loss: 0.004807531833648682 \n",
      "Epoch: 1/1:  mini-batch 323/1896:  Train loss: 0.004491155967116356  Test loss: 0.004812466446310282 \n",
      "Epoch: 1/1:  mini-batch 324/1896:  Train loss: 0.004977685399353504  Test loss: 0.004799455404281616 \n",
      "Epoch: 1/1:  mini-batch 325/1896:  Train loss: 0.001068920362740755  Test loss: 0.004789893515408039 \n",
      "Epoch: 1/1:  mini-batch 326/1896:  Train loss: 0.0037299799732863903  Test loss: 0.004772568587213755 \n",
      "Epoch: 1/1:  mini-batch 327/1896:  Train loss: 0.005477672442793846  Test loss: 0.00476701557636261 \n",
      "Epoch: 1/1:  mini-batch 328/1896:  Train loss: 0.004514188040047884  Test loss: 0.004768523387610912 \n",
      "Epoch: 1/1:  mini-batch 329/1896:  Train loss: 0.0025321077555418015  Test loss: 0.004772309213876724 \n",
      "Epoch: 1/1:  mini-batch 330/1896:  Train loss: 0.0027177322190254927  Test loss: 0.004769526422023773 \n",
      "Epoch: 1/1:  mini-batch 331/1896:  Train loss: 0.0031685081776231527  Test loss: 0.004770438186824322 \n",
      "Epoch: 1/1:  mini-batch 332/1896:  Train loss: 0.0011369745479896665  Test loss: 0.0047731781378388405 \n",
      "Epoch: 1/1:  mini-batch 333/1896:  Train loss: 0.00689252745360136  Test loss: 0.004786033183336258 \n",
      "Epoch: 1/1:  mini-batch 334/1896:  Train loss: 0.005995495244860649  Test loss: 0.004852007143199444 \n",
      "Epoch: 1/1:  mini-batch 335/1896:  Train loss: 8.463951235171407e-05  Test loss: 0.00490941945463419 \n",
      "Epoch: 1/1:  mini-batch 336/1896:  Train loss: 0.005472735036164522  Test loss: 0.00490534957498312 \n",
      "Epoch: 1/1:  mini-batch 337/1896:  Train loss: 0.004499777220189571  Test loss: 0.004834864754229784 \n",
      "Epoch: 1/1:  mini-batch 338/1896:  Train loss: 0.005436221603304148  Test loss: 0.0047824084758758545 \n",
      "Epoch: 1/1:  mini-batch 339/1896:  Train loss: 0.004954504780471325  Test loss: 0.004775957204401493 \n",
      "Epoch: 1/1:  mini-batch 340/1896:  Train loss: 0.0027340685483068228  Test loss: 0.004799495916813612 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 341/1896:  Train loss: 0.0037022982724010944  Test loss: 0.004824107047170401 \n",
      "Epoch: 1/1:  mini-batch 342/1896:  Train loss: 0.005205176770687103  Test loss: 0.004819326102733612 \n",
      "Epoch: 1/1:  mini-batch 343/1896:  Train loss: 0.0013534848112612963  Test loss: 0.004805846139788628 \n",
      "Epoch: 1/1:  mini-batch 344/1896:  Train loss: 0.0013701739953830838  Test loss: 0.004781587980687618 \n",
      "Epoch: 1/1:  mini-batch 345/1896:  Train loss: 0.0023226661141961813  Test loss: 0.004770251922309399 \n",
      "Epoch: 1/1:  mini-batch 346/1896:  Train loss: 0.0041910456493496895  Test loss: 0.004783593118190765 \n",
      "Epoch: 1/1:  mini-batch 347/1896:  Train loss: 0.0009160077897831798  Test loss: 0.004811451770365238 \n",
      "Epoch: 1/1:  mini-batch 348/1896:  Train loss: 0.002151933964341879  Test loss: 0.0048371171578764915 \n",
      "Epoch: 1/1:  mini-batch 349/1896:  Train loss: 0.0043931398540735245  Test loss: 0.0048393071629107 \n",
      "Epoch: 1/1:  mini-batch 350/1896:  Train loss: 0.004140316508710384  Test loss: 0.004824622999876738 \n",
      "Epoch: 1/1:  mini-batch 351/1896:  Train loss: 0.00036954833194613457  Test loss: 0.00479997368529439 \n",
      "Epoch: 1/1:  mini-batch 352/1896:  Train loss: 0.006241916213184595  Test loss: 0.004773213993757963 \n",
      "Epoch: 1/1:  mini-batch 353/1896:  Train loss: 0.0025634660851210356  Test loss: 0.0047647058963775635 \n",
      "Epoch: 1/1:  mini-batch 354/1896:  Train loss: 0.0009554412681609392  Test loss: 0.004763316363096237 \n",
      "Epoch: 1/1:  mini-batch 355/1896:  Train loss: 0.0016389285447075963  Test loss: 0.004766364581882954 \n",
      "Epoch: 1/1:  mini-batch 356/1896:  Train loss: 0.0026443451642990112  Test loss: 0.004764053504914045 \n",
      "Epoch: 1/1:  mini-batch 357/1896:  Train loss: 0.001073608174920082  Test loss: 0.004762636497616768 \n",
      "Epoch: 1/1:  mini-batch 358/1896:  Train loss: 0.003222249448299408  Test loss: 0.0047659799456596375 \n",
      "Epoch: 1/1:  mini-batch 359/1896:  Train loss: 0.005004184320569038  Test loss: 0.00477349478751421 \n",
      "Epoch: 1/1:  mini-batch 360/1896:  Train loss: 0.0033262786455452442  Test loss: 0.004779130220413208 \n",
      "Epoch: 1/1:  mini-batch 361/1896:  Train loss: 0.0012253394816070795  Test loss: 0.004780125804245472 \n",
      "Epoch: 1/1:  mini-batch 362/1896:  Train loss: 0.0060697332955896854  Test loss: 0.004781573079526424 \n",
      "Epoch: 1/1:  mini-batch 363/1896:  Train loss: 0.00411874009296298  Test loss: 0.004782393574714661 \n",
      "Epoch: 1/1:  mini-batch 364/1896:  Train loss: 0.0037998021580278873  Test loss: 0.0047776843421161175 \n",
      "Epoch: 1/1:  mini-batch 365/1896:  Train loss: 0.002502072835341096  Test loss: 0.004770682193338871 \n",
      "Epoch: 1/1:  mini-batch 366/1896:  Train loss: 0.005648230202496052  Test loss: 0.004766928963363171 \n",
      "Epoch: 1/1:  mini-batch 367/1896:  Train loss: 0.007926812395453453  Test loss: 0.004792623221874237 \n",
      "Epoch: 1/1:  mini-batch 368/1896:  Train loss: 0.0006770821055397391  Test loss: 0.004834775812923908 \n",
      "Epoch: 1/1:  mini-batch 369/1896:  Train loss: 0.002808438614010811  Test loss: 0.004874468315392733 \n",
      "Epoch: 1/1:  mini-batch 370/1896:  Train loss: 0.0021023163571953773  Test loss: 0.0049075172282755375 \n",
      "Epoch: 1/1:  mini-batch 371/1896:  Train loss: 0.0025910332333296537  Test loss: 0.0048934463411569595 \n",
      "Epoch: 1/1:  mini-batch 372/1896:  Train loss: 0.0034279152750968933  Test loss: 0.004857323132455349 \n",
      "Epoch: 1/1:  mini-batch 373/1896:  Train loss: 0.005761845037341118  Test loss: 0.0048051439225673676 \n",
      "Epoch: 1/1:  mini-batch 374/1896:  Train loss: 0.004107622429728508  Test loss: 0.0047809770330786705 \n",
      "Epoch: 1/1:  mini-batch 375/1896:  Train loss: 0.0031451501417905092  Test loss: 0.004776010289788246 \n",
      "Epoch: 1/1:  mini-batch 376/1896:  Train loss: 0.005827657878398895  Test loss: 0.004791893996298313 \n",
      "Epoch: 1/1:  mini-batch 377/1896:  Train loss: 0.0038591569755226374  Test loss: 0.0048008933663368225 \n",
      "Epoch: 1/1:  mini-batch 378/1896:  Train loss: 0.00292567303404212  Test loss: 0.00481523759663105 \n",
      "Epoch: 1/1:  mini-batch 379/1896:  Train loss: 0.004333347547799349  Test loss: 0.004794769920408726 \n",
      "Epoch: 1/1:  mini-batch 380/1896:  Train loss: 0.0029301312752068043  Test loss: 0.0047701033763587475 \n",
      "Epoch: 1/1:  mini-batch 381/1896:  Train loss: 0.003734802594408393  Test loss: 0.004772568587213755 \n",
      "Epoch: 1/1:  mini-batch 382/1896:  Train loss: 0.0026801303029060364  Test loss: 0.0048127006739377975 \n",
      "Epoch: 1/1:  mini-batch 383/1896:  Train loss: 0.005123271141201258  Test loss: 0.004844767041504383 \n",
      "Epoch: 1/1:  mini-batch 384/1896:  Train loss: 0.003832621732726693  Test loss: 0.004828190430998802 \n",
      "Epoch: 1/1:  mini-batch 385/1896:  Train loss: 0.005450164899230003  Test loss: 0.004808495752513409 \n",
      "Epoch: 1/1:  mini-batch 386/1896:  Train loss: 0.004151113331317902  Test loss: 0.00477976817637682 \n",
      "Epoch: 1/1:  mini-batch 387/1896:  Train loss: 0.004550579469650984  Test loss: 0.004781655035912991 \n",
      "Epoch: 1/1:  mini-batch 388/1896:  Train loss: 0.002309535862877965  Test loss: 0.004823061637580395 \n",
      "Epoch: 1/1:  mini-batch 389/1896:  Train loss: 0.006874216254800558  Test loss: 0.004870581440627575 \n",
      "Epoch: 1/1:  mini-batch 390/1896:  Train loss: 0.0014484260464087129  Test loss: 0.0048677148297429085 \n",
      "Epoch: 1/1:  mini-batch 391/1896:  Train loss: 0.004393022507429123  Test loss: 0.004823256283998489 \n",
      "Epoch: 1/1:  mini-batch 392/1896:  Train loss: 0.0031717969104647636  Test loss: 0.0047916024923324585 \n",
      "Epoch: 1/1:  mini-batch 393/1896:  Train loss: 0.004786740057170391  Test loss: 0.004772934131324291 \n",
      "Epoch: 1/1:  mini-batch 394/1896:  Train loss: 0.002771184779703617  Test loss: 0.004774983040988445 \n",
      "Epoch: 1/1:  mini-batch 395/1896:  Train loss: 0.0028387256897985935  Test loss: 0.00478840246796608 \n",
      "Epoch: 1/1:  mini-batch 396/1896:  Train loss: 0.004676150158047676  Test loss: 0.004805360455065966 \n",
      "Epoch: 1/1:  mini-batch 397/1896:  Train loss: 0.0011657108552753925  Test loss: 0.004817744717001915 \n",
      "Epoch: 1/1:  mini-batch 398/1896:  Train loss: 0.002804764546453953  Test loss: 0.004833146929740906 \n",
      "Epoch: 1/1:  mini-batch 399/1896:  Train loss: 0.0004704941820818931  Test loss: 0.004829530604183674 \n",
      "Epoch: 1/1:  mini-batch 400/1896:  Train loss: 0.005654746666550636  Test loss: 0.004807205405086279 \n",
      "Epoch: 1/1:  mini-batch 401/1896:  Train loss: 0.005205672234296799  Test loss: 0.004795422311872244 \n",
      "Epoch: 1/1:  mini-batch 402/1896:  Train loss: 0.0031771757639944553  Test loss: 0.004799205809831619 \n",
      "Epoch: 1/1:  mini-batch 403/1896:  Train loss: 0.006327623035758734  Test loss: 0.004802614450454712 \n",
      "Epoch: 1/1:  mini-batch 404/1896:  Train loss: 0.004422481637448072  Test loss: 0.004800797905772924 \n",
      "Epoch: 1/1:  mini-batch 405/1896:  Train loss: 0.0062050288543105125  Test loss: 0.004781518597155809 \n",
      "Epoch: 1/1:  mini-batch 406/1896:  Train loss: 0.0033692261204123497  Test loss: 0.004770392552018166 \n",
      "Epoch: 1/1:  mini-batch 407/1896:  Train loss: 0.005549274384975433  Test loss: 0.004765816032886505 \n",
      "Epoch: 1/1:  mini-batch 408/1896:  Train loss: 0.010014012455940247  Test loss: 0.004774587228894234 \n",
      "Epoch: 1/1:  mini-batch 409/1896:  Train loss: 0.007418567314743996  Test loss: 0.004777402617037296 \n",
      "Epoch: 1/1:  mini-batch 410/1896:  Train loss: 0.002561882371082902  Test loss: 0.004781033843755722 \n",
      "Epoch: 1/1:  mini-batch 411/1896:  Train loss: 0.004540511406958103  Test loss: 0.004772618412971497 \n",
      "Epoch: 1/1:  mini-batch 412/1896:  Train loss: 0.003892425447702408  Test loss: 0.0047745369374752045 \n",
      "Epoch: 1/1:  mini-batch 413/1896:  Train loss: 0.0016407333314418793  Test loss: 0.004777383990585804 \n",
      "Epoch: 1/1:  mini-batch 414/1896:  Train loss: 0.002735099522396922  Test loss: 0.004771272651851177 \n",
      "Epoch: 1/1:  mini-batch 415/1896:  Train loss: 0.0041948058642446995  Test loss: 0.0047728531062603 \n",
      "Epoch: 1/1:  mini-batch 416/1896:  Train loss: 0.0024188095703721046  Test loss: 0.0047733173705637455 \n",
      "Epoch: 1/1:  mini-batch 417/1896:  Train loss: 0.0004528707650024444  Test loss: 0.004780616611242294 \n",
      "Epoch: 1/1:  mini-batch 418/1896:  Train loss: 0.002127239480614662  Test loss: 0.004781848285347223 \n",
      "Epoch: 1/1:  mini-batch 419/1896:  Train loss: 0.0025970442220568657  Test loss: 0.004781201481819153 \n",
      "Epoch: 1/1:  mini-batch 420/1896:  Train loss: 0.007775880862027407  Test loss: 0.004783164709806442 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 421/1896:  Train loss: 0.0028446107171475887  Test loss: 0.004788356367498636 \n",
      "Epoch: 1/1:  mini-batch 422/1896:  Train loss: 0.002199955051764846  Test loss: 0.0048001972027122974 \n",
      "Epoch: 1/1:  mini-batch 423/1896:  Train loss: 0.003463122993707657  Test loss: 0.00482327351346612 \n",
      "Epoch: 1/1:  mini-batch 424/1896:  Train loss: 0.0036252751015126705  Test loss: 0.004838546272367239 \n",
      "Epoch: 1/1:  mini-batch 425/1896:  Train loss: 0.005850458983331919  Test loss: 0.004810815677046776 \n",
      "Epoch: 1/1:  mini-batch 426/1896:  Train loss: 0.003984293434768915  Test loss: 0.004789366386830807 \n",
      "Epoch: 1/1:  mini-batch 427/1896:  Train loss: 0.004460532683879137  Test loss: 0.004776250571012497 \n",
      "Epoch: 1/1:  mini-batch 428/1896:  Train loss: 0.007024700753390789  Test loss: 0.004768468905240297 \n",
      "Epoch: 1/1:  mini-batch 429/1896:  Train loss: 0.003591981017962098  Test loss: 0.004772624000906944 \n",
      "Epoch: 1/1:  mini-batch 430/1896:  Train loss: 0.0027534044347703457  Test loss: 0.004777879454195499 \n",
      "Epoch: 1/1:  mini-batch 431/1896:  Train loss: 0.006068059243261814  Test loss: 0.004776629153639078 \n",
      "Epoch: 1/1:  mini-batch 432/1896:  Train loss: 0.00049813580699265  Test loss: 0.004772927612066269 \n",
      "Epoch: 1/1:  mini-batch 433/1896:  Train loss: 0.01162471529096365  Test loss: 0.004771143198013306 \n",
      "Epoch: 1/1:  mini-batch 434/1896:  Train loss: 0.015806153416633606  Test loss: 0.004768011160194874 \n",
      "Epoch: 1/1:  mini-batch 435/1896:  Train loss: 0.008475979790091515  Test loss: 0.004767467267811298 \n",
      "Epoch: 1/1:  mini-batch 436/1896:  Train loss: 0.003210684284567833  Test loss: 0.004772989545017481 \n",
      "Epoch: 1/1:  mini-batch 437/1896:  Train loss: 0.007435224484652281  Test loss: 0.004778670147061348 \n",
      "Epoch: 1/1:  mini-batch 438/1896:  Train loss: 0.008866935037076473  Test loss: 0.004791790619492531 \n",
      "Epoch: 1/1:  mini-batch 439/1896:  Train loss: 0.004910253453999758  Test loss: 0.004786351695656776 \n",
      "Epoch: 1/1:  mini-batch 440/1896:  Train loss: 0.002135758986696601  Test loss: 0.0047777919098734856 \n",
      "Epoch: 1/1:  mini-batch 441/1896:  Train loss: 0.007580505684018135  Test loss: 0.0047722384333610535 \n",
      "Epoch: 1/1:  mini-batch 442/1896:  Train loss: 0.009341261349618435  Test loss: 0.004783823154866695 \n",
      "Epoch: 1/1:  mini-batch 443/1896:  Train loss: 0.006619702558964491  Test loss: 0.004789825528860092 \n",
      "Epoch: 1/1:  mini-batch 444/1896:  Train loss: 0.005578730721026659  Test loss: 0.0047849323600530624 \n",
      "Epoch: 1/1:  mini-batch 445/1896:  Train loss: 0.0019517571199685335  Test loss: 0.00477557722479105 \n",
      "Epoch: 1/1:  mini-batch 446/1896:  Train loss: 0.001408164738677442  Test loss: 0.004771186970174313 \n",
      "Epoch: 1/1:  mini-batch 447/1896:  Train loss: 0.003811245784163475  Test loss: 0.0047835661098361015 \n",
      "Epoch: 1/1:  mini-batch 448/1896:  Train loss: 0.002842063084244728  Test loss: 0.004800604656338692 \n",
      "Epoch: 1/1:  mini-batch 449/1896:  Train loss: 0.0020799064077436924  Test loss: 0.0048272255808115005 \n",
      "Epoch: 1/1:  mini-batch 450/1896:  Train loss: 0.00046846765326336026  Test loss: 0.004848049022257328 \n",
      "Epoch: 1/1:  mini-batch 451/1896:  Train loss: 0.0060510095208883286  Test loss: 0.004838088061660528 \n",
      "Epoch: 1/1:  mini-batch 452/1896:  Train loss: 0.003390899859368801  Test loss: 0.004821021109819412 \n",
      "Epoch: 1/1:  mini-batch 453/1896:  Train loss: 0.0008268626406788826  Test loss: 0.004795803688466549 \n",
      "Epoch: 1/1:  mini-batch 454/1896:  Train loss: 0.003762673819437623  Test loss: 0.0047749727964401245 \n",
      "Epoch: 1/1:  mini-batch 455/1896:  Train loss: 0.0003658669302240014  Test loss: 0.0047659315168857574 \n",
      "Epoch: 1/1:  mini-batch 456/1896:  Train loss: 0.0008435538038611412  Test loss: 0.004762507509440184 \n",
      "Epoch: 1/1:  mini-batch 457/1896:  Train loss: 0.0030876146629452705  Test loss: 0.00476453360170126 \n",
      "Epoch: 1/1:  mini-batch 458/1896:  Train loss: 0.0018219092162325978  Test loss: 0.004780581220984459 \n",
      "Epoch: 1/1:  mini-batch 459/1896:  Train loss: 0.0009829788468778133  Test loss: 0.004817489068955183 \n",
      "Epoch: 1/1:  mini-batch 460/1896:  Train loss: 0.0033995420671999454  Test loss: 0.004867650102823973 \n",
      "Epoch: 1/1:  mini-batch 461/1896:  Train loss: 0.0037269832100719213  Test loss: 0.004912707023322582 \n",
      "Epoch: 1/1:  mini-batch 462/1896:  Train loss: 0.004397086799144745  Test loss: 0.004920029081404209 \n",
      "Epoch: 1/1:  mini-batch 463/1896:  Train loss: 0.003407454350963235  Test loss: 0.004894299432635307 \n",
      "Epoch: 1/1:  mini-batch 464/1896:  Train loss: 0.0022705071605741978  Test loss: 0.004854271188378334 \n",
      "Epoch: 1/1:  mini-batch 465/1896:  Train loss: 0.0029372889548540115  Test loss: 0.0048133903183043 \n",
      "Epoch: 1/1:  mini-batch 466/1896:  Train loss: 0.005483192391693592  Test loss: 0.004784991964697838 \n",
      "Epoch: 1/1:  mini-batch 467/1896:  Train loss: 0.0021083843894302845  Test loss: 0.004769820719957352 \n",
      "Epoch: 1/1:  mini-batch 468/1896:  Train loss: 0.002596646547317505  Test loss: 0.004770094063133001 \n",
      "Epoch: 1/1:  mini-batch 469/1896:  Train loss: 0.00265214079990983  Test loss: 0.004769805818796158 \n",
      "Epoch: 1/1:  mini-batch 470/1896:  Train loss: 0.001746414229273796  Test loss: 0.004766359925270081 \n",
      "Epoch: 1/1:  mini-batch 471/1896:  Train loss: 0.002537022577598691  Test loss: 0.004765206016600132 \n",
      "Epoch: 1/1:  mini-batch 472/1896:  Train loss: 0.0040813046507537365  Test loss: 0.004763918928802013 \n",
      "Epoch: 1/1:  mini-batch 473/1896:  Train loss: 0.0032470952719449997  Test loss: 0.004773738794028759 \n",
      "Epoch: 1/1:  mini-batch 474/1896:  Train loss: 0.0054106684401631355  Test loss: 0.004796597175300121 \n",
      "Epoch: 1/1:  mini-batch 475/1896:  Train loss: 0.008804178796708584  Test loss: 0.00482734153047204 \n",
      "Epoch: 1/1:  mini-batch 476/1896:  Train loss: 0.0009541144245304167  Test loss: 0.004842151887714863 \n",
      "Epoch: 1/1:  mini-batch 477/1896:  Train loss: 0.003532935632392764  Test loss: 0.004835158586502075 \n",
      "Epoch: 1/1:  mini-batch 478/1896:  Train loss: 0.002041767816990614  Test loss: 0.0047968532890081406 \n",
      "Epoch: 1/1:  mini-batch 479/1896:  Train loss: 0.007749688345938921  Test loss: 0.0047639720141887665 \n",
      "Epoch: 1/1:  mini-batch 480/1896:  Train loss: 0.005628193262964487  Test loss: 0.004768419545143843 \n",
      "Epoch: 1/1:  mini-batch 481/1896:  Train loss: 0.0034217615611851215  Test loss: 0.004782426171004772 \n",
      "Epoch: 1/1:  mini-batch 482/1896:  Train loss: 0.00363426236435771  Test loss: 0.004783112555742264 \n",
      "Epoch: 1/1:  mini-batch 483/1896:  Train loss: 0.002218269743025303  Test loss: 0.004773024469614029 \n",
      "Epoch: 1/1:  mini-batch 484/1896:  Train loss: 0.0030540244188159704  Test loss: 0.004774156026542187 \n",
      "Epoch: 1/1:  mini-batch 485/1896:  Train loss: 0.004775465931743383  Test loss: 0.0047812010161578655 \n",
      "Epoch: 1/1:  mini-batch 486/1896:  Train loss: 0.0034180411603301764  Test loss: 0.0047979457303881645 \n",
      "Epoch: 1/1:  mini-batch 487/1896:  Train loss: 0.005254026036709547  Test loss: 0.004810498096048832 \n",
      "Epoch: 1/1:  mini-batch 488/1896:  Train loss: 0.0018853845540434122  Test loss: 0.004806678742170334 \n",
      "Epoch: 1/1:  mini-batch 489/1896:  Train loss: 0.0012870386708527803  Test loss: 0.0047930022701621056 \n",
      "Epoch: 1/1:  mini-batch 490/1896:  Train loss: 0.00409243069589138  Test loss: 0.004778803326189518 \n",
      "Epoch: 1/1:  mini-batch 491/1896:  Train loss: 0.0021976816933602095  Test loss: 0.00477389432489872 \n",
      "Epoch: 1/1:  mini-batch 492/1896:  Train loss: 0.007582374848425388  Test loss: 0.004770476371049881 \n",
      "Epoch: 1/1:  mini-batch 493/1896:  Train loss: 0.00261296471580863  Test loss: 0.004765779711306095 \n",
      "Epoch: 1/1:  mini-batch 494/1896:  Train loss: 0.0037076924927532673  Test loss: 0.004770583473145962 \n",
      "Epoch: 1/1:  mini-batch 495/1896:  Train loss: 0.002314936835318804  Test loss: 0.004772482439875603 \n",
      "Epoch: 1/1:  mini-batch 496/1896:  Train loss: 0.005629466846585274  Test loss: 0.004771408159285784 \n",
      "Epoch: 1/1:  mini-batch 497/1896:  Train loss: 0.004506715573370457  Test loss: 0.004764840006828308 \n",
      "Epoch: 1/1:  mini-batch 498/1896:  Train loss: 0.0067721763625741005  Test loss: 0.004776483401656151 \n",
      "Epoch: 1/1:  mini-batch 499/1896:  Train loss: 0.0031729191541671753  Test loss: 0.004792765248566866 \n",
      "Epoch: 1/1:  mini-batch 500/1896:  Train loss: 0.004058899823576212  Test loss: 0.004806941375136375 \n",
      "Epoch: 1/1:  mini-batch 501/1896:  Train loss: 0.008939436636865139  Test loss: 0.004812517203390598 \n",
      "Epoch: 1/1:  mini-batch 502/1896:  Train loss: 0.004346106201410294  Test loss: 0.004801273811608553 \n",
      "Epoch: 1/1:  mini-batch 503/1896:  Train loss: 0.004263495095074177  Test loss: 0.004809816833585501 \n",
      "Epoch: 1/1:  mini-batch 504/1896:  Train loss: 0.009755020029842854  Test loss: 0.004857524298131466 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 505/1896:  Train loss: 0.010458279401063919  Test loss: 0.004871438257396221 \n",
      "Epoch: 1/1:  mini-batch 506/1896:  Train loss: 0.008918767794966698  Test loss: 0.004847426898777485 \n",
      "Epoch: 1/1:  mini-batch 507/1896:  Train loss: 0.0042986380867660046  Test loss: 0.0048142001032829285 \n",
      "Epoch: 1/1:  mini-batch 508/1896:  Train loss: 0.007682637311518192  Test loss: 0.004787839017808437 \n",
      "Epoch: 1/1:  mini-batch 509/1896:  Train loss: 0.004266418982297182  Test loss: 0.004776583984494209 \n",
      "Epoch: 1/1:  mini-batch 510/1896:  Train loss: 0.0018440408166497946  Test loss: 0.004776450339704752 \n",
      "Epoch: 1/1:  mini-batch 511/1896:  Train loss: 0.0029617566615343094  Test loss: 0.004771880805492401 \n",
      "Epoch: 1/1:  mini-batch 512/1896:  Train loss: 0.007010023109614849  Test loss: 0.0047871749848127365 \n",
      "Epoch: 1/1:  mini-batch 513/1896:  Train loss: 0.004640220198780298  Test loss: 0.004808669909834862 \n",
      "Epoch: 1/1:  mini-batch 514/1896:  Train loss: 0.003693206701427698  Test loss: 0.004849193152040243 \n",
      "Epoch: 1/1:  mini-batch 515/1896:  Train loss: 0.005730423144996166  Test loss: 0.004841480404138565 \n",
      "Epoch: 1/1:  mini-batch 516/1896:  Train loss: 0.0027868172619491816  Test loss: 0.004819475580006838 \n",
      "Epoch: 1/1:  mini-batch 517/1896:  Train loss: 0.0015235921600833535  Test loss: 0.004800175316631794 \n",
      "Epoch: 1/1:  mini-batch 518/1896:  Train loss: 0.0033405085559934378  Test loss: 0.004795528948307037 \n",
      "Epoch: 1/1:  mini-batch 519/1896:  Train loss: 0.006421073339879513  Test loss: 0.004790555685758591 \n",
      "Epoch: 1/1:  mini-batch 520/1896:  Train loss: 0.005070535000413656  Test loss: 0.00478061567991972 \n",
      "Epoch: 1/1:  mini-batch 521/1896:  Train loss: 0.0031289616599678993  Test loss: 0.004773642867803574 \n",
      "Epoch: 1/1:  mini-batch 522/1896:  Train loss: 0.003194850869476795  Test loss: 0.004769535269588232 \n",
      "Epoch: 1/1:  mini-batch 523/1896:  Train loss: 0.006147022824734449  Test loss: 0.004775719717144966 \n",
      "Epoch: 1/1:  mini-batch 524/1896:  Train loss: 0.0013034900184720755  Test loss: 0.0047952644526958466 \n",
      "Epoch: 1/1:  mini-batch 525/1896:  Train loss: 0.0035099938977509737  Test loss: 0.004823412746191025 \n",
      "Epoch: 1/1:  mini-batch 526/1896:  Train loss: 0.0019235445652157068  Test loss: 0.004862172529101372 \n",
      "Epoch: 1/1:  mini-batch 527/1896:  Train loss: 0.002730380278080702  Test loss: 0.004856774117797613 \n",
      "Epoch: 1/1:  mini-batch 528/1896:  Train loss: 0.004095147363841534  Test loss: 0.004813837818801403 \n",
      "Epoch: 1/1:  mini-batch 529/1896:  Train loss: 0.002385564148426056  Test loss: 0.0047859856858849525 \n",
      "Epoch: 1/1:  mini-batch 530/1896:  Train loss: 0.002353151561692357  Test loss: 0.004765908233821392 \n",
      "Epoch: 1/1:  mini-batch 531/1896:  Train loss: 0.0010153843322768807  Test loss: 0.00476826960220933 \n",
      "Epoch: 1/1:  mini-batch 532/1896:  Train loss: 0.0045515066012740135  Test loss: 0.004775697831064463 \n",
      "Epoch: 1/1:  mini-batch 533/1896:  Train loss: 0.006193755194544792  Test loss: 0.004786285571753979 \n",
      "Epoch: 1/1:  mini-batch 534/1896:  Train loss: 0.004866261966526508  Test loss: 0.004787934012711048 \n",
      "Epoch: 1/1:  mini-batch 535/1896:  Train loss: 0.006490917876362801  Test loss: 0.004773897118866444 \n",
      "Epoch: 1/1:  mini-batch 536/1896:  Train loss: 0.005713829770684242  Test loss: 0.0047790128737688065 \n",
      "Epoch: 1/1:  mini-batch 537/1896:  Train loss: 0.0032017468474805355  Test loss: 0.0047767749056220055 \n",
      "Epoch: 1/1:  mini-batch 538/1896:  Train loss: 0.005192186683416367  Test loss: 0.004775220528244972 \n",
      "Epoch: 1/1:  mini-batch 539/1896:  Train loss: 0.0014834771864116192  Test loss: 0.004770570434629917 \n",
      "Epoch: 1/1:  mini-batch 540/1896:  Train loss: 0.004059394355863333  Test loss: 0.0047677988186478615 \n",
      "Epoch: 1/1:  mini-batch 541/1896:  Train loss: 0.00431729294359684  Test loss: 0.004765973426401615 \n",
      "Epoch: 1/1:  mini-batch 542/1896:  Train loss: 0.005586016457527876  Test loss: 0.004783879034221172 \n",
      "Epoch: 1/1:  mini-batch 543/1896:  Train loss: 0.003987310919910669  Test loss: 0.0048027923330664635 \n",
      "Epoch: 1/1:  mini-batch 544/1896:  Train loss: 0.006003206130117178  Test loss: 0.004832449369132519 \n",
      "Epoch: 1/1:  mini-batch 545/1896:  Train loss: 0.005544180981814861  Test loss: 0.004866890609264374 \n",
      "Epoch: 1/1:  mini-batch 546/1896:  Train loss: 0.0013820172753185034  Test loss: 0.004883946850895882 \n",
      "Epoch: 1/1:  mini-batch 547/1896:  Train loss: 0.004414966329932213  Test loss: 0.004838724620640278 \n",
      "Epoch: 1/1:  mini-batch 548/1896:  Train loss: 0.008436720818281174  Test loss: 0.004775681532919407 \n",
      "Epoch: 1/1:  mini-batch 549/1896:  Train loss: 0.004197159316390753  Test loss: 0.004770277999341488 \n",
      "Epoch: 1/1:  mini-batch 550/1896:  Train loss: 0.009303290396928787  Test loss: 0.004780986811965704 \n",
      "Epoch: 1/1:  mini-batch 551/1896:  Train loss: 0.002404998056590557  Test loss: 0.00478023337200284 \n",
      "Epoch: 1/1:  mini-batch 552/1896:  Train loss: 0.00640992633998394  Test loss: 0.004773267544806004 \n",
      "Epoch: 1/1:  mini-batch 553/1896:  Train loss: 0.0019808157812803984  Test loss: 0.004774494096636772 \n",
      "Epoch: 1/1:  mini-batch 554/1896:  Train loss: 0.004937297664582729  Test loss: 0.004780671093612909 \n",
      "Epoch: 1/1:  mini-batch 555/1896:  Train loss: 0.0007938630878925323  Test loss: 0.004794072359800339 \n",
      "Epoch: 1/1:  mini-batch 556/1896:  Train loss: 0.007356175687164068  Test loss: 0.004793916363269091 \n",
      "Epoch: 1/1:  mini-batch 557/1896:  Train loss: 0.004627663642168045  Test loss: 0.004773137159645557 \n",
      "Epoch: 1/1:  mini-batch 558/1896:  Train loss: 0.0038413824513554573  Test loss: 0.0047712428495287895 \n",
      "Epoch: 1/1:  mini-batch 559/1896:  Train loss: 0.0013792969984933734  Test loss: 0.004771946929395199 \n",
      "Epoch: 1/1:  mini-batch 560/1896:  Train loss: 0.003265401115640998  Test loss: 0.00477474695071578 \n",
      "Epoch: 1/1:  mini-batch 561/1896:  Train loss: 0.002052170457318425  Test loss: 0.004778051748871803 \n",
      "Epoch: 1/1:  mini-batch 562/1896:  Train loss: 0.003928260877728462  Test loss: 0.004782174713909626 \n",
      "Epoch: 1/1:  mini-batch 563/1896:  Train loss: 0.003391822800040245  Test loss: 0.004796086810529232 \n",
      "Epoch: 1/1:  mini-batch 564/1896:  Train loss: 0.0019914917647838593  Test loss: 0.004800606053322554 \n",
      "Epoch: 1/1:  mini-batch 565/1896:  Train loss: 0.0008430464076809585  Test loss: 0.004790318198502064 \n",
      "Epoch: 1/1:  mini-batch 566/1896:  Train loss: 0.0012093876721337438  Test loss: 0.00478061567991972 \n",
      "Epoch: 1/1:  mini-batch 567/1896:  Train loss: 0.0026719714514911175  Test loss: 0.004766012541949749 \n",
      "Epoch: 1/1:  mini-batch 568/1896:  Train loss: 0.0006828800542280078  Test loss: 0.004761574789881706 \n",
      "Epoch: 1/1:  mini-batch 569/1896:  Train loss: 0.0027729282155632973  Test loss: 0.004762037191540003 \n",
      "Epoch: 1/1:  mini-batch 570/1896:  Train loss: 0.0009705822449177504  Test loss: 0.0047650085762143135 \n",
      "Epoch: 1/1:  mini-batch 571/1896:  Train loss: 0.003273313632234931  Test loss: 0.00477677583694458 \n",
      "Epoch: 1/1:  mini-batch 572/1896:  Train loss: 0.0022277659736573696  Test loss: 0.004785236436873674 \n",
      "Epoch: 1/1:  mini-batch 573/1896:  Train loss: 0.002564417663961649  Test loss: 0.004783545155078173 \n",
      "Epoch: 1/1:  mini-batch 574/1896:  Train loss: 0.0008250752580352128  Test loss: 0.004785564262419939 \n",
      "Epoch: 1/1:  mini-batch 575/1896:  Train loss: 0.001852498040534556  Test loss: 0.004812651313841343 \n",
      "Epoch: 1/1:  mini-batch 576/1896:  Train loss: 0.0008065719157457352  Test loss: 0.004842295311391354 \n",
      "Epoch: 1/1:  mini-batch 577/1896:  Train loss: 0.002059344435110688  Test loss: 0.004851018078625202 \n",
      "Epoch: 1/1:  mini-batch 578/1896:  Train loss: 0.003948659636080265  Test loss: 0.004811763763427734 \n",
      "Epoch: 1/1:  mini-batch 579/1896:  Train loss: 0.0062578157521784306  Test loss: 0.004767607431858778 \n",
      "Epoch: 1/1:  mini-batch 580/1896:  Train loss: 0.003716664155945182  Test loss: 0.00476565957069397 \n",
      "Epoch: 1/1:  mini-batch 581/1896:  Train loss: 0.004958733916282654  Test loss: 0.004793059080839157 \n",
      "Epoch: 1/1:  mini-batch 582/1896:  Train loss: 0.0026425146497786045  Test loss: 0.004805983044207096 \n",
      "Epoch: 1/1:  mini-batch 583/1896:  Train loss: 0.002522908616811037  Test loss: 0.004799961112439632 \n",
      "Epoch: 1/1:  mini-batch 584/1896:  Train loss: 0.0007711724028922617  Test loss: 0.004784065298736095 \n",
      "Epoch: 1/1:  mini-batch 585/1896:  Train loss: 0.0017631638329476118  Test loss: 0.004780924879014492 \n",
      "Epoch: 1/1:  mini-batch 586/1896:  Train loss: 0.0005216288845986128  Test loss: 0.004812938626855612 \n",
      "Epoch: 1/1:  mini-batch 587/1896:  Train loss: 0.0010101641528308392  Test loss: 0.004850718192756176 \n",
      "Epoch: 1/1:  mini-batch 588/1896:  Train loss: 0.0025056260637938976  Test loss: 0.004864429589360952 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 589/1896:  Train loss: 0.003443459514528513  Test loss: 0.0048233638517558575 \n",
      "Epoch: 1/1:  mini-batch 590/1896:  Train loss: 0.00180351035669446  Test loss: 0.004775865934789181 \n",
      "Epoch: 1/1:  mini-batch 591/1896:  Train loss: 0.0034224078990519047  Test loss: 0.004770252853631973 \n",
      "Epoch: 1/1:  mini-batch 592/1896:  Train loss: 0.001793840667232871  Test loss: 0.004807229153811932 \n",
      "Epoch: 1/1:  mini-batch 593/1896:  Train loss: 0.00437180744484067  Test loss: 0.004831346683204174 \n",
      "Epoch: 1/1:  mini-batch 594/1896:  Train loss: 0.002677090000361204  Test loss: 0.004806973971426487 \n",
      "Epoch: 1/1:  mini-batch 595/1896:  Train loss: 0.004390290938317776  Test loss: 0.004777503199875355 \n",
      "Epoch: 1/1:  mini-batch 596/1896:  Train loss: 0.0031455932185053825  Test loss: 0.0047775376588106155 \n",
      "Epoch: 1/1:  mini-batch 597/1896:  Train loss: 0.002613619901239872  Test loss: 0.004792089574038982 \n",
      "Epoch: 1/1:  mini-batch 598/1896:  Train loss: 0.0015795518411323428  Test loss: 0.004802638199180365 \n",
      "Epoch: 1/1:  mini-batch 599/1896:  Train loss: 0.0039762272499501705  Test loss: 0.004788154736161232 \n",
      "Epoch: 1/1:  mini-batch 600/1896:  Train loss: 0.003657187568023801  Test loss: 0.00477137416601181 \n",
      "Epoch: 1/1:  mini-batch 601/1896:  Train loss: 0.0022790911607444286  Test loss: 0.004771234467625618 \n",
      "Epoch: 1/1:  mini-batch 602/1896:  Train loss: 0.0029256276320666075  Test loss: 0.004766017198562622 \n",
      "Epoch: 1/1:  mini-batch 603/1896:  Train loss: 0.003988396376371384  Test loss: 0.004766606725752354 \n",
      "Epoch: 1/1:  mini-batch 604/1896:  Train loss: 0.004237422253936529  Test loss: 0.004772521555423737 \n",
      "Epoch: 1/1:  mini-batch 605/1896:  Train loss: 0.0010705702006816864  Test loss: 0.004773704335093498 \n",
      "Epoch: 1/1:  mini-batch 606/1896:  Train loss: 0.0032155001536011696  Test loss: 0.0047723958268761635 \n",
      "Epoch: 1/1:  mini-batch 607/1896:  Train loss: 0.0026294186245650053  Test loss: 0.004771490581333637 \n",
      "Epoch: 1/1:  mini-batch 608/1896:  Train loss: 0.0006404152372851968  Test loss: 0.004766855388879776 \n",
      "Epoch: 1/1:  mini-batch 609/1896:  Train loss: 0.001708615687675774  Test loss: 0.004767176695168018 \n",
      "Epoch: 1/1:  mini-batch 610/1896:  Train loss: 0.0027519462164491415  Test loss: 0.004772693384438753 \n",
      "Epoch: 1/1:  mini-batch 611/1896:  Train loss: 0.0008241369505412877  Test loss: 0.0047838385216891766 \n",
      "Epoch: 1/1:  mini-batch 612/1896:  Train loss: 0.0025408456567674875  Test loss: 0.004786354023963213 \n",
      "Epoch: 1/1:  mini-batch 613/1896:  Train loss: 0.002988476539030671  Test loss: 0.004793735686689615 \n",
      "Epoch: 1/1:  mini-batch 614/1896:  Train loss: 0.004001301247626543  Test loss: 0.004793504718691111 \n",
      "Epoch: 1/1:  mini-batch 615/1896:  Train loss: 0.00469767302274704  Test loss: 0.004773127380758524 \n",
      "Epoch: 1/1:  mini-batch 616/1896:  Train loss: 0.0047426605597138405  Test loss: 0.004783269017934799 \n",
      "Epoch: 1/1:  mini-batch 617/1896:  Train loss: 0.008453728631138802  Test loss: 0.004789390601217747 \n",
      "Epoch: 1/1:  mini-batch 618/1896:  Train loss: 0.000755300628952682  Test loss: 0.0047789886593818665 \n",
      "Epoch: 1/1:  mini-batch 619/1896:  Train loss: 0.004963170737028122  Test loss: 0.004763483069837093 \n",
      "Epoch: 1/1:  mini-batch 620/1896:  Train loss: 0.002685647225007415  Test loss: 0.004805382806807756 \n",
      "Epoch: 1/1:  mini-batch 621/1896:  Train loss: 0.0009635793976485729  Test loss: 0.004887842573225498 \n",
      "Epoch: 1/1:  mini-batch 622/1896:  Train loss: 0.0026219235733151436  Test loss: 0.004922367166727781 \n",
      "Epoch: 1/1:  mini-batch 623/1896:  Train loss: 0.0033196525182574987  Test loss: 0.00492632482200861 \n",
      "Epoch: 1/1:  mini-batch 624/1896:  Train loss: 0.004037146922200918  Test loss: 0.004828714765608311 \n",
      "Epoch: 1/1:  mini-batch 625/1896:  Train loss: 0.004742625169456005  Test loss: 0.0047658951953053474 \n",
      "Epoch: 1/1:  mini-batch 626/1896:  Train loss: 0.001716099795885384  Test loss: 0.0047751544043421745 \n",
      "Epoch: 1/1:  mini-batch 627/1896:  Train loss: 0.005632174666970968  Test loss: 0.004809101112186909 \n",
      "Epoch: 1/1:  mini-batch 628/1896:  Train loss: 0.0015646407846361399  Test loss: 0.004821646027266979 \n",
      "Epoch: 1/1:  mini-batch 629/1896:  Train loss: 0.0027925793547183275  Test loss: 0.004797699861228466 \n",
      "Epoch: 1/1:  mini-batch 630/1896:  Train loss: 0.004476576577872038  Test loss: 0.004779335111379623 \n",
      "Epoch: 1/1:  mini-batch 631/1896:  Train loss: 0.004257849417626858  Test loss: 0.004771939478814602 \n",
      "Epoch: 1/1:  mini-batch 632/1896:  Train loss: 0.005362819880247116  Test loss: 0.004787788726389408 \n",
      "Epoch: 1/1:  mini-batch 633/1896:  Train loss: 0.002365419641137123  Test loss: 0.0048263464123010635 \n",
      "Epoch: 1/1:  mini-batch 634/1896:  Train loss: 0.004674606490880251  Test loss: 0.004837022162973881 \n",
      "Epoch: 1/1:  mini-batch 635/1896:  Train loss: 0.003179299645125866  Test loss: 0.00480828620493412 \n",
      "Epoch: 1/1:  mini-batch 636/1896:  Train loss: 0.004446513019502163  Test loss: 0.004777403548359871 \n",
      "Epoch: 1/1:  mini-batch 637/1896:  Train loss: 0.0016858902527019382  Test loss: 0.004767066799104214 \n",
      "Epoch: 1/1:  mini-batch 638/1896:  Train loss: 0.007720626424998045  Test loss: 0.004768266808241606 \n",
      "Epoch: 1/1:  mini-batch 639/1896:  Train loss: 0.005639693234115839  Test loss: 0.0047630504705011845 \n",
      "Epoch: 1/1:  mini-batch 640/1896:  Train loss: 0.006877921521663666  Test loss: 0.004777354653924704 \n",
      "Epoch: 1/1:  mini-batch 641/1896:  Train loss: 0.00653900858014822  Test loss: 0.004776934161782265 \n",
      "Epoch: 1/1:  mini-batch 642/1896:  Train loss: 0.0040711527690291405  Test loss: 0.004788591526448727 \n",
      "Epoch: 1/1:  mini-batch 643/1896:  Train loss: 0.005006829276680946  Test loss: 0.00478341244161129 \n",
      "Epoch: 1/1:  mini-batch 644/1896:  Train loss: 0.004601125605404377  Test loss: 0.004786936566233635 \n",
      "Epoch: 1/1:  mini-batch 645/1896:  Train loss: 0.005789057817310095  Test loss: 0.004799803718924522 \n",
      "Epoch: 1/1:  mini-batch 646/1896:  Train loss: 0.0023566680029034615  Test loss: 0.004801928531378508 \n",
      "Epoch: 1/1:  mini-batch 647/1896:  Train loss: 0.0013678395189344883  Test loss: 0.004783728625625372 \n",
      "Epoch: 1/1:  mini-batch 648/1896:  Train loss: 0.0025627664290368557  Test loss: 0.004777225665748119 \n",
      "Epoch: 1/1:  mini-batch 649/1896:  Train loss: 0.0037713823840022087  Test loss: 0.004769864492118359 \n",
      "Epoch: 1/1:  mini-batch 650/1896:  Train loss: 0.0007335128029808402  Test loss: 0.0047626374289393425 \n",
      "Epoch: 1/1:  mini-batch 651/1896:  Train loss: 0.0025491793639957905  Test loss: 0.004762979224324226 \n",
      "Epoch: 1/1:  mini-batch 652/1896:  Train loss: 0.005061678122729063  Test loss: 0.0047640325501561165 \n",
      "Epoch: 1/1:  mini-batch 653/1896:  Train loss: 0.006706806365400553  Test loss: 0.004773889202624559 \n",
      "Epoch: 1/1:  mini-batch 654/1896:  Train loss: 0.0037896158173680305  Test loss: 0.0047835418954491615 \n",
      "Epoch: 1/1:  mini-batch 655/1896:  Train loss: 0.004015729762613773  Test loss: 0.004780875518918037 \n",
      "Epoch: 1/1:  mini-batch 656/1896:  Train loss: 0.0034221960231661797  Test loss: 0.004777057096362114 \n",
      "Epoch: 1/1:  mini-batch 657/1896:  Train loss: 0.004712746012955904  Test loss: 0.004775472451001406 \n",
      "Epoch: 1/1:  mini-batch 658/1896:  Train loss: 0.0022988945711404085  Test loss: 0.004773993510752916 \n",
      "Epoch: 1/1:  mini-batch 659/1896:  Train loss: 0.005062507465481758  Test loss: 0.004766680765897036 \n",
      "Epoch: 1/1:  mini-batch 660/1896:  Train loss: 0.0021537523716688156  Test loss: 0.004764558747410774 \n",
      "Epoch: 1/1:  mini-batch 661/1896:  Train loss: 0.0006979070021770895  Test loss: 0.004778796806931496 \n",
      "Epoch: 1/1:  mini-batch 662/1896:  Train loss: 0.003907836973667145  Test loss: 0.0048201908357441425 \n",
      "Epoch: 1/1:  mini-batch 663/1896:  Train loss: 0.002426385646685958  Test loss: 0.004868015646934509 \n",
      "Epoch: 1/1:  mini-batch 664/1896:  Train loss: 0.0026828055270016193  Test loss: 0.004915270023047924 \n",
      "Epoch: 1/1:  mini-batch 665/1896:  Train loss: 0.002925032749772072  Test loss: 0.004880070686340332 \n",
      "Epoch: 1/1:  mini-batch 666/1896:  Train loss: 0.002500807400792837  Test loss: 0.004841756075620651 \n",
      "Epoch: 1/1:  mini-batch 667/1896:  Train loss: 0.004027608782052994  Test loss: 0.0047988202422857285 \n",
      "Epoch: 1/1:  mini-batch 668/1896:  Train loss: 0.002859491156414151  Test loss: 0.004773723892867565 \n",
      "Epoch: 1/1:  mini-batch 669/1896:  Train loss: 0.00240809703245759  Test loss: 0.004765793215483427 \n",
      "Epoch: 1/1:  mini-batch 670/1896:  Train loss: 0.001960081048309803  Test loss: 0.004761131480336189 \n",
      "Epoch: 1/1:  mini-batch 671/1896:  Train loss: 0.0023741223849356174  Test loss: 0.004766941070556641 \n",
      "Epoch: 1/1:  mini-batch 672/1896:  Train loss: 0.0005995615501888096  Test loss: 0.00477265939116478 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 673/1896:  Train loss: 0.004649881273508072  Test loss: 0.004765704739838839 \n",
      "Epoch: 1/1:  mini-batch 674/1896:  Train loss: 0.0010317008709535003  Test loss: 0.00477122375741601 \n",
      "Epoch: 1/1:  mini-batch 675/1896:  Train loss: 0.0030530437361449003  Test loss: 0.004789473954588175 \n",
      "Epoch: 1/1:  mini-batch 676/1896:  Train loss: 0.0022374875843524933  Test loss: 0.004808453842997551 \n",
      "Epoch: 1/1:  mini-batch 677/1896:  Train loss: 0.004873814061284065  Test loss: 0.004806572571396828 \n",
      "Epoch: 1/1:  mini-batch 678/1896:  Train loss: 0.003087816061452031  Test loss: 0.004783298820257187 \n",
      "Epoch: 1/1:  mini-batch 679/1896:  Train loss: 0.0027461363933980465  Test loss: 0.004765472374856472 \n",
      "Epoch: 1/1:  mini-batch 680/1896:  Train loss: 0.004627926275134087  Test loss: 0.004768974147737026 \n",
      "Epoch: 1/1:  mini-batch 681/1896:  Train loss: 0.0009093717089854181  Test loss: 0.004777918104082346 \n",
      "Epoch: 1/1:  mini-batch 682/1896:  Train loss: 0.0007825186476111412  Test loss: 0.004778095055371523 \n",
      "Epoch: 1/1:  mini-batch 683/1896:  Train loss: 0.0008055955404415727  Test loss: 0.004770382307469845 \n",
      "Epoch: 1/1:  mini-batch 684/1896:  Train loss: 0.0034309271723031998  Test loss: 0.004766636528074741 \n",
      "Epoch: 1/1:  mini-batch 685/1896:  Train loss: 0.001308166072703898  Test loss: 0.004777660593390465 \n",
      "Epoch: 1/1:  mini-batch 686/1896:  Train loss: 0.0011647616047412157  Test loss: 0.004785284399986267 \n",
      "Epoch: 1/1:  mini-batch 687/1896:  Train loss: 0.0021519428119063377  Test loss: 0.004792052321135998 \n",
      "Epoch: 1/1:  mini-batch 688/1896:  Train loss: 0.0027392981573939323  Test loss: 0.004795340355485678 \n",
      "Epoch: 1/1:  mini-batch 689/1896:  Train loss: 0.0029168776236474514  Test loss: 0.004787711892277002 \n",
      "Epoch: 1/1:  mini-batch 690/1896:  Train loss: 0.0071502747014164925  Test loss: 0.004772484768182039 \n",
      "Epoch: 1/1:  mini-batch 691/1896:  Train loss: 0.004214146640151739  Test loss: 0.004763117991387844 \n",
      "Epoch: 1/1:  mini-batch 692/1896:  Train loss: 0.0019117912743240595  Test loss: 0.004770058207213879 \n",
      "Epoch: 1/1:  mini-batch 693/1896:  Train loss: 0.007302894257009029  Test loss: 0.00476524792611599 \n",
      "Epoch: 1/1:  mini-batch 694/1896:  Train loss: 0.0007720584399066865  Test loss: 0.004773465916514397 \n",
      "Epoch: 1/1:  mini-batch 695/1896:  Train loss: 0.0004977727658115327  Test loss: 0.004795893095433712 \n",
      "Epoch: 1/1:  mini-batch 696/1896:  Train loss: 0.0008058496750891209  Test loss: 0.004823616705834866 \n",
      "Epoch: 1/1:  mini-batch 697/1896:  Train loss: 0.0028870427049696445  Test loss: 0.004823447670787573 \n",
      "Epoch: 1/1:  mini-batch 698/1896:  Train loss: 0.004192573484033346  Test loss: 0.004782562144100666 \n",
      "Epoch: 1/1:  mini-batch 699/1896:  Train loss: 0.0030006940942257643  Test loss: 0.004764340352267027 \n",
      "Epoch: 1/1:  mini-batch 700/1896:  Train loss: 0.0006508483784273267  Test loss: 0.004769749939441681 \n",
      "Epoch: 1/1:  mini-batch 701/1896:  Train loss: 0.0004337304853834212  Test loss: 0.004784991033375263 \n",
      "Epoch: 1/1:  mini-batch 702/1896:  Train loss: 0.0009368677856400609  Test loss: 0.004787897691130638 \n",
      "Epoch: 1/1:  mini-batch 703/1896:  Train loss: 0.002242656424641609  Test loss: 0.004788192920386791 \n",
      "Epoch: 1/1:  mini-batch 704/1896:  Train loss: 0.004232248291373253  Test loss: 0.004785668104887009 \n",
      "Epoch: 1/1:  mini-batch 705/1896:  Train loss: 0.003271391149610281  Test loss: 0.004781242460012436 \n",
      "Epoch: 1/1:  mini-batch 706/1896:  Train loss: 0.005122336558997631  Test loss: 0.004784510470926762 \n",
      "Epoch: 1/1:  mini-batch 707/1896:  Train loss: 0.006872277241200209  Test loss: 0.004773627035319805 \n",
      "Epoch: 1/1:  mini-batch 708/1896:  Train loss: 0.0040999241173267365  Test loss: 0.0047708069905638695 \n",
      "Epoch: 1/1:  mini-batch 709/1896:  Train loss: 0.006957270670682192  Test loss: 0.0047791460528969765 \n",
      "Epoch: 1/1:  mini-batch 710/1896:  Train loss: 0.004479268565773964  Test loss: 0.004793298430740833 \n",
      "Epoch: 1/1:  mini-batch 711/1896:  Train loss: 0.0024847020395100117  Test loss: 0.004795461893081665 \n",
      "Epoch: 1/1:  mini-batch 712/1896:  Train loss: 0.004106723237782717  Test loss: 0.004790923558175564 \n",
      "Epoch: 1/1:  mini-batch 713/1896:  Train loss: 0.0016008388483896852  Test loss: 0.004780258983373642 \n",
      "Epoch: 1/1:  mini-batch 714/1896:  Train loss: 0.00652399193495512  Test loss: 0.004766726400703192 \n",
      "Epoch: 1/1:  mini-batch 715/1896:  Train loss: 0.0032198303379118443  Test loss: 0.004768213257193565 \n",
      "Epoch: 1/1:  mini-batch 716/1896:  Train loss: 0.008068700321018696  Test loss: 0.004809296689927578 \n",
      "Epoch: 1/1:  mini-batch 717/1896:  Train loss: 0.0014550617197528481  Test loss: 0.004829329438507557 \n",
      "Epoch: 1/1:  mini-batch 718/1896:  Train loss: 0.006271504331380129  Test loss: 0.004791847430169582 \n",
      "Epoch: 1/1:  mini-batch 719/1896:  Train loss: 0.003912793938070536  Test loss: 0.004774837754666805 \n",
      "Epoch: 1/1:  mini-batch 720/1896:  Train loss: 0.008760299533605576  Test loss: 0.004781973082572222 \n",
      "Epoch: 1/1:  mini-batch 721/1896:  Train loss: 0.002525864401832223  Test loss: 0.004789568018168211 \n",
      "Epoch: 1/1:  mini-batch 722/1896:  Train loss: 0.0050798580050468445  Test loss: 0.004790342412889004 \n",
      "Epoch: 1/1:  mini-batch 723/1896:  Train loss: 0.008274920284748077  Test loss: 0.004764131270349026 \n",
      "Epoch: 1/1:  mini-batch 724/1896:  Train loss: 0.0014183006715029478  Test loss: 0.004765333607792854 \n",
      "Epoch: 1/1:  mini-batch 725/1896:  Train loss: 0.006258392706513405  Test loss: 0.004775571171194315 \n",
      "Epoch: 1/1:  mini-batch 726/1896:  Train loss: 0.005546660162508488  Test loss: 0.004787399433553219 \n",
      "Epoch: 1/1:  mini-batch 727/1896:  Train loss: 0.004076859913766384  Test loss: 0.004771358333528042 \n",
      "Epoch: 1/1:  mini-batch 728/1896:  Train loss: 0.004525723401457071  Test loss: 0.004763076547533274 \n",
      "Epoch: 1/1:  mini-batch 729/1896:  Train loss: 0.005133385770022869  Test loss: 0.0047891344875097275 \n",
      "Epoch: 1/1:  mini-batch 730/1896:  Train loss: 0.0058027999475598335  Test loss: 0.00481195654720068 \n",
      "Epoch: 1/1:  mini-batch 731/1896:  Train loss: 0.011000269092619419  Test loss: 0.004809180274605751 \n",
      "Epoch: 1/1:  mini-batch 732/1896:  Train loss: 0.008436549454927444  Test loss: 0.0048027196899056435 \n",
      "Epoch: 1/1:  mini-batch 733/1896:  Train loss: 0.0008417779463343322  Test loss: 0.004793455824255943 \n",
      "Epoch: 1/1:  mini-batch 734/1896:  Train loss: 0.0029364521615207195  Test loss: 0.004782020580023527 \n",
      "Epoch: 1/1:  mini-batch 735/1896:  Train loss: 0.0027281870134174824  Test loss: 0.004765062592923641 \n",
      "Epoch: 1/1:  mini-batch 736/1896:  Train loss: 0.007642116863280535  Test loss: 0.004761291202157736 \n",
      "Epoch: 1/1:  mini-batch 737/1896:  Train loss: 0.003917343448847532  Test loss: 0.004768754355609417 \n",
      "Epoch: 1/1:  mini-batch 738/1896:  Train loss: 0.002906359266489744  Test loss: 0.004776475485414267 \n",
      "Epoch: 1/1:  mini-batch 739/1896:  Train loss: 0.002481454750522971  Test loss: 0.0047792308032512665 \n",
      "Epoch: 1/1:  mini-batch 740/1896:  Train loss: 0.0030834991484880447  Test loss: 0.004781937226653099 \n",
      "Epoch: 1/1:  mini-batch 741/1896:  Train loss: 0.007085141260176897  Test loss: 0.004782832693308592 \n",
      "Epoch: 1/1:  mini-batch 742/1896:  Train loss: 0.007036121562123299  Test loss: 0.00478917732834816 \n",
      "Epoch: 1/1:  mini-batch 743/1896:  Train loss: 0.0034462681505829096  Test loss: 0.004809727892279625 \n",
      "Epoch: 1/1:  mini-batch 744/1896:  Train loss: 0.004385767970234156  Test loss: 0.004829219542443752 \n",
      "Epoch: 1/1:  mini-batch 745/1896:  Train loss: 0.0028666271828114986  Test loss: 0.004859378561377525 \n",
      "Epoch: 1/1:  mini-batch 746/1896:  Train loss: 0.00600459286943078  Test loss: 0.0048538437113165855 \n",
      "Epoch: 1/1:  mini-batch 747/1896:  Train loss: 0.001054899301379919  Test loss: 0.004822115413844585 \n",
      "Epoch: 1/1:  mini-batch 748/1896:  Train loss: 0.003973107784986496  Test loss: 0.00479927146807313 \n",
      "Epoch: 1/1:  mini-batch 749/1896:  Train loss: 0.003306485014036298  Test loss: 0.004786703735589981 \n",
      "Epoch: 1/1:  mini-batch 750/1896:  Train loss: 0.002160249976441264  Test loss: 0.004788331687450409 \n",
      "Epoch: 1/1:  mini-batch 751/1896:  Train loss: 0.004457559436559677  Test loss: 0.004809115082025528 \n",
      "Epoch: 1/1:  mini-batch 752/1896:  Train loss: 0.0026171617209911346  Test loss: 0.004820122383534908 \n",
      "Epoch: 1/1:  mini-batch 753/1896:  Train loss: 0.004918358288705349  Test loss: 0.004803764633834362 \n",
      "Epoch: 1/1:  mini-batch 754/1896:  Train loss: 0.0020602731965482235  Test loss: 0.004786897450685501 \n",
      "Epoch: 1/1:  mini-batch 755/1896:  Train loss: 0.0004102750972378999  Test loss: 0.004765982273966074 \n",
      "Epoch: 1/1:  mini-batch 756/1896:  Train loss: 0.004745251964777708  Test loss: 0.004768350627273321 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 757/1896:  Train loss: 0.0006933080148883164  Test loss: 0.0047691743820905685 \n",
      "Epoch: 1/1:  mini-batch 758/1896:  Train loss: 0.005850397981703281  Test loss: 0.004777018446475267 \n",
      "Epoch: 1/1:  mini-batch 759/1896:  Train loss: 0.004689154680818319  Test loss: 0.004787153098732233 \n",
      "Epoch: 1/1:  mini-batch 760/1896:  Train loss: 0.002285404596477747  Test loss: 0.004787623882293701 \n",
      "Epoch: 1/1:  mini-batch 761/1896:  Train loss: 0.004885862115770578  Test loss: 0.004785208962857723 \n",
      "Epoch: 1/1:  mini-batch 762/1896:  Train loss: 0.00205594883300364  Test loss: 0.004792121239006519 \n",
      "Epoch: 1/1:  mini-batch 763/1896:  Train loss: 0.003976080100983381  Test loss: 0.004795417655259371 \n",
      "Epoch: 1/1:  mini-batch 764/1896:  Train loss: 0.00411573238670826  Test loss: 0.00477917306125164 \n",
      "Epoch: 1/1:  mini-batch 765/1896:  Train loss: 0.0020760418847203255  Test loss: 0.004766179248690605 \n",
      "Epoch: 1/1:  mini-batch 766/1896:  Train loss: 0.004306158050894737  Test loss: 0.004762793425470591 \n",
      "Epoch: 1/1:  mini-batch 767/1896:  Train loss: 0.0017806258983910084  Test loss: 0.004768936894834042 \n",
      "Epoch: 1/1:  mini-batch 768/1896:  Train loss: 0.0011950640473514795  Test loss: 0.004794174339622259 \n",
      "Epoch: 1/1:  mini-batch 769/1896:  Train loss: 0.0031854617409408092  Test loss: 0.004813706502318382 \n",
      "Epoch: 1/1:  mini-batch 770/1896:  Train loss: 0.0028903745114803314  Test loss: 0.004844021983444691 \n",
      "Epoch: 1/1:  mini-batch 771/1896:  Train loss: 0.0025103867519646883  Test loss: 0.004836813546717167 \n",
      "Epoch: 1/1:  mini-batch 772/1896:  Train loss: 0.004245555493980646  Test loss: 0.004808045923709869 \n",
      "Epoch: 1/1:  mini-batch 773/1896:  Train loss: 0.005898008123040199  Test loss: 0.0047959452494978905 \n",
      "Epoch: 1/1:  mini-batch 774/1896:  Train loss: 0.002141124103218317  Test loss: 0.004785272292792797 \n",
      "Epoch: 1/1:  mini-batch 775/1896:  Train loss: 0.005631769075989723  Test loss: 0.004766520112752914 \n",
      "Epoch: 1/1:  mini-batch 776/1896:  Train loss: 0.0026941357646137476  Test loss: 0.0047692786902189255 \n",
      "Epoch: 1/1:  mini-batch 777/1896:  Train loss: 0.002801257651299238  Test loss: 0.004788396880030632 \n",
      "Epoch: 1/1:  mini-batch 778/1896:  Train loss: 0.001271983957849443  Test loss: 0.004790406674146652 \n",
      "Epoch: 1/1:  mini-batch 779/1896:  Train loss: 0.003962363116443157  Test loss: 0.004782560281455517 \n",
      "Epoch: 1/1:  mini-batch 780/1896:  Train loss: 0.003666146192699671  Test loss: 0.0047986432909965515 \n",
      "Epoch: 1/1:  mini-batch 781/1896:  Train loss: 0.006545884069055319  Test loss: 0.0048472392372787 \n",
      "Epoch: 1/1:  mini-batch 782/1896:  Train loss: 0.0036326791159808636  Test loss: 0.004843925125896931 \n",
      "Epoch: 1/1:  mini-batch 783/1896:  Train loss: 0.0024199094623327255  Test loss: 0.0048126354813575745 \n",
      "Epoch: 1/1:  mini-batch 784/1896:  Train loss: 0.0072482191026210785  Test loss: 0.004767420701682568 \n",
      "Epoch: 1/1:  mini-batch 785/1896:  Train loss: 0.0027216423768550158  Test loss: 0.004776302725076675 \n",
      "Epoch: 1/1:  mini-batch 786/1896:  Train loss: 0.0007938647759146988  Test loss: 0.004820412956178188 \n",
      "Epoch: 1/1:  mini-batch 787/1896:  Train loss: 0.0054679508320987225  Test loss: 0.004851152189075947 \n",
      "Epoch: 1/1:  mini-batch 788/1896:  Train loss: 0.006466494407504797  Test loss: 0.004857571795582771 \n",
      "Epoch: 1/1:  mini-batch 789/1896:  Train loss: 0.00299840047955513  Test loss: 0.004849049262702465 \n",
      "Epoch: 1/1:  mini-batch 790/1896:  Train loss: 0.002858548890799284  Test loss: 0.004795984365046024 \n",
      "Epoch: 1/1:  mini-batch 791/1896:  Train loss: 0.002831620629876852  Test loss: 0.0047643473371863365 \n",
      "Epoch: 1/1:  mini-batch 792/1896:  Train loss: 0.0017257569124922156  Test loss: 0.004775303415954113 \n",
      "Epoch: 1/1:  mini-batch 793/1896:  Train loss: 0.003306834027171135  Test loss: 0.004818492569029331 \n",
      "Epoch: 1/1:  mini-batch 794/1896:  Train loss: 0.0027729764115065336  Test loss: 0.004897125065326691 \n",
      "Epoch: 1/1:  mini-batch 795/1896:  Train loss: 0.0043954867869615555  Test loss: 0.004988576285541058 \n",
      "Epoch: 1/1:  mini-batch 796/1896:  Train loss: 0.003577037248760462  Test loss: 0.004935374949127436 \n",
      "Epoch: 1/1:  mini-batch 797/1896:  Train loss: 0.002577240113168955  Test loss: 0.004856238607317209 \n",
      "Epoch: 1/1:  mini-batch 798/1896:  Train loss: 0.0029598427936434746  Test loss: 0.0047799572348594666 \n",
      "Epoch: 1/1:  mini-batch 799/1896:  Train loss: 0.006009248550981283  Test loss: 0.004767934791743755 \n",
      "Epoch: 1/1:  mini-batch 800/1896:  Train loss: 0.006409821100533009  Test loss: 0.004789248108863831 \n",
      "Epoch: 1/1:  mini-batch 801/1896:  Train loss: 0.0029710223898291588  Test loss: 0.004801309201866388 \n",
      "Epoch: 1/1:  mini-batch 802/1896:  Train loss: 0.0033215186558663845  Test loss: 0.004794719163328409 \n",
      "Epoch: 1/1:  mini-batch 803/1896:  Train loss: 0.0030387169681489468  Test loss: 0.0047761909663677216 \n",
      "Epoch: 1/1:  mini-batch 804/1896:  Train loss: 0.006727196741849184  Test loss: 0.004779310896992683 \n",
      "Epoch: 1/1:  mini-batch 805/1896:  Train loss: 0.0033998126164078712  Test loss: 0.004790784325450659 \n",
      "Epoch: 1/1:  mini-batch 806/1896:  Train loss: 0.006102859973907471  Test loss: 0.004818236455321312 \n",
      "Epoch: 1/1:  mini-batch 807/1896:  Train loss: 0.00446573318913579  Test loss: 0.004842321388423443 \n",
      "Epoch: 1/1:  mini-batch 808/1896:  Train loss: 0.0070929741486907005  Test loss: 0.0048016514629125595 \n",
      "Epoch: 1/1:  mini-batch 809/1896:  Train loss: 0.004014500882476568  Test loss: 0.0047897472977638245 \n",
      "Epoch: 1/1:  mini-batch 810/1896:  Train loss: 0.0037361823488026857  Test loss: 0.004781176336109638 \n",
      "Epoch: 1/1:  mini-batch 811/1896:  Train loss: 0.0043200175277888775  Test loss: 0.004771469160914421 \n",
      "Epoch: 1/1:  mini-batch 812/1896:  Train loss: 0.004912707023322582  Test loss: 0.004769827704876661 \n",
      "Epoch: 1/1:  mini-batch 813/1896:  Train loss: 0.001945209689438343  Test loss: 0.0047752647660672665 \n",
      "Epoch: 1/1:  mini-batch 814/1896:  Train loss: 0.0023087430745363235  Test loss: 0.004771252162754536 \n",
      "Epoch: 1/1:  mini-batch 815/1896:  Train loss: 0.003659733571112156  Test loss: 0.0047732205130159855 \n",
      "Epoch: 1/1:  mini-batch 816/1896:  Train loss: 0.004148506559431553  Test loss: 0.004782866686582565 \n",
      "Epoch: 1/1:  mini-batch 817/1896:  Train loss: 0.003760840278118849  Test loss: 0.004795145243406296 \n",
      "Epoch: 1/1:  mini-batch 818/1896:  Train loss: 0.0029983664862811565  Test loss: 0.004801057279109955 \n",
      "Epoch: 1/1:  mini-batch 819/1896:  Train loss: 0.007524663116782904  Test loss: 0.00477963499724865 \n",
      "Epoch: 1/1:  mini-batch 820/1896:  Train loss: 0.0015987211372703314  Test loss: 0.004788991529494524 \n",
      "Epoch: 1/1:  mini-batch 821/1896:  Train loss: 0.0049630808643996716  Test loss: 0.004782174713909626 \n",
      "Epoch: 1/1:  mini-batch 822/1896:  Train loss: 0.0028083068318665028  Test loss: 0.004770441446453333 \n",
      "Epoch: 1/1:  mini-batch 823/1896:  Train loss: 0.0011976291425526142  Test loss: 0.00478135421872139 \n",
      "Epoch: 1/1:  mini-batch 824/1896:  Train loss: 0.004529950674623251  Test loss: 0.004804980009794235 \n",
      "Epoch: 1/1:  mini-batch 825/1896:  Train loss: 0.002182363998144865  Test loss: 0.004831428173929453 \n",
      "Epoch: 1/1:  mini-batch 826/1896:  Train loss: 0.007441995665431023  Test loss: 0.0048183174803853035 \n",
      "Epoch: 1/1:  mini-batch 827/1896:  Train loss: 0.00102232675999403  Test loss: 0.004811602644622326 \n",
      "Epoch: 1/1:  mini-batch 828/1896:  Train loss: 0.005568439606577158  Test loss: 0.004781263414770365 \n",
      "Epoch: 1/1:  mini-batch 829/1896:  Train loss: 0.002482471987605095  Test loss: 0.0047845360822975636 \n",
      "Epoch: 1/1:  mini-batch 830/1896:  Train loss: 0.001202098443172872  Test loss: 0.004778257571160793 \n",
      "Epoch: 1/1:  mini-batch 831/1896:  Train loss: 0.0025232420302927494  Test loss: 0.0047723473981022835 \n",
      "Epoch: 1/1:  mini-batch 832/1896:  Train loss: 0.005972880404442549  Test loss: 0.004763329401612282 \n",
      "Epoch: 1/1:  mini-batch 833/1896:  Train loss: 0.00605031056329608  Test loss: 0.004777388647198677 \n",
      "Epoch: 1/1:  mini-batch 834/1896:  Train loss: 0.0021480820141732693  Test loss: 0.0048080673441290855 \n",
      "Epoch: 1/1:  mini-batch 835/1896:  Train loss: 0.007334649097174406  Test loss: 0.004846692085266113 \n",
      "Epoch: 1/1:  mini-batch 836/1896:  Train loss: 0.008200624957680702  Test loss: 0.0048313140869140625 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 837/1896:  Train loss: 0.0033626530785113573  Test loss: 0.0047717224806547165 \n",
      "Epoch: 1/1:  mini-batch 838/1896:  Train loss: 0.006189869716763496  Test loss: 0.0047659482806921005 \n",
      "Epoch: 1/1:  mini-batch 839/1896:  Train loss: 0.005340910516679287  Test loss: 0.0048190453089773655 \n",
      "Epoch: 1/1:  mini-batch 840/1896:  Train loss: 0.007988049648702145  Test loss: 0.004966077394783497 \n",
      "Epoch: 1/1:  mini-batch 841/1896:  Train loss: 0.0022761777509003878  Test loss: 0.0050443680956959724 \n",
      "Epoch: 1/1:  mini-batch 842/1896:  Train loss: 0.002632129704579711  Test loss: 0.004960958380252123 \n",
      "Epoch: 1/1:  mini-batch 843/1896:  Train loss: 0.0036526250187307596  Test loss: 0.00481627881526947 \n",
      "Epoch: 1/1:  mini-batch 844/1896:  Train loss: 0.0026450729928910732  Test loss: 0.00477925781160593 \n",
      "Epoch: 1/1:  mini-batch 845/1896:  Train loss: 0.0020353365689516068  Test loss: 0.004826407413929701 \n",
      "Epoch: 1/1:  mini-batch 846/1896:  Train loss: 0.0008956167148426175  Test loss: 0.00485499482601881 \n",
      "Epoch: 1/1:  mini-batch 847/1896:  Train loss: 0.0006977186421863735  Test loss: 0.004815653897821903 \n",
      "Epoch: 1/1:  mini-batch 848/1896:  Train loss: 0.004122401122003794  Test loss: 0.0047751362435519695 \n",
      "Epoch: 1/1:  mini-batch 849/1896:  Train loss: 0.005118722096085548  Test loss: 0.004767373204231262 \n",
      "Epoch: 1/1:  mini-batch 850/1896:  Train loss: 0.0026683970354497433  Test loss: 0.004828893579542637 \n",
      "Epoch: 1/1:  mini-batch 851/1896:  Train loss: 0.0018054430838674307  Test loss: 0.004884873516857624 \n",
      "Epoch: 1/1:  mini-batch 852/1896:  Train loss: 0.0035516046918928623  Test loss: 0.004881285596638918 \n",
      "Epoch: 1/1:  mini-batch 853/1896:  Train loss: 0.0035068702418357134  Test loss: 0.004878106527030468 \n",
      "Epoch: 1/1:  mini-batch 854/1896:  Train loss: 0.004587517119944096  Test loss: 0.004812730476260185 \n",
      "Epoch: 1/1:  mini-batch 855/1896:  Train loss: 0.002845475450158119  Test loss: 0.004772622603923082 \n",
      "Epoch: 1/1:  mini-batch 856/1896:  Train loss: 0.00047172640915960073  Test loss: 0.004792400170117617 \n",
      "Epoch: 1/1:  mini-batch 857/1896:  Train loss: 0.0031019188463687897  Test loss: 0.004801608622074127 \n",
      "Epoch: 1/1:  mini-batch 858/1896:  Train loss: 0.0026310812681913376  Test loss: 0.004788261838257313 \n",
      "Epoch: 1/1:  mini-batch 859/1896:  Train loss: 0.0026681534945964813  Test loss: 0.004773327615112066 \n",
      "Epoch: 1/1:  mini-batch 860/1896:  Train loss: 0.0012760291574522853  Test loss: 0.004768813960254192 \n",
      "Epoch: 1/1:  mini-batch 861/1896:  Train loss: 0.000627896748483181  Test loss: 0.00479968823492527 \n",
      "Epoch: 1/1:  mini-batch 862/1896:  Train loss: 0.002100158017128706  Test loss: 0.0048357234336435795 \n",
      "Epoch: 1/1:  mini-batch 863/1896:  Train loss: 0.0030237382743507624  Test loss: 0.004839428700506687 \n",
      "Epoch: 1/1:  mini-batch 864/1896:  Train loss: 0.0008633059915155172  Test loss: 0.00481470488011837 \n",
      "Epoch: 1/1:  mini-batch 865/1896:  Train loss: 0.0020171005744487047  Test loss: 0.0047724200412631035 \n",
      "Epoch: 1/1:  mini-batch 866/1896:  Train loss: 0.0075170183554291725  Test loss: 0.004767578095197678 \n",
      "Epoch: 1/1:  mini-batch 867/1896:  Train loss: 0.0019396356074139476  Test loss: 0.004784770309925079 \n",
      "Epoch: 1/1:  mini-batch 868/1896:  Train loss: 0.0018716825870797038  Test loss: 0.004791629035025835 \n",
      "Epoch: 1/1:  mini-batch 869/1896:  Train loss: 0.0020877360366284847  Test loss: 0.004771346226334572 \n",
      "Epoch: 1/1:  mini-batch 870/1896:  Train loss: 0.0031878123991191387  Test loss: 0.004771961830556393 \n",
      "Epoch: 1/1:  mini-batch 871/1896:  Train loss: 0.004319873638451099  Test loss: 0.00479244627058506 \n",
      "Epoch: 1/1:  mini-batch 872/1896:  Train loss: 0.003197504673153162  Test loss: 0.004815797787159681 \n",
      "Epoch: 1/1:  mini-batch 873/1896:  Train loss: 0.007579656317830086  Test loss: 0.004770974162966013 \n",
      "Epoch: 1/1:  mini-batch 874/1896:  Train loss: 0.002749356208369136  Test loss: 0.004768896847963333 \n",
      "Epoch: 1/1:  mini-batch 875/1896:  Train loss: 0.0053952839225530624  Test loss: 0.0048340908251702785 \n",
      "Epoch: 1/1:  mini-batch 876/1896:  Train loss: 0.006316023878753185  Test loss: 0.004842178896069527 \n",
      "Epoch: 1/1:  mini-batch 877/1896:  Train loss: 0.004943914711475372  Test loss: 0.004816096276044846 \n",
      "Epoch: 1/1:  mini-batch 878/1896:  Train loss: 0.006599654443562031  Test loss: 0.004785074852406979 \n",
      "Epoch: 1/1:  mini-batch 879/1896:  Train loss: 0.0032870834693312645  Test loss: 0.004828393459320068 \n",
      "Epoch: 1/1:  mini-batch 880/1896:  Train loss: 0.00393743347376585  Test loss: 0.004941415973007679 \n",
      "Epoch: 1/1:  mini-batch 881/1896:  Train loss: 0.0020142546854913235  Test loss: 0.005028686486184597 \n",
      "Epoch: 1/1:  mini-batch 882/1896:  Train loss: 0.005079108756035566  Test loss: 0.005010061431676149 \n",
      "Epoch: 1/1:  mini-batch 883/1896:  Train loss: 0.004531091079115868  Test loss: 0.004848460666835308 \n",
      "Epoch: 1/1:  mini-batch 884/1896:  Train loss: 0.0036333282478153706  Test loss: 0.004761597607284784 \n",
      "Epoch: 1/1:  mini-batch 885/1896:  Train loss: 0.005382482893764973  Test loss: 0.004804716911166906 \n",
      "Epoch: 1/1:  mini-batch 886/1896:  Train loss: 0.0062619103118777275  Test loss: 0.004909384064376354 \n",
      "Epoch: 1/1:  mini-batch 887/1896:  Train loss: 0.001856061746366322  Test loss: 0.0049544088542461395 \n",
      "Epoch: 1/1:  mini-batch 888/1896:  Train loss: 0.002524495590478182  Test loss: 0.004863684065639973 \n",
      "Epoch: 1/1:  mini-batch 889/1896:  Train loss: 0.004823240451514721  Test loss: 0.004771871026605368 \n",
      "Epoch: 1/1:  mini-batch 890/1896:  Train loss: 0.0015681827208027244  Test loss: 0.004807286895811558 \n",
      "Epoch: 1/1:  mini-batch 891/1896:  Train loss: 0.009897996671497822  Test loss: 0.004905104637145996 \n",
      "Epoch: 1/1:  mini-batch 892/1896:  Train loss: 0.004604957066476345  Test loss: 0.0048767803236842155 \n",
      "Epoch: 1/1:  mini-batch 893/1896:  Train loss: 0.00283627538010478  Test loss: 0.004811582155525684 \n",
      "Epoch: 1/1:  mini-batch 894/1896:  Train loss: 0.006403887644410133  Test loss: 0.004770100582391024 \n",
      "Epoch: 1/1:  mini-batch 895/1896:  Train loss: 0.0022624938283115625  Test loss: 0.00478016585111618 \n",
      "Epoch: 1/1:  mini-batch 896/1896:  Train loss: 0.0006870635552331805  Test loss: 0.004790592007339001 \n",
      "Epoch: 1/1:  mini-batch 897/1896:  Train loss: 0.0021028483752161264  Test loss: 0.004791419021785259 \n",
      "Epoch: 1/1:  mini-batch 898/1896:  Train loss: 0.005102436523884535  Test loss: 0.004773165564984083 \n",
      "Epoch: 1/1:  mini-batch 899/1896:  Train loss: 0.004532929975539446  Test loss: 0.004770677071064711 \n",
      "Epoch: 1/1:  mini-batch 900/1896:  Train loss: 0.0024789166636765003  Test loss: 0.004785478580743074 \n",
      "Epoch: 1/1:  mini-batch 901/1896:  Train loss: 0.0024530314840376377  Test loss: 0.004815328866243362 \n",
      "Epoch: 1/1:  mini-batch 902/1896:  Train loss: 0.002575126476585865  Test loss: 0.00483145285397768 \n",
      "Epoch: 1/1:  mini-batch 903/1896:  Train loss: 0.003555447096005082  Test loss: 0.004831406287848949 \n",
      "Epoch: 1/1:  mini-batch 904/1896:  Train loss: 0.007209040690213442  Test loss: 0.004781513474881649 \n",
      "Epoch: 1/1:  mini-batch 905/1896:  Train loss: 0.005704669747501612  Test loss: 0.004780175164341927 \n",
      "Epoch: 1/1:  mini-batch 906/1896:  Train loss: 0.002643282525241375  Test loss: 0.004795573651790619 \n",
      "Epoch: 1/1:  mini-batch 907/1896:  Train loss: 0.004439404234290123  Test loss: 0.0047850110568106174 \n",
      "Epoch: 1/1:  mini-batch 908/1896:  Train loss: 0.005060947500169277  Test loss: 0.004765302874147892 \n",
      "Epoch: 1/1:  mini-batch 909/1896:  Train loss: 0.0027364417910575867  Test loss: 0.004762980155646801 \n",
      "Epoch: 1/1:  mini-batch 910/1896:  Train loss: 0.005355823319405317  Test loss: 0.004763285629451275 \n",
      "Epoch: 1/1:  mini-batch 911/1896:  Train loss: 0.0012662247754633427  Test loss: 0.004763892386108637 \n",
      "Epoch: 1/1:  mini-batch 912/1896:  Train loss: 0.0023736716248095036  Test loss: 0.004781175404787064 \n",
      "Epoch: 1/1:  mini-batch 913/1896:  Train loss: 0.0035607877653092146  Test loss: 0.004814809188246727 \n",
      "Epoch: 1/1:  mini-batch 914/1896:  Train loss: 0.0025101900100708008  Test loss: 0.004820484668016434 \n",
      "Epoch: 1/1:  mini-batch 915/1896:  Train loss: 0.003696083789691329  Test loss: 0.004799797665327787 \n",
      "Epoch: 1/1:  mini-batch 916/1896:  Train loss: 0.005081612151116133  Test loss: 0.004776191897690296 \n",
      "Epoch: 1/1:  mini-batch 917/1896:  Train loss: 0.0019398103468120098  Test loss: 0.004767535254359245 \n",
      "Epoch: 1/1:  mini-batch 918/1896:  Train loss: 0.0011322812642902136  Test loss: 0.004767967388033867 \n",
      "Epoch: 1/1:  mini-batch 919/1896:  Train loss: 0.006145351100713015  Test loss: 0.004783788230270147 \n",
      "Epoch: 1/1:  mini-batch 920/1896:  Train loss: 0.003550418186932802  Test loss: 0.004794621840119362 \n",
      "Epoch: 1/1:  mini-batch 921/1896:  Train loss: 0.0031676539219915867  Test loss: 0.004790038336068392 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 922/1896:  Train loss: 0.004181730095297098  Test loss: 0.004768797196447849 \n",
      "Epoch: 1/1:  mini-batch 923/1896:  Train loss: 0.002584472531452775  Test loss: 0.004783349111676216 \n",
      "Epoch: 1/1:  mini-batch 924/1896:  Train loss: 0.0026874674949795008  Test loss: 0.004887689836323261 \n",
      "Epoch: 1/1:  mini-batch 925/1896:  Train loss: 0.0022849799133837223  Test loss: 0.0050008660182356834 \n",
      "Epoch: 1/1:  mini-batch 926/1896:  Train loss: 0.0027709000278264284  Test loss: 0.004954219330102205 \n",
      "Epoch: 1/1:  mini-batch 927/1896:  Train loss: 0.0025445695500820875  Test loss: 0.004869731608778238 \n",
      "Epoch: 1/1:  mini-batch 928/1896:  Train loss: 0.0030416876543313265  Test loss: 0.004785692319273949 \n",
      "Epoch: 1/1:  mini-batch 929/1896:  Train loss: 0.001519298879429698  Test loss: 0.004768132232129574 \n",
      "Epoch: 1/1:  mini-batch 930/1896:  Train loss: 0.005869843065738678  Test loss: 0.004795426968485117 \n",
      "Epoch: 1/1:  mini-batch 931/1896:  Train loss: 0.0017221459420397878  Test loss: 0.004822032526135445 \n",
      "Epoch: 1/1:  mini-batch 932/1896:  Train loss: 0.0025657988153398037  Test loss: 0.0047915177419781685 \n",
      "Epoch: 1/1:  mini-batch 933/1896:  Train loss: 0.003769155591726303  Test loss: 0.004763975273817778 \n",
      "Epoch: 1/1:  mini-batch 934/1896:  Train loss: 0.002006951253861189  Test loss: 0.004780310206115246 \n",
      "Epoch: 1/1:  mini-batch 935/1896:  Train loss: 0.0027099368162453175  Test loss: 0.004844522103667259 \n",
      "Epoch: 1/1:  mini-batch 936/1896:  Train loss: 0.0029396815225481987  Test loss: 0.004886614624410868 \n",
      "Epoch: 1/1:  mini-batch 937/1896:  Train loss: 0.005080102011561394  Test loss: 0.004827931523323059 \n",
      "Epoch: 1/1:  mini-batch 938/1896:  Train loss: 0.0007854301366023719  Test loss: 0.0047769201919436455 \n",
      "Epoch: 1/1:  mini-batch 939/1896:  Train loss: 0.0017223721370100975  Test loss: 0.004798890091478825 \n",
      "Epoch: 1/1:  mini-batch 940/1896:  Train loss: 0.0036819633096456528  Test loss: 0.004842708818614483 \n",
      "Epoch: 1/1:  mini-batch 941/1896:  Train loss: 0.00328050390817225  Test loss: 0.004853640217334032 \n",
      "Epoch: 1/1:  mini-batch 942/1896:  Train loss: 0.004200184252113104  Test loss: 0.004809732083231211 \n",
      "Epoch: 1/1:  mini-batch 943/1896:  Train loss: 0.003704447066411376  Test loss: 0.004790971986949444 \n",
      "Epoch: 1/1:  mini-batch 944/1896:  Train loss: 0.0004406449443195015  Test loss: 0.004775294568389654 \n",
      "Epoch: 1/1:  mini-batch 945/1896:  Train loss: 0.0015868657501414418  Test loss: 0.00477227009832859 \n",
      "Epoch: 1/1:  mini-batch 946/1896:  Train loss: 0.0012057905551046133  Test loss: 0.004805700853466988 \n",
      "Epoch: 1/1:  mini-batch 947/1896:  Train loss: 0.004099706187844276  Test loss: 0.004883169662207365 \n",
      "Epoch: 1/1:  mini-batch 948/1896:  Train loss: 0.005693951155990362  Test loss: 0.00488086836412549 \n",
      "Epoch: 1/1:  mini-batch 949/1896:  Train loss: 0.002460807329043746  Test loss: 0.004824484698474407 \n",
      "Epoch: 1/1:  mini-batch 950/1896:  Train loss: 0.004236093256622553  Test loss: 0.004785340279340744 \n",
      "Epoch: 1/1:  mini-batch 951/1896:  Train loss: 0.0038985172286629677  Test loss: 0.004770204424858093 \n",
      "Epoch: 1/1:  mini-batch 952/1896:  Train loss: 0.006161123514175415  Test loss: 0.004837577231228352 \n",
      "Epoch: 1/1:  mini-batch 953/1896:  Train loss: 0.006286075338721275  Test loss: 0.004994444083422422 \n",
      "Epoch: 1/1:  mini-batch 954/1896:  Train loss: 0.003065527882426977  Test loss: 0.00502476841211319 \n",
      "Epoch: 1/1:  mini-batch 955/1896:  Train loss: 0.003936713095754385  Test loss: 0.0048866281285882 \n",
      "Epoch: 1/1:  mini-batch 956/1896:  Train loss: 0.0071299318224191666  Test loss: 0.004783862270414829 \n",
      "Epoch: 1/1:  mini-batch 957/1896:  Train loss: 0.0007589407032355666  Test loss: 0.004838773049414158 \n",
      "Epoch: 1/1:  mini-batch 958/1896:  Train loss: 0.0032207826152443886  Test loss: 0.004931881092488766 \n",
      "Epoch: 1/1:  mini-batch 959/1896:  Train loss: 0.004761766642332077  Test loss: 0.004962342791259289 \n",
      "Epoch: 1/1:  mini-batch 960/1896:  Train loss: 0.002665570704266429  Test loss: 0.004928690381348133 \n",
      "Epoch: 1/1:  mini-batch 961/1896:  Train loss: 0.003974331542849541  Test loss: 0.004826986696571112 \n",
      "Epoch: 1/1:  mini-batch 962/1896:  Train loss: 0.0039245435036718845  Test loss: 0.004776727873831987 \n",
      "Epoch: 1/1:  mini-batch 963/1896:  Train loss: 0.007668313570320606  Test loss: 0.004773546010255814 \n",
      "Epoch: 1/1:  mini-batch 964/1896:  Train loss: 0.000563340203370899  Test loss: 0.004787358921021223 \n",
      "Epoch: 1/1:  mini-batch 965/1896:  Train loss: 0.000562215456739068  Test loss: 0.004788140766322613 \n",
      "Epoch: 1/1:  mini-batch 966/1896:  Train loss: 0.0034096415620297194  Test loss: 0.0047817775048315525 \n",
      "Epoch: 1/1:  mini-batch 967/1896:  Train loss: 0.002269536256790161  Test loss: 0.004791649524122477 \n",
      "Epoch: 1/1:  mini-batch 968/1896:  Train loss: 0.0012078798608854413  Test loss: 0.004806058015674353 \n",
      "Epoch: 1/1:  mini-batch 969/1896:  Train loss: 0.000918625621125102  Test loss: 0.0048373788595199585 \n",
      "Epoch: 1/1:  mini-batch 970/1896:  Train loss: 0.0007027833489701152  Test loss: 0.004858048632740974 \n",
      "Epoch: 1/1:  mini-batch 971/1896:  Train loss: 0.0012765851570293307  Test loss: 0.004838916473090649 \n",
      "Epoch: 1/1:  mini-batch 972/1896:  Train loss: 0.0042082760483026505  Test loss: 0.004799259826540947 \n",
      "Epoch: 1/1:  mini-batch 973/1896:  Train loss: 0.0009660520008765161  Test loss: 0.0047702849842607975 \n",
      "Epoch: 1/1:  mini-batch 974/1896:  Train loss: 0.003571482375264168  Test loss: 0.0047712428495287895 \n",
      "Epoch: 1/1:  mini-batch 975/1896:  Train loss: 0.002446882426738739  Test loss: 0.004815298598259687 \n",
      "Epoch: 1/1:  mini-batch 976/1896:  Train loss: 0.0013415263965725899  Test loss: 0.004847478121519089 \n",
      "Epoch: 1/1:  mini-batch 977/1896:  Train loss: 0.003077136352658272  Test loss: 0.004798478912562132 \n",
      "Epoch: 1/1:  mini-batch 978/1896:  Train loss: 0.0028069731779396534  Test loss: 0.004767044447362423 \n",
      "Epoch: 1/1:  mini-batch 979/1896:  Train loss: 0.0015286743873730302  Test loss: 0.004822458606213331 \n",
      "Epoch: 1/1:  mini-batch 980/1896:  Train loss: 0.000747410929761827  Test loss: 0.004931256175041199 \n",
      "Epoch: 1/1:  mini-batch 981/1896:  Train loss: 0.0041078925132751465  Test loss: 0.004938721656799316 \n",
      "Epoch: 1/1:  mini-batch 982/1896:  Train loss: 0.001967889489606023  Test loss: 0.004848182201385498 \n",
      "Epoch: 1/1:  mini-batch 983/1896:  Train loss: 0.0020771317649632692  Test loss: 0.004781734198331833 \n",
      "Epoch: 1/1:  mini-batch 984/1896:  Train loss: 0.0071629430167376995  Test loss: 0.0048622433096170425 \n",
      "Epoch: 1/1:  mini-batch 985/1896:  Train loss: 0.002777497749775648  Test loss: 0.004965954460203648 \n",
      "Epoch: 1/1:  mini-batch 986/1896:  Train loss: 0.0029007624834775925  Test loss: 0.004982509650290012 \n",
      "Epoch: 1/1:  mini-batch 987/1896:  Train loss: 0.0025759930722415447  Test loss: 0.004841259680688381 \n",
      "Epoch: 1/1:  mini-batch 988/1896:  Train loss: 0.000748903607018292  Test loss: 0.004766755737364292 \n",
      "Epoch: 1/1:  mini-batch 989/1896:  Train loss: 0.000701182521879673  Test loss: 0.004861215129494667 \n",
      "Epoch: 1/1:  mini-batch 990/1896:  Train loss: 0.001981616485863924  Test loss: 0.005019683390855789 \n",
      "Epoch: 1/1:  mini-batch 991/1896:  Train loss: 0.0016246667364612222  Test loss: 0.005097445100545883 \n",
      "Epoch: 1/1:  mini-batch 992/1896:  Train loss: 0.007924892008304596  Test loss: 0.004976863507181406 \n",
      "Epoch: 1/1:  mini-batch 993/1896:  Train loss: 0.001191045274026692  Test loss: 0.004814545623958111 \n",
      "Epoch: 1/1:  mini-batch 994/1896:  Train loss: 0.006916391663253307  Test loss: 0.0047916229814291 \n",
      "Epoch: 1/1:  mini-batch 995/1896:  Train loss: 0.00502679031342268  Test loss: 0.004844863899052143 \n",
      "Epoch: 1/1:  mini-batch 996/1896:  Train loss: 0.0006238784990273416  Test loss: 0.00486262422055006 \n",
      "Epoch: 1/1:  mini-batch 997/1896:  Train loss: 0.0051849642768502235  Test loss: 0.004822991322726011 \n",
      "Epoch: 1/1:  mini-batch 998/1896:  Train loss: 0.005259456112980843  Test loss: 0.0047785937786102295 \n",
      "Epoch: 1/1:  mini-batch 999/1896:  Train loss: 0.0005762393120676279  Test loss: 0.00478135934099555 \n",
      "Epoch: 1/1:  mini-batch 1000/1896:  Train loss: 0.003928464371711016  Test loss: 0.004801962058991194 \n",
      "Epoch: 1/1:  mini-batch 1001/1896:  Train loss: 0.005854337941855192  Test loss: 0.004793747328221798 \n",
      "Epoch: 1/1:  mini-batch 1002/1896:  Train loss: 0.0007585666608065367  Test loss: 0.0047989157028496265 \n",
      "Epoch: 1/1:  mini-batch 1003/1896:  Train loss: 0.0020402136724442244  Test loss: 0.004786288365721703 \n",
      "Epoch: 1/1:  mini-batch 1004/1896:  Train loss: 0.0007725186296738684  Test loss: 0.004769238643348217 \n",
      "Epoch: 1/1:  mini-batch 1005/1896:  Train loss: 0.003014775225892663  Test loss: 0.004762063734233379 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1006/1896:  Train loss: 0.002110507572069764  Test loss: 0.004774036817252636 \n",
      "Epoch: 1/1:  mini-batch 1007/1896:  Train loss: 0.003617475973442197  Test loss: 0.004796084947884083 \n",
      "Epoch: 1/1:  mini-batch 1008/1896:  Train loss: 0.004362235311418772  Test loss: 0.0048361048102378845 \n",
      "Epoch: 1/1:  mini-batch 1009/1896:  Train loss: 0.005034221336245537  Test loss: 0.004891451448202133 \n",
      "Epoch: 1/1:  mini-batch 1010/1896:  Train loss: 0.0029928358271718025  Test loss: 0.004882228560745716 \n",
      "Epoch: 1/1:  mini-batch 1011/1896:  Train loss: 0.0010944443056359887  Test loss: 0.0048416187055408955 \n",
      "Epoch: 1/1:  mini-batch 1012/1896:  Train loss: 0.003493984928354621  Test loss: 0.004786372184753418 \n",
      "Epoch: 1/1:  mini-batch 1013/1896:  Train loss: 0.0018022727454081178  Test loss: 0.004820847883820534 \n",
      "Epoch: 1/1:  mini-batch 1014/1896:  Train loss: 0.0032019440550357103  Test loss: 0.004851837642490864 \n",
      "Epoch: 1/1:  mini-batch 1015/1896:  Train loss: 0.005060444585978985  Test loss: 0.0048500788398087025 \n",
      "Epoch: 1/1:  mini-batch 1016/1896:  Train loss: 0.0030164753552526236  Test loss: 0.004806268494576216 \n",
      "Epoch: 1/1:  mini-batch 1017/1896:  Train loss: 0.003968456760048866  Test loss: 0.004780926741659641 \n",
      "Epoch: 1/1:  mini-batch 1018/1896:  Train loss: 0.0058904411271214485  Test loss: 0.0048035564832389355 \n",
      "Epoch: 1/1:  mini-batch 1019/1896:  Train loss: 0.0033000933472067118  Test loss: 0.004850868135690689 \n",
      "Epoch: 1/1:  mini-batch 1020/1896:  Train loss: 0.009345177561044693  Test loss: 0.004801170434802771 \n",
      "Epoch: 1/1:  mini-batch 1021/1896:  Train loss: 0.0017981772543862462  Test loss: 0.004798694979399443 \n",
      "Epoch: 1/1:  mini-batch 1022/1896:  Train loss: 0.000971555127762258  Test loss: 0.004825244192034006 \n",
      "Epoch: 1/1:  mini-batch 1023/1896:  Train loss: 0.0025279810652136803  Test loss: 0.0048290640115737915 \n",
      "Epoch: 1/1:  mini-batch 1024/1896:  Train loss: 0.0011605164036154747  Test loss: 0.004798182286322117 \n",
      "Epoch: 1/1:  mini-batch 1025/1896:  Train loss: 0.0036516389809548855  Test loss: 0.00477431807667017 \n",
      "Epoch: 1/1:  mini-batch 1026/1896:  Train loss: 0.00532518932595849  Test loss: 0.0048302775248885155 \n",
      "Epoch: 1/1:  mini-batch 1027/1896:  Train loss: 0.005109811667352915  Test loss: 0.004872493911534548 \n",
      "Epoch: 1/1:  mini-batch 1028/1896:  Train loss: 0.004506667144596577  Test loss: 0.0049074674025177956 \n",
      "Epoch: 1/1:  mini-batch 1029/1896:  Train loss: 0.0014036037027835846  Test loss: 0.004893835633993149 \n",
      "Epoch: 1/1:  mini-batch 1030/1896:  Train loss: 0.00399175425991416  Test loss: 0.004825632553547621 \n",
      "Epoch: 1/1:  mini-batch 1031/1896:  Train loss: 0.0029733460396528244  Test loss: 0.00480633182451129 \n",
      "Epoch: 1/1:  mini-batch 1032/1896:  Train loss: 0.0039995331317186356  Test loss: 0.004830098710954189 \n",
      "Epoch: 1/1:  mini-batch 1033/1896:  Train loss: 0.007300508674234152  Test loss: 0.0048616789281368256 \n",
      "Epoch: 1/1:  mini-batch 1034/1896:  Train loss: 0.0038411403074860573  Test loss: 0.004830479621887207 \n",
      "Epoch: 1/1:  mini-batch 1035/1896:  Train loss: 0.004185613244771957  Test loss: 0.004812311381101608 \n",
      "Epoch: 1/1:  mini-batch 1036/1896:  Train loss: 0.0054433997720479965  Test loss: 0.004816437140107155 \n",
      "Epoch: 1/1:  mini-batch 1037/1896:  Train loss: 0.001415409380570054  Test loss: 0.00486663356423378 \n",
      "Epoch: 1/1:  mini-batch 1038/1896:  Train loss: 0.003844986902549863  Test loss: 0.004909000359475613 \n",
      "Epoch: 1/1:  mini-batch 1039/1896:  Train loss: 0.006091506220400333  Test loss: 0.004850085824728012 \n",
      "Epoch: 1/1:  mini-batch 1040/1896:  Train loss: 0.002724621444940567  Test loss: 0.004797622561454773 \n",
      "Epoch: 1/1:  mini-batch 1041/1896:  Train loss: 0.0024744991678744555  Test loss: 0.004792164079844952 \n",
      "Epoch: 1/1:  mini-batch 1042/1896:  Train loss: 0.006799466907978058  Test loss: 0.004808748606592417 \n",
      "Epoch: 1/1:  mini-batch 1043/1896:  Train loss: 0.0032759588211774826  Test loss: 0.004789927043020725 \n",
      "Epoch: 1/1:  mini-batch 1044/1896:  Train loss: 0.00494734151288867  Test loss: 0.004769765306264162 \n",
      "Epoch: 1/1:  mini-batch 1045/1896:  Train loss: 0.004936781711876392  Test loss: 0.0048355176113545895 \n",
      "Epoch: 1/1:  mini-batch 1046/1896:  Train loss: 0.003841724246740341  Test loss: 0.004874873906373978 \n",
      "Epoch: 1/1:  mini-batch 1047/1896:  Train loss: 0.006130619440227747  Test loss: 0.004810899496078491 \n",
      "Epoch: 1/1:  mini-batch 1048/1896:  Train loss: 0.0023505878634750843  Test loss: 0.004769851453602314 \n",
      "Epoch: 1/1:  mini-batch 1049/1896:  Train loss: 0.0016529192216694355  Test loss: 0.004768081940710545 \n",
      "Epoch: 1/1:  mini-batch 1050/1896:  Train loss: 0.004058061633259058  Test loss: 0.004789318423718214 \n",
      "Epoch: 1/1:  mini-batch 1051/1896:  Train loss: 0.006544384174048901  Test loss: 0.004803011193871498 \n",
      "Epoch: 1/1:  mini-batch 1052/1896:  Train loss: 0.004899058490991592  Test loss: 0.0048014381900429726 \n",
      "Epoch: 1/1:  mini-batch 1053/1896:  Train loss: 0.003602611133828759  Test loss: 0.004773388616740704 \n",
      "Epoch: 1/1:  mini-batch 1054/1896:  Train loss: 0.0015758643858134747  Test loss: 0.004774844273924828 \n",
      "Epoch: 1/1:  mini-batch 1055/1896:  Train loss: 0.0024305947590619326  Test loss: 0.004835745319724083 \n",
      "Epoch: 1/1:  mini-batch 1056/1896:  Train loss: 0.00589574733749032  Test loss: 0.005039719864726067 \n",
      "Epoch: 1/1:  mini-batch 1057/1896:  Train loss: 0.003090022364631295  Test loss: 0.005076245404779911 \n",
      "Epoch: 1/1:  mini-batch 1058/1896:  Train loss: 0.0049228970892727375  Test loss: 0.004873475059866905 \n",
      "Epoch: 1/1:  mini-batch 1059/1896:  Train loss: 0.0016910425620153546  Test loss: 0.004771694540977478 \n",
      "Epoch: 1/1:  mini-batch 1060/1896:  Train loss: 0.0036978491116315126  Test loss: 0.004890534095466137 \n",
      "Epoch: 1/1:  mini-batch 1061/1896:  Train loss: 0.007104507181793451  Test loss: 0.0050541432574391365 \n",
      "Epoch: 1/1:  mini-batch 1062/1896:  Train loss: 0.008404325693845749  Test loss: 0.004974904004484415 \n",
      "Epoch: 1/1:  mini-batch 1063/1896:  Train loss: 0.002258661901578307  Test loss: 0.004870294593274593 \n",
      "Epoch: 1/1:  mini-batch 1064/1896:  Train loss: 0.004489128943532705  Test loss: 0.004841257818043232 \n",
      "Epoch: 1/1:  mini-batch 1065/1896:  Train loss: 0.0037694405764341354  Test loss: 0.004916395992040634 \n",
      "Epoch: 1/1:  mini-batch 1066/1896:  Train loss: 0.0011967953760176897  Test loss: 0.005020598880946636 \n",
      "Epoch: 1/1:  mini-batch 1067/1896:  Train loss: 0.004852336831390858  Test loss: 0.005076320841908455 \n",
      "Epoch: 1/1:  mini-batch 1068/1896:  Train loss: 0.004736519418656826  Test loss: 0.004971420858055353 \n",
      "Epoch: 1/1:  mini-batch 1069/1896:  Train loss: 0.001959107583388686  Test loss: 0.004847779870033264 \n",
      "Epoch: 1/1:  mini-batch 1070/1896:  Train loss: 0.002989209722727537  Test loss: 0.004781365394592285 \n",
      "Epoch: 1/1:  mini-batch 1071/1896:  Train loss: 0.0014201048761606216  Test loss: 0.004773581400513649 \n",
      "Epoch: 1/1:  mini-batch 1072/1896:  Train loss: 0.0030963446479290724  Test loss: 0.004811668768525124 \n",
      "Epoch: 1/1:  mini-batch 1073/1896:  Train loss: 0.004344329237937927  Test loss: 0.004852971062064171 \n",
      "Epoch: 1/1:  mini-batch 1074/1896:  Train loss: 0.00515696220099926  Test loss: 0.004825422540307045 \n",
      "Epoch: 1/1:  mini-batch 1075/1896:  Train loss: 0.003261631354689598  Test loss: 0.004765334539115429 \n",
      "Epoch: 1/1:  mini-batch 1076/1896:  Train loss: 0.0014064919669181108  Test loss: 0.004781658761203289 \n",
      "Epoch: 1/1:  mini-batch 1077/1896:  Train loss: 0.002891013864427805  Test loss: 0.004810047335922718 \n",
      "Epoch: 1/1:  mini-batch 1078/1896:  Train loss: 0.0041098869405686855  Test loss: 0.0047810813412070274 \n",
      "Epoch: 1/1:  mini-batch 1079/1896:  Train loss: 0.000280995067441836  Test loss: 0.004769074730575085 \n",
      "Epoch: 1/1:  mini-batch 1080/1896:  Train loss: 0.004001764114946127  Test loss: 0.004790949169546366 \n",
      "Epoch: 1/1:  mini-batch 1081/1896:  Train loss: 0.0020093200728297234  Test loss: 0.004807987250387669 \n",
      "Epoch: 1/1:  mini-batch 1082/1896:  Train loss: 0.004228220321238041  Test loss: 0.004812643863260746 \n",
      "Epoch: 1/1:  mini-batch 1083/1896:  Train loss: 0.0015924437902867794  Test loss: 0.004787101410329342 \n",
      "Epoch: 1/1:  mini-batch 1084/1896:  Train loss: 0.001093460712581873  Test loss: 0.004762611351907253 \n",
      "Epoch: 1/1:  mini-batch 1085/1896:  Train loss: 0.005136401392519474  Test loss: 0.004771005362272263 \n",
      "Epoch: 1/1:  mini-batch 1086/1896:  Train loss: 0.0009853239171206951  Test loss: 0.004792514257133007 \n",
      "Epoch: 1/1:  mini-batch 1087/1896:  Train loss: 0.004665229003876448  Test loss: 0.004812699742615223 \n",
      "Epoch: 1/1:  mini-batch 1088/1896:  Train loss: 0.0011866494314745069  Test loss: 0.004800301045179367 \n",
      "Epoch: 1/1:  mini-batch 1089/1896:  Train loss: 0.0026500665117055178  Test loss: 0.004779445938766003 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1090/1896:  Train loss: 0.002281543333083391  Test loss: 0.004772821441292763 \n",
      "Epoch: 1/1:  mini-batch 1091/1896:  Train loss: 0.007932620123028755  Test loss: 0.004775054287165403 \n",
      "Epoch: 1/1:  mini-batch 1092/1896:  Train loss: 0.00475437194108963  Test loss: 0.004765760153532028 \n",
      "Epoch: 1/1:  mini-batch 1093/1896:  Train loss: 0.007465685717761517  Test loss: 0.0047619277611374855 \n",
      "Epoch: 1/1:  mini-batch 1094/1896:  Train loss: 0.003134277882054448  Test loss: 0.004766836762428284 \n",
      "Epoch: 1/1:  mini-batch 1095/1896:  Train loss: 0.00206176470965147  Test loss: 0.004763654433190823 \n",
      "Epoch: 1/1:  mini-batch 1096/1896:  Train loss: 0.0022642179392278194  Test loss: 0.004762914031744003 \n",
      "Epoch: 1/1:  mini-batch 1097/1896:  Train loss: 0.004191831685602665  Test loss: 0.004761535674333572 \n",
      "Epoch: 1/1:  mini-batch 1098/1896:  Train loss: 0.0020107196178287268  Test loss: 0.004769912920892239 \n",
      "Epoch: 1/1:  mini-batch 1099/1896:  Train loss: 0.0033955485559999943  Test loss: 0.004819634836167097 \n",
      "Epoch: 1/1:  mini-batch 1100/1896:  Train loss: 0.005087143741548061  Test loss: 0.004930359777063131 \n",
      "Epoch: 1/1:  mini-batch 1101/1896:  Train loss: 0.0018015961395576596  Test loss: 0.004942134488373995 \n",
      "Epoch: 1/1:  mini-batch 1102/1896:  Train loss: 0.005058031529188156  Test loss: 0.004814219661056995 \n",
      "Epoch: 1/1:  mini-batch 1103/1896:  Train loss: 0.0026416070759296417  Test loss: 0.004761457443237305 \n",
      "Epoch: 1/1:  mini-batch 1104/1896:  Train loss: 0.0028024266939610243  Test loss: 0.004808470606803894 \n",
      "Epoch: 1/1:  mini-batch 1105/1896:  Train loss: 0.004647630266845226  Test loss: 0.004839598201215267 \n",
      "Epoch: 1/1:  mini-batch 1106/1896:  Train loss: 0.0006536761065945029  Test loss: 0.0048151761293411255 \n",
      "Epoch: 1/1:  mini-batch 1107/1896:  Train loss: 0.007567457854747772  Test loss: 0.004792028106749058 \n",
      "Epoch: 1/1:  mini-batch 1108/1896:  Train loss: 0.003989038988947868  Test loss: 0.004804006312042475 \n",
      "Epoch: 1/1:  mini-batch 1109/1896:  Train loss: 0.0029840776696801186  Test loss: 0.004790734499692917 \n",
      "Epoch: 1/1:  mini-batch 1110/1896:  Train loss: 0.003393269143998623  Test loss: 0.004815219901502132 \n",
      "Epoch: 1/1:  mini-batch 1111/1896:  Train loss: 0.004562230780720711  Test loss: 0.0048094242811203 \n",
      "Epoch: 1/1:  mini-batch 1112/1896:  Train loss: 0.005340841598808765  Test loss: 0.004768021404743195 \n",
      "Epoch: 1/1:  mini-batch 1113/1896:  Train loss: 0.004750767722725868  Test loss: 0.004781382158398628 \n",
      "Epoch: 1/1:  mini-batch 1114/1896:  Train loss: 0.000993387307971716  Test loss: 0.0048093791119754314 \n",
      "Epoch: 1/1:  mini-batch 1115/1896:  Train loss: 0.0025517805479466915  Test loss: 0.004816778004169464 \n",
      "Epoch: 1/1:  mini-batch 1116/1896:  Train loss: 0.004309832118451595  Test loss: 0.004809689242392778 \n",
      "Epoch: 1/1:  mini-batch 1117/1896:  Train loss: 0.001874203560873866  Test loss: 0.004774302709847689 \n",
      "Epoch: 1/1:  mini-batch 1118/1896:  Train loss: 0.00608071917667985  Test loss: 0.004779261536896229 \n",
      "Epoch: 1/1:  mini-batch 1119/1896:  Train loss: 0.009281221777200699  Test loss: 0.0049308063462376595 \n",
      "Epoch: 1/1:  mini-batch 1120/1896:  Train loss: 0.0026891781017184258  Test loss: 0.005023485980927944 \n",
      "Epoch: 1/1:  mini-batch 1121/1896:  Train loss: 0.002974005648866296  Test loss: 0.005023003090173006 \n",
      "Epoch: 1/1:  mini-batch 1122/1896:  Train loss: 0.003800856415182352  Test loss: 0.004858235828578472 \n",
      "Epoch: 1/1:  mini-batch 1123/1896:  Train loss: 0.005139823071658611  Test loss: 0.004798037465661764 \n",
      "Epoch: 1/1:  mini-batch 1124/1896:  Train loss: 0.004060088656842709  Test loss: 0.004883459769189358 \n",
      "Epoch: 1/1:  mini-batch 1125/1896:  Train loss: 0.0027863027062267065  Test loss: 0.00487899174913764 \n",
      "Epoch: 1/1:  mini-batch 1126/1896:  Train loss: 0.0023700527381151915  Test loss: 0.004813234321773052 \n",
      "Epoch: 1/1:  mini-batch 1127/1896:  Train loss: 0.005816061981022358  Test loss: 0.004805666394531727 \n",
      "Epoch: 1/1:  mini-batch 1128/1896:  Train loss: 0.005013346206396818  Test loss: 0.004793060012161732 \n",
      "Epoch: 1/1:  mini-batch 1129/1896:  Train loss: 0.005833233240991831  Test loss: 0.004790988750755787 \n",
      "Epoch: 1/1:  mini-batch 1130/1896:  Train loss: 0.0016481209313496947  Test loss: 0.004853937774896622 \n",
      "Epoch: 1/1:  mini-batch 1131/1896:  Train loss: 0.00389682874083519  Test loss: 0.004919347353279591 \n",
      "Epoch: 1/1:  mini-batch 1132/1896:  Train loss: 0.0026100410614162683  Test loss: 0.004896093625575304 \n",
      "Epoch: 1/1:  mini-batch 1133/1896:  Train loss: 0.00470614992082119  Test loss: 0.004811813123524189 \n",
      "Epoch: 1/1:  mini-batch 1134/1896:  Train loss: 0.006412549875676632  Test loss: 0.00478778500109911 \n",
      "Epoch: 1/1:  mini-batch 1135/1896:  Train loss: 0.006029216572642326  Test loss: 0.004944478627294302 \n",
      "Epoch: 1/1:  mini-batch 1136/1896:  Train loss: 0.0075797755271196365  Test loss: 0.005043184384703636 \n",
      "Epoch: 1/1:  mini-batch 1137/1896:  Train loss: 0.001049622194841504  Test loss: 0.004904561676084995 \n",
      "Epoch: 1/1:  mini-batch 1138/1896:  Train loss: 0.00963226892054081  Test loss: 0.004762222059071064 \n",
      "Epoch: 1/1:  mini-batch 1139/1896:  Train loss: 0.0019335401011630893  Test loss: 0.004966164007782936 \n",
      "Epoch: 1/1:  mini-batch 1140/1896:  Train loss: 0.0008523900178261101  Test loss: 0.005248191300779581 \n",
      "Epoch: 1/1:  mini-batch 1141/1896:  Train loss: 0.0015654662856832147  Test loss: 0.0051846690475940704 \n",
      "Epoch: 1/1:  mini-batch 1142/1896:  Train loss: 0.0021775411441922188  Test loss: 0.004941706545650959 \n",
      "Epoch: 1/1:  mini-batch 1143/1896:  Train loss: 0.001258466742001474  Test loss: 0.0047675492241978645 \n",
      "Epoch: 1/1:  mini-batch 1144/1896:  Train loss: 0.0031750472262501717  Test loss: 0.004830453544855118 \n",
      "Epoch: 1/1:  mini-batch 1145/1896:  Train loss: 0.006468710023909807  Test loss: 0.005022016819566488 \n",
      "Epoch: 1/1:  mini-batch 1146/1896:  Train loss: 0.004621634725481272  Test loss: 0.004988637287169695 \n",
      "Epoch: 1/1:  mini-batch 1147/1896:  Train loss: 0.003973148763179779  Test loss: 0.004791738465428352 \n",
      "Epoch: 1/1:  mini-batch 1148/1896:  Train loss: 0.0021144538186490536  Test loss: 0.00478376867249608 \n",
      "Epoch: 1/1:  mini-batch 1149/1896:  Train loss: 0.005986350122839212  Test loss: 0.004866913426667452 \n",
      "Epoch: 1/1:  mini-batch 1150/1896:  Train loss: 0.0013159499503672123  Test loss: 0.0049173710867762566 \n",
      "Epoch: 1/1:  mini-batch 1151/1896:  Train loss: 0.004379850346595049  Test loss: 0.0048584709875285625 \n",
      "Epoch: 1/1:  mini-batch 1152/1896:  Train loss: 0.004256247077137232  Test loss: 0.004793204832822084 \n",
      "Epoch: 1/1:  mini-batch 1153/1896:  Train loss: 0.0029984230641275644  Test loss: 0.004782825242727995 \n",
      "Epoch: 1/1:  mini-batch 1154/1896:  Train loss: 0.000964019272942096  Test loss: 0.004811879247426987 \n",
      "Epoch: 1/1:  mini-batch 1155/1896:  Train loss: 0.003797770943492651  Test loss: 0.004796778783202171 \n",
      "Epoch: 1/1:  mini-batch 1156/1896:  Train loss: 0.0027372289914637804  Test loss: 0.004767145030200481 \n",
      "Epoch: 1/1:  mini-batch 1157/1896:  Train loss: 0.0036745190154761076  Test loss: 0.0047795530408620834 \n",
      "Epoch: 1/1:  mini-batch 1158/1896:  Train loss: 0.0026304996572434902  Test loss: 0.004855741746723652 \n",
      "Epoch: 1/1:  mini-batch 1159/1896:  Train loss: 0.0009696344495750964  Test loss: 0.004898514598608017 \n",
      "Epoch: 1/1:  mini-batch 1160/1896:  Train loss: 0.0018374392529949546  Test loss: 0.004853764083236456 \n",
      "Epoch: 1/1:  mini-batch 1161/1896:  Train loss: 0.0009291380411013961  Test loss: 0.004815368913114071 \n",
      "Epoch: 1/1:  mini-batch 1162/1896:  Train loss: 0.005198491737246513  Test loss: 0.004780587274581194 \n",
      "Epoch: 1/1:  mini-batch 1163/1896:  Train loss: 0.002728534396737814  Test loss: 0.004822677932679653 \n",
      "Epoch: 1/1:  mini-batch 1164/1896:  Train loss: 0.004319231957197189  Test loss: 0.004906000569462776 \n",
      "Epoch: 1/1:  mini-batch 1165/1896:  Train loss: 0.0033915098756551743  Test loss: 0.004896026104688644 \n",
      "Epoch: 1/1:  mini-batch 1166/1896:  Train loss: 0.004639111924916506  Test loss: 0.0048440853133797646 \n",
      "Epoch: 1/1:  mini-batch 1167/1896:  Train loss: 0.006130032241344452  Test loss: 0.004793317522853613 \n",
      "Epoch: 1/1:  mini-batch 1168/1896:  Train loss: 0.003933057654649019  Test loss: 0.004867258481681347 \n",
      "Epoch: 1/1:  mini-batch 1169/1896:  Train loss: 0.0017214012332260609  Test loss: 0.005058908835053444 \n",
      "Epoch: 1/1:  mini-batch 1170/1896:  Train loss: 0.0028110535349696875  Test loss: 0.005077812820672989 \n",
      "Epoch: 1/1:  mini-batch 1171/1896:  Train loss: 0.002746403682976961  Test loss: 0.004905473440885544 \n",
      "Epoch: 1/1:  mini-batch 1172/1896:  Train loss: 0.0004870975390076637  Test loss: 0.004783939570188522 \n",
      "Epoch: 1/1:  mini-batch 1173/1896:  Train loss: 0.0024585016071796417  Test loss: 0.0048052058555185795 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1174/1896:  Train loss: 0.0017513396451249719  Test loss: 0.0048939078114926815 \n",
      "Epoch: 1/1:  mini-batch 1175/1896:  Train loss: 0.0010725497268140316  Test loss: 0.0048728762194514275 \n",
      "Epoch: 1/1:  mini-batch 1176/1896:  Train loss: 0.004359821788966656  Test loss: 0.004764022305607796 \n",
      "Epoch: 1/1:  mini-batch 1177/1896:  Train loss: 0.002892295364290476  Test loss: 0.004846639931201935 \n",
      "Epoch: 1/1:  mini-batch 1178/1896:  Train loss: 0.005334814544767141  Test loss: 0.004944339394569397 \n",
      "Epoch: 1/1:  mini-batch 1179/1896:  Train loss: 0.0020846757106482983  Test loss: 0.004899138584733009 \n",
      "Epoch: 1/1:  mini-batch 1180/1896:  Train loss: 0.00421679113060236  Test loss: 0.004888742230832577 \n",
      "Epoch: 1/1:  mini-batch 1181/1896:  Train loss: 0.0027653309516608715  Test loss: 0.004861210938543081 \n",
      "Epoch: 1/1:  mini-batch 1182/1896:  Train loss: 0.004638033919036388  Test loss: 0.0048373048193752766 \n",
      "Epoch: 1/1:  mini-batch 1183/1896:  Train loss: 0.006666806526482105  Test loss: 0.004806568846106529 \n",
      "Epoch: 1/1:  mini-batch 1184/1896:  Train loss: 0.00496926624327898  Test loss: 0.004856357350945473 \n",
      "Epoch: 1/1:  mini-batch 1185/1896:  Train loss: 0.007033794652670622  Test loss: 0.004817334935069084 \n",
      "Epoch: 1/1:  mini-batch 1186/1896:  Train loss: 0.001132866949774325  Test loss: 0.004781391471624374 \n",
      "Epoch: 1/1:  mini-batch 1187/1896:  Train loss: 0.006086691282689571  Test loss: 0.004767480306327343 \n",
      "Epoch: 1/1:  mini-batch 1188/1896:  Train loss: 0.006453180685639381  Test loss: 0.004870594013482332 \n",
      "Epoch: 1/1:  mini-batch 1189/1896:  Train loss: 0.004346110858023167  Test loss: 0.005109566263854504 \n",
      "Epoch: 1/1:  mini-batch 1190/1896:  Train loss: 0.0034713565837591887  Test loss: 0.005085865966975689 \n",
      "Epoch: 1/1:  mini-batch 1191/1896:  Train loss: 0.0010606070281937718  Test loss: 0.004885623697191477 \n",
      "Epoch: 1/1:  mini-batch 1192/1896:  Train loss: 0.007006523199379444  Test loss: 0.004766095895320177 \n",
      "Epoch: 1/1:  mini-batch 1193/1896:  Train loss: 0.0038246174808591604  Test loss: 0.004889465868473053 \n",
      "Epoch: 1/1:  mini-batch 1194/1896:  Train loss: 0.0016899204347282648  Test loss: 0.004943294450640678 \n",
      "Epoch: 1/1:  mini-batch 1195/1896:  Train loss: 0.0049158488400280476  Test loss: 0.004933939781039953 \n",
      "Epoch: 1/1:  mini-batch 1196/1896:  Train loss: 0.0015316391363739967  Test loss: 0.0048553310334682465 \n",
      "Epoch: 1/1:  mini-batch 1197/1896:  Train loss: 0.003374928142875433  Test loss: 0.004803920164704323 \n",
      "Epoch: 1/1:  mini-batch 1198/1896:  Train loss: 0.003107592463493347  Test loss: 0.004874199628829956 \n",
      "Epoch: 1/1:  mini-batch 1199/1896:  Train loss: 0.0021915961988270283  Test loss: 0.005077000241726637 \n",
      "Epoch: 1/1:  mini-batch 1200/1896:  Train loss: 0.004399863537400961  Test loss: 0.0051784515380859375 \n",
      "Epoch: 1/1:  mini-batch 1201/1896:  Train loss: 0.0016396630089730024  Test loss: 0.005082930903881788 \n",
      "Epoch: 1/1:  mini-batch 1202/1896:  Train loss: 0.007361925207078457  Test loss: 0.00479088956490159 \n",
      "Epoch: 1/1:  mini-batch 1203/1896:  Train loss: 0.0029166664462536573  Test loss: 0.004804139956831932 \n",
      "Epoch: 1/1:  mini-batch 1204/1896:  Train loss: 0.003730825614184141  Test loss: 0.004984833300113678 \n",
      "Epoch: 1/1:  mini-batch 1205/1896:  Train loss: 0.004073607735335827  Test loss: 0.004977311939001083 \n",
      "Epoch: 1/1:  mini-batch 1206/1896:  Train loss: 0.005097812972962856  Test loss: 0.004858248867094517 \n",
      "Epoch: 1/1:  mini-batch 1207/1896:  Train loss: 0.0010536023182794452  Test loss: 0.004767618142068386 \n",
      "Epoch: 1/1:  mini-batch 1208/1896:  Train loss: 0.0020586359314620495  Test loss: 0.004823652096092701 \n",
      "Epoch: 1/1:  mini-batch 1209/1896:  Train loss: 0.007776594255119562  Test loss: 0.0049232314340770245 \n",
      "Epoch: 1/1:  mini-batch 1210/1896:  Train loss: 0.0017643205355852842  Test loss: 0.004975081421434879 \n",
      "Epoch: 1/1:  mini-batch 1211/1896:  Train loss: 0.0033639557659626007  Test loss: 0.0049211797304451466 \n",
      "Epoch: 1/1:  mini-batch 1212/1896:  Train loss: 0.002009154763072729  Test loss: 0.004846083000302315 \n",
      "Epoch: 1/1:  mini-batch 1213/1896:  Train loss: 0.0011800720822066069  Test loss: 0.00478754099458456 \n",
      "Epoch: 1/1:  mini-batch 1214/1896:  Train loss: 0.0036352933384478092  Test loss: 0.004799674265086651 \n",
      "Epoch: 1/1:  mini-batch 1215/1896:  Train loss: 0.0032939238008111715  Test loss: 0.004865056369453669 \n",
      "Epoch: 1/1:  mini-batch 1216/1896:  Train loss: 0.003657399909570813  Test loss: 0.004913189448416233 \n",
      "Epoch: 1/1:  mini-batch 1217/1896:  Train loss: 0.004574642051011324  Test loss: 0.004823068156838417 \n",
      "Epoch: 1/1:  mini-batch 1218/1896:  Train loss: 0.0027151787653565407  Test loss: 0.004802778363227844 \n",
      "Epoch: 1/1:  mini-batch 1219/1896:  Train loss: 0.005798144731670618  Test loss: 0.00490149250254035 \n",
      "Epoch: 1/1:  mini-batch 1220/1896:  Train loss: 0.004127980209887028  Test loss: 0.00493466854095459 \n",
      "Epoch: 1/1:  mini-batch 1221/1896:  Train loss: 0.0036698023322969675  Test loss: 0.0048901778645813465 \n",
      "Epoch: 1/1:  mini-batch 1222/1896:  Train loss: 0.0010786871425807476  Test loss: 0.0048453956842422485 \n",
      "Epoch: 1/1:  mini-batch 1223/1896:  Train loss: 0.0024875355884432793  Test loss: 0.004790563136339188 \n",
      "Epoch: 1/1:  mini-batch 1224/1896:  Train loss: 0.004248416051268578  Test loss: 0.004772677086293697 \n",
      "Epoch: 1/1:  mini-batch 1225/1896:  Train loss: 0.003737518098205328  Test loss: 0.0048289792612195015 \n",
      "Epoch: 1/1:  mini-batch 1226/1896:  Train loss: 0.002169993007555604  Test loss: 0.004812576808035374 \n",
      "Epoch: 1/1:  mini-batch 1227/1896:  Train loss: 0.0036503802984952927  Test loss: 0.004781029187142849 \n",
      "Epoch: 1/1:  mini-batch 1228/1896:  Train loss: 0.0038255834951996803  Test loss: 0.004767381586134434 \n",
      "Epoch: 1/1:  mini-batch 1229/1896:  Train loss: 0.002600149717181921  Test loss: 0.004805037751793861 \n",
      "Epoch: 1/1:  mini-batch 1230/1896:  Train loss: 0.0038105749990791082  Test loss: 0.004837478511035442 \n",
      "Epoch: 1/1:  mini-batch 1231/1896:  Train loss: 0.002707589417695999  Test loss: 0.00480624521151185 \n",
      "Epoch: 1/1:  mini-batch 1232/1896:  Train loss: 0.003772942814975977  Test loss: 0.004789391532540321 \n",
      "Epoch: 1/1:  mini-batch 1233/1896:  Train loss: 0.0025491989217698574  Test loss: 0.004788121208548546 \n",
      "Epoch: 1/1:  mini-batch 1234/1896:  Train loss: 0.0023329800460487604  Test loss: 0.004797515459358692 \n",
      "Epoch: 1/1:  mini-batch 1235/1896:  Train loss: 0.0013748471392318606  Test loss: 0.004802266135811806 \n",
      "Epoch: 1/1:  mini-batch 1236/1896:  Train loss: 0.002782900584861636  Test loss: 0.004803407937288284 \n",
      "Epoch: 1/1:  mini-batch 1237/1896:  Train loss: 0.0025749828200787306  Test loss: 0.004830032587051392 \n",
      "Epoch: 1/1:  mini-batch 1238/1896:  Train loss: 0.0013233269564807415  Test loss: 0.004829861223697662 \n",
      "Epoch: 1/1:  mini-batch 1239/1896:  Train loss: 0.004920371808111668  Test loss: 0.0048150490038096905 \n",
      "Epoch: 1/1:  mini-batch 1240/1896:  Train loss: 0.004103758372366428  Test loss: 0.00480240024626255 \n",
      "Epoch: 1/1:  mini-batch 1241/1896:  Train loss: 0.0017686712089926004  Test loss: 0.004822540562599897 \n",
      "Epoch: 1/1:  mini-batch 1242/1896:  Train loss: 0.005067853722721338  Test loss: 0.004800558090209961 \n",
      "Epoch: 1/1:  mini-batch 1243/1896:  Train loss: 0.0009031168883666396  Test loss: 0.00478532537817955 \n",
      "Epoch: 1/1:  mini-batch 1244/1896:  Train loss: 0.002687025349587202  Test loss: 0.004808636382222176 \n",
      "Epoch: 1/1:  mini-batch 1245/1896:  Train loss: 0.0027813385240733624  Test loss: 0.004838813096284866 \n",
      "Epoch: 1/1:  mini-batch 1246/1896:  Train loss: 0.006280337926000357  Test loss: 0.004830586723983288 \n",
      "Epoch: 1/1:  mini-batch 1247/1896:  Train loss: 0.0019813021644949913  Test loss: 0.004786997102200985 \n",
      "Epoch: 1/1:  mini-batch 1248/1896:  Train loss: 0.0038050988223403692  Test loss: 0.00481406319886446 \n",
      "Epoch: 1/1:  mini-batch 1249/1896:  Train loss: 0.0015592132695019245  Test loss: 0.004886036738753319 \n",
      "Epoch: 1/1:  mini-batch 1250/1896:  Train loss: 0.0022898323368281126  Test loss: 0.00487575214356184 \n",
      "Epoch: 1/1:  mini-batch 1251/1896:  Train loss: 0.0038137449882924557  Test loss: 0.0048016877844929695 \n",
      "Epoch: 1/1:  mini-batch 1252/1896:  Train loss: 0.0028168344870209694  Test loss: 0.004806771408766508 \n",
      "Epoch: 1/1:  mini-batch 1253/1896:  Train loss: 0.004037456586956978  Test loss: 0.004819470923393965 \n",
      "Epoch: 1/1:  mini-batch 1254/1896:  Train loss: 0.0022240090183913708  Test loss: 0.00480640958994627 \n",
      "Epoch: 1/1:  mini-batch 1255/1896:  Train loss: 0.0066236169077456  Test loss: 0.004869001917541027 \n",
      "Epoch: 1/1:  mini-batch 1256/1896:  Train loss: 0.005967393517494202  Test loss: 0.0049104876816272736 \n",
      "Epoch: 1/1:  mini-batch 1257/1896:  Train loss: 0.0034479524474591017  Test loss: 0.0048522031866014 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1258/1896:  Train loss: 0.0014525542501360178  Test loss: 0.004764222539961338 \n",
      "Epoch: 1/1:  mini-batch 1259/1896:  Train loss: 0.005144616588950157  Test loss: 0.00487627275288105 \n",
      "Epoch: 1/1:  mini-batch 1260/1896:  Train loss: 0.0037782154977321625  Test loss: 0.0051842937245965 \n",
      "Epoch: 1/1:  mini-batch 1261/1896:  Train loss: 0.005510757677257061  Test loss: 0.005283411126583815 \n",
      "Epoch: 1/1:  mini-batch 1262/1896:  Train loss: 0.0063978759571909904  Test loss: 0.00494473846629262 \n",
      "Epoch: 1/1:  mini-batch 1263/1896:  Train loss: 0.004788202699273825  Test loss: 0.004770274274051189 \n",
      "Epoch: 1/1:  mini-batch 1264/1896:  Train loss: 0.002140349242836237  Test loss: 0.004811851307749748 \n",
      "Epoch: 1/1:  mini-batch 1265/1896:  Train loss: 0.00732042733579874  Test loss: 0.004831327125430107 \n",
      "Epoch: 1/1:  mini-batch 1266/1896:  Train loss: 0.006669997703284025  Test loss: 0.004801386967301369 \n",
      "Epoch: 1/1:  mini-batch 1267/1896:  Train loss: 0.0020277928560972214  Test loss: 0.004805929493159056 \n",
      "Epoch: 1/1:  mini-batch 1268/1896:  Train loss: 0.0067934379912912846  Test loss: 0.004834678024053574 \n",
      "Epoch: 1/1:  mini-batch 1269/1896:  Train loss: 0.004166038706898689  Test loss: 0.004811125807464123 \n",
      "Epoch: 1/1:  mini-batch 1270/1896:  Train loss: 0.0016888404497876763  Test loss: 0.0047903601080179214 \n",
      "Epoch: 1/1:  mini-batch 1271/1896:  Train loss: 0.00521438242867589  Test loss: 0.004772155545651913 \n",
      "Epoch: 1/1:  mini-batch 1272/1896:  Train loss: 0.0011403125245124102  Test loss: 0.0047720326110720634 \n",
      "Epoch: 1/1:  mini-batch 1273/1896:  Train loss: 0.0026522367261350155  Test loss: 0.0047757732681930065 \n",
      "Epoch: 1/1:  mini-batch 1274/1896:  Train loss: 0.003811116795986891  Test loss: 0.004783276468515396 \n",
      "Epoch: 1/1:  mini-batch 1275/1896:  Train loss: 0.0004948361893184483  Test loss: 0.004800410941243172 \n",
      "Epoch: 1/1:  mini-batch 1276/1896:  Train loss: 0.002472896594554186  Test loss: 0.004823316354304552 \n",
      "Epoch: 1/1:  mini-batch 1277/1896:  Train loss: 0.005142455920577049  Test loss: 0.004838583059608936 \n",
      "Epoch: 1/1:  mini-batch 1278/1896:  Train loss: 0.0047089094296097755  Test loss: 0.004858817905187607 \n",
      "Epoch: 1/1:  mini-batch 1279/1896:  Train loss: 0.005711022764444351  Test loss: 0.004841595888137817 \n",
      "Epoch: 1/1:  mini-batch 1280/1896:  Train loss: 0.0021855549421161413  Test loss: 0.004796070046722889 \n",
      "Epoch: 1/1:  mini-batch 1281/1896:  Train loss: 0.004121442791074514  Test loss: 0.004765619523823261 \n",
      "Epoch: 1/1:  mini-batch 1282/1896:  Train loss: 0.004844821058213711  Test loss: 0.004786903038620949 \n",
      "Epoch: 1/1:  mini-batch 1283/1896:  Train loss: 0.004805518314242363  Test loss: 0.004850673023611307 \n",
      "Epoch: 1/1:  mini-batch 1284/1896:  Train loss: 0.0075920759700238705  Test loss: 0.0048440927639603615 \n",
      "Epoch: 1/1:  mini-batch 1285/1896:  Train loss: 0.0008847647113725543  Test loss: 0.004778115078806877 \n",
      "Epoch: 1/1:  mini-batch 1286/1896:  Train loss: 0.003750569187104702  Test loss: 0.004770265426486731 \n",
      "Epoch: 1/1:  mini-batch 1287/1896:  Train loss: 0.0012556429719552398  Test loss: 0.004829494748264551 \n",
      "Epoch: 1/1:  mini-batch 1288/1896:  Train loss: 0.007158549502491951  Test loss: 0.004916752688586712 \n",
      "Epoch: 1/1:  mini-batch 1289/1896:  Train loss: 0.005959801841527224  Test loss: 0.004956632852554321 \n",
      "Epoch: 1/1:  mini-batch 1290/1896:  Train loss: 0.0018037647241726518  Test loss: 0.004882881883531809 \n",
      "Epoch: 1/1:  mini-batch 1291/1896:  Train loss: 0.0031356988474726677  Test loss: 0.00480438070371747 \n",
      "Epoch: 1/1:  mini-batch 1292/1896:  Train loss: 0.00499647855758667  Test loss: 0.004873758647590876 \n",
      "Epoch: 1/1:  mini-batch 1293/1896:  Train loss: 0.006591827142983675  Test loss: 0.00501912459731102 \n",
      "Epoch: 1/1:  mini-batch 1294/1896:  Train loss: 0.0038722162134945393  Test loss: 0.005052994005382061 \n",
      "Epoch: 1/1:  mini-batch 1295/1896:  Train loss: 0.006022464483976364  Test loss: 0.004862487781792879 \n",
      "Epoch: 1/1:  mini-batch 1296/1896:  Train loss: 0.006995610427111387  Test loss: 0.004778199829161167 \n",
      "Epoch: 1/1:  mini-batch 1297/1896:  Train loss: 0.002494458109140396  Test loss: 0.004801526665687561 \n",
      "Epoch: 1/1:  mini-batch 1298/1896:  Train loss: 0.0033737588673830032  Test loss: 0.004938122350722551 \n",
      "Epoch: 1/1:  mini-batch 1299/1896:  Train loss: 0.0063108280301094055  Test loss: 0.005107351578772068 \n",
      "Epoch: 1/1:  mini-batch 1300/1896:  Train loss: 0.005465309135615826  Test loss: 0.005053351167589426 \n",
      "Epoch: 1/1:  mini-batch 1301/1896:  Train loss: 0.0031498319003731012  Test loss: 0.0048280153423547745 \n",
      "Epoch: 1/1:  mini-batch 1302/1896:  Train loss: 0.0037856330163776875  Test loss: 0.004772505722939968 \n",
      "Epoch: 1/1:  mini-batch 1303/1896:  Train loss: 0.003917656373232603  Test loss: 0.004781458061188459 \n",
      "Epoch: 1/1:  mini-batch 1304/1896:  Train loss: 0.00392865389585495  Test loss: 0.004809203557670116 \n",
      "Epoch: 1/1:  mini-batch 1305/1896:  Train loss: 0.0009394170483574271  Test loss: 0.00479622557759285 \n",
      "Epoch: 1/1:  mini-batch 1306/1896:  Train loss: 0.005261825397610664  Test loss: 0.004782890900969505 \n",
      "Epoch: 1/1:  mini-batch 1307/1896:  Train loss: 0.0036575021222233772  Test loss: 0.004791007377207279 \n",
      "Epoch: 1/1:  mini-batch 1308/1896:  Train loss: 0.007190300151705742  Test loss: 0.004845475312322378 \n",
      "Epoch: 1/1:  mini-batch 1309/1896:  Train loss: 0.0020925533026456833  Test loss: 0.0049394830130040646 \n",
      "Epoch: 1/1:  mini-batch 1310/1896:  Train loss: 0.006556770298629999  Test loss: 0.004880276508629322 \n",
      "Epoch: 1/1:  mini-batch 1311/1896:  Train loss: 0.007519261445850134  Test loss: 0.004767077974975109 \n",
      "Epoch: 1/1:  mini-batch 1312/1896:  Train loss: 0.005803205072879791  Test loss: 0.0048849135637283325 \n",
      "Epoch: 1/1:  mini-batch 1313/1896:  Train loss: 0.004798704758286476  Test loss: 0.004920560866594315 \n",
      "Epoch: 1/1:  mini-batch 1314/1896:  Train loss: 0.0029781463090330362  Test loss: 0.004816984757781029 \n",
      "Epoch: 1/1:  mini-batch 1315/1896:  Train loss: 0.005525515880435705  Test loss: 0.0047819362953305244 \n",
      "Epoch: 1/1:  mini-batch 1316/1896:  Train loss: 0.0026536339428275824  Test loss: 0.00486784428358078 \n",
      "Epoch: 1/1:  mini-batch 1317/1896:  Train loss: 0.003942785318940878  Test loss: 0.0048897527158260345 \n",
      "Epoch: 1/1:  mini-batch 1318/1896:  Train loss: 0.0020889388397336006  Test loss: 0.004879866261035204 \n",
      "Epoch: 1/1:  mini-batch 1319/1896:  Train loss: 0.004146185703575611  Test loss: 0.004832425154745579 \n",
      "Epoch: 1/1:  mini-batch 1320/1896:  Train loss: 0.005208941642194986  Test loss: 0.004787192679941654 \n",
      "Epoch: 1/1:  mini-batch 1321/1896:  Train loss: 0.0027542260941118  Test loss: 0.004797869361937046 \n",
      "Epoch: 1/1:  mini-batch 1322/1896:  Train loss: 0.004145107232034206  Test loss: 0.004828706383705139 \n",
      "Epoch: 1/1:  mini-batch 1323/1896:  Train loss: 0.001500852289609611  Test loss: 0.00481455959379673 \n",
      "Epoch: 1/1:  mini-batch 1324/1896:  Train loss: 0.006059974431991577  Test loss: 0.004783899988979101 \n",
      "Epoch: 1/1:  mini-batch 1325/1896:  Train loss: 0.001103560090996325  Test loss: 0.004790800157934427 \n",
      "Epoch: 1/1:  mini-batch 1326/1896:  Train loss: 0.005724603775888681  Test loss: 0.004817336797714233 \n",
      "Epoch: 1/1:  mini-batch 1327/1896:  Train loss: 0.005557958502322435  Test loss: 0.004823504947125912 \n",
      "Epoch: 1/1:  mini-batch 1328/1896:  Train loss: 0.001988093601539731  Test loss: 0.0048175593838095665 \n",
      "Epoch: 1/1:  mini-batch 1329/1896:  Train loss: 0.005422818474471569  Test loss: 0.004785018041729927 \n",
      "Epoch: 1/1:  mini-batch 1330/1896:  Train loss: 0.006130128167569637  Test loss: 0.00477371271699667 \n",
      "Epoch: 1/1:  mini-batch 1331/1896:  Train loss: 0.0007736866245977581  Test loss: 0.0048142410814762115 \n",
      "Epoch: 1/1:  mini-batch 1332/1896:  Train loss: 0.006212736014276743  Test loss: 0.0048764776438474655 \n",
      "Epoch: 1/1:  mini-batch 1333/1896:  Train loss: 0.005079749505966902  Test loss: 0.004813436418771744 \n",
      "Epoch: 1/1:  mini-batch 1334/1896:  Train loss: 0.0028003889601677656  Test loss: 0.004787069745361805 \n",
      "Epoch: 1/1:  mini-batch 1335/1896:  Train loss: 0.0037593550514429808  Test loss: 0.004958776757121086 \n",
      "Epoch: 1/1:  mini-batch 1336/1896:  Train loss: 0.007990777492523193  Test loss: 0.0052322568371891975 \n",
      "Epoch: 1/1:  mini-batch 1337/1896:  Train loss: 0.003515907796099782  Test loss: 0.005161866545677185 \n",
      "Epoch: 1/1:  mini-batch 1338/1896:  Train loss: 0.005585802253335714  Test loss: 0.004901178181171417 \n",
      "Epoch: 1/1:  mini-batch 1339/1896:  Train loss: 0.004942526109516621  Test loss: 0.004850754514336586 \n",
      "Epoch: 1/1:  mini-batch 1340/1896:  Train loss: 0.0015997921582311392  Test loss: 0.005038882605731487 \n",
      "Epoch: 1/1:  mini-batch 1341/1896:  Train loss: 0.0064087193459272385  Test loss: 0.005081119015812874 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1342/1896:  Train loss: 0.002712847897782922  Test loss: 0.00495121069252491 \n",
      "Epoch: 1/1:  mini-batch 1343/1896:  Train loss: 0.003188575617969036  Test loss: 0.004776955582201481 \n",
      "Epoch: 1/1:  mini-batch 1344/1896:  Train loss: 0.0017569790361449122  Test loss: 0.00484597310423851 \n",
      "Epoch: 1/1:  mini-batch 1345/1896:  Train loss: 0.001569933257997036  Test loss: 0.005140863358974457 \n",
      "Epoch: 1/1:  mini-batch 1346/1896:  Train loss: 0.003025003243237734  Test loss: 0.005191135685890913 \n",
      "Epoch: 1/1:  mini-batch 1347/1896:  Train loss: 0.003996134735643864  Test loss: 0.0050080036744475365 \n",
      "Epoch: 1/1:  mini-batch 1348/1896:  Train loss: 0.0019256890518590808  Test loss: 0.004807954654097557 \n",
      "Epoch: 1/1:  mini-batch 1349/1896:  Train loss: 0.0004984041443094611  Test loss: 0.004772231448441744 \n",
      "Epoch: 1/1:  mini-batch 1350/1896:  Train loss: 0.004549757111817598  Test loss: 0.004796475637704134 \n",
      "Epoch: 1/1:  mini-batch 1351/1896:  Train loss: 0.0019055008888244629  Test loss: 0.004798697307705879 \n",
      "Epoch: 1/1:  mini-batch 1352/1896:  Train loss: 0.002495602937415242  Test loss: 0.004780127201229334 \n",
      "Epoch: 1/1:  mini-batch 1353/1896:  Train loss: 0.004114508628845215  Test loss: 0.004787379875779152 \n",
      "Epoch: 1/1:  mini-batch 1354/1896:  Train loss: 0.0012863280717283487  Test loss: 0.004777746275067329 \n",
      "Epoch: 1/1:  mini-batch 1355/1896:  Train loss: 0.00416853092610836  Test loss: 0.004786395467817783 \n",
      "Epoch: 1/1:  mini-batch 1356/1896:  Train loss: 0.0016185628483071923  Test loss: 0.004841732792556286 \n",
      "Epoch: 1/1:  mini-batch 1357/1896:  Train loss: 0.004305778071284294  Test loss: 0.004880391061306 \n",
      "Epoch: 1/1:  mini-batch 1358/1896:  Train loss: 0.002131943590939045  Test loss: 0.004857433959841728 \n",
      "Epoch: 1/1:  mini-batch 1359/1896:  Train loss: 0.003077263478189707  Test loss: 0.004797295667231083 \n",
      "Epoch: 1/1:  mini-batch 1360/1896:  Train loss: 0.003085792064666748  Test loss: 0.004780195187777281 \n",
      "Epoch: 1/1:  mini-batch 1361/1896:  Train loss: 0.0038363286294043064  Test loss: 0.004824161529541016 \n",
      "Epoch: 1/1:  mini-batch 1362/1896:  Train loss: 0.00831139087677002  Test loss: 0.0049131568521261215 \n",
      "Epoch: 1/1:  mini-batch 1363/1896:  Train loss: 0.003515595803037286  Test loss: 0.0049348026514053345 \n",
      "Epoch: 1/1:  mini-batch 1364/1896:  Train loss: 0.002224739408120513  Test loss: 0.004813813604414463 \n",
      "Epoch: 1/1:  mini-batch 1365/1896:  Train loss: 0.001209890004247427  Test loss: 0.004766993224620819 \n",
      "Epoch: 1/1:  mini-batch 1366/1896:  Train loss: 0.0035474449396133423  Test loss: 0.004845292307436466 \n",
      "Epoch: 1/1:  mini-batch 1367/1896:  Train loss: 0.005027017556130886  Test loss: 0.004887589253485203 \n",
      "Epoch: 1/1:  mini-batch 1368/1896:  Train loss: 0.0023555762600153685  Test loss: 0.004864176735281944 \n",
      "Epoch: 1/1:  mini-batch 1369/1896:  Train loss: 0.006350881885737181  Test loss: 0.004813044331967831 \n",
      "Epoch: 1/1:  mini-batch 1370/1896:  Train loss: 0.0013009266695007682  Test loss: 0.004783879965543747 \n",
      "Epoch: 1/1:  mini-batch 1371/1896:  Train loss: 0.0045674992725253105  Test loss: 0.0047847190871834755 \n",
      "Epoch: 1/1:  mini-batch 1372/1896:  Train loss: 0.002616163808852434  Test loss: 0.0048071108758449554 \n",
      "Epoch: 1/1:  mini-batch 1373/1896:  Train loss: 0.0011678957380354404  Test loss: 0.004817581735551357 \n",
      "Epoch: 1/1:  mini-batch 1374/1896:  Train loss: 0.00810583122074604  Test loss: 0.004765252582728863 \n",
      "Epoch: 1/1:  mini-batch 1375/1896:  Train loss: 0.004865728318691254  Test loss: 0.004885736852884293 \n",
      "Epoch: 1/1:  mini-batch 1376/1896:  Train loss: 0.005637936294078827  Test loss: 0.004966684151440859 \n",
      "Epoch: 1/1:  mini-batch 1377/1896:  Train loss: 0.0019609080627560616  Test loss: 0.004948497749865055 \n",
      "Epoch: 1/1:  mini-batch 1378/1896:  Train loss: 0.002345549874007702  Test loss: 0.004823037423193455 \n",
      "Epoch: 1/1:  mini-batch 1379/1896:  Train loss: 0.0022497549653053284  Test loss: 0.004790202714502811 \n",
      "Epoch: 1/1:  mini-batch 1380/1896:  Train loss: 0.00309671345166862  Test loss: 0.0049421293660998344 \n",
      "Epoch: 1/1:  mini-batch 1381/1896:  Train loss: 0.0016219915123656392  Test loss: 0.005079236812889576 \n",
      "Epoch: 1/1:  mini-batch 1382/1896:  Train loss: 0.0027606049552559853  Test loss: 0.004961740225553513 \n",
      "Epoch: 1/1:  mini-batch 1383/1896:  Train loss: 0.003779444843530655  Test loss: 0.004767249338328838 \n",
      "Epoch: 1/1:  mini-batch 1384/1896:  Train loss: 0.0031945521477609873  Test loss: 0.004959775600582361 \n",
      "Epoch: 1/1:  mini-batch 1385/1896:  Train loss: 0.0043161665089428425  Test loss: 0.005279770120978355 \n",
      "Epoch: 1/1:  mini-batch 1386/1896:  Train loss: 0.002794796833768487  Test loss: 0.005205575376749039 \n",
      "Epoch: 1/1:  mini-batch 1387/1896:  Train loss: 0.0015499577857553959  Test loss: 0.004910356365144253 \n",
      "Epoch: 1/1:  mini-batch 1388/1896:  Train loss: 0.0022072517313063145  Test loss: 0.004775800742208958 \n",
      "Epoch: 1/1:  mini-batch 1389/1896:  Train loss: 0.0031696718651801348  Test loss: 0.0048775505274534225 \n",
      "Epoch: 1/1:  mini-batch 1390/1896:  Train loss: 0.0009772909106686711  Test loss: 0.00499354675412178 \n",
      "Epoch: 1/1:  mini-batch 1391/1896:  Train loss: 0.0032786079682409763  Test loss: 0.004948169458657503 \n",
      "Epoch: 1/1:  mini-batch 1392/1896:  Train loss: 0.001834105234593153  Test loss: 0.004790015518665314 \n",
      "Epoch: 1/1:  mini-batch 1393/1896:  Train loss: 0.004190047271549702  Test loss: 0.004778769798576832 \n",
      "Epoch: 1/1:  mini-batch 1394/1896:  Train loss: 0.005650722421705723  Test loss: 0.00493934890255332 \n",
      "Epoch: 1/1:  mini-batch 1395/1896:  Train loss: 0.002393701346591115  Test loss: 0.005040757358074188 \n",
      "Epoch: 1/1:  mini-batch 1396/1896:  Train loss: 0.007463974878191948  Test loss: 0.004925750195980072 \n",
      "Epoch: 1/1:  mini-batch 1397/1896:  Train loss: 0.00447883689776063  Test loss: 0.004777540452778339 \n",
      "Epoch: 1/1:  mini-batch 1398/1896:  Train loss: 0.0057695647701621056  Test loss: 0.004815608263015747 \n",
      "Epoch: 1/1:  mini-batch 1399/1896:  Train loss: 0.00345493177883327  Test loss: 0.0048751323483884335 \n",
      "Epoch: 1/1:  mini-batch 1400/1896:  Train loss: 0.002071069087833166  Test loss: 0.0048491195775568485 \n",
      "Epoch: 1/1:  mini-batch 1401/1896:  Train loss: 0.0016456983285024762  Test loss: 0.004813355393707752 \n",
      "Epoch: 1/1:  mini-batch 1402/1896:  Train loss: 0.0008552214130759239  Test loss: 0.004788578953593969 \n",
      "Epoch: 1/1:  mini-batch 1403/1896:  Train loss: 0.006697293370962143  Test loss: 0.004916471429169178 \n",
      "Epoch: 1/1:  mini-batch 1404/1896:  Train loss: 0.00304209440946579  Test loss: 0.004944027401506901 \n",
      "Epoch: 1/1:  mini-batch 1405/1896:  Train loss: 0.0008867498254403472  Test loss: 0.004850192926824093 \n",
      "Epoch: 1/1:  mini-batch 1406/1896:  Train loss: 0.0004757103160955012  Test loss: 0.004768138751387596 \n",
      "Epoch: 1/1:  mini-batch 1407/1896:  Train loss: 0.0008807394187897444  Test loss: 0.004772382788360119 \n",
      "Epoch: 1/1:  mini-batch 1408/1896:  Train loss: 0.0013412542175501585  Test loss: 0.00478243175894022 \n",
      "Epoch: 1/1:  mini-batch 1409/1896:  Train loss: 0.0019704613368958235  Test loss: 0.004804309457540512 \n",
      "Epoch: 1/1:  mini-batch 1410/1896:  Train loss: 0.0032492652535438538  Test loss: 0.004784194752573967 \n",
      "Epoch: 1/1:  mini-batch 1411/1896:  Train loss: 0.005631769075989723  Test loss: 0.0047722188755869865 \n",
      "Epoch: 1/1:  mini-batch 1412/1896:  Train loss: 0.0042367177084088326  Test loss: 0.004772885702550411 \n",
      "Epoch: 1/1:  mini-batch 1413/1896:  Train loss: 0.0036104407627135515  Test loss: 0.004788552410900593 \n",
      "Epoch: 1/1:  mini-batch 1414/1896:  Train loss: 0.005834246054291725  Test loss: 0.00479412917047739 \n",
      "Epoch: 1/1:  mini-batch 1415/1896:  Train loss: 0.007126519922167063  Test loss: 0.004782618954777718 \n",
      "Epoch: 1/1:  mini-batch 1416/1896:  Train loss: 0.002678732853382826  Test loss: 0.004792478401213884 \n",
      "Epoch: 1/1:  mini-batch 1417/1896:  Train loss: 0.0019785368349403143  Test loss: 0.004802921786904335 \n",
      "Epoch: 1/1:  mini-batch 1418/1896:  Train loss: 0.004229139070957899  Test loss: 0.004799654707312584 \n",
      "Epoch: 1/1:  mini-batch 1419/1896:  Train loss: 0.004859359003603458  Test loss: 0.004785202443599701 \n",
      "Epoch: 1/1:  mini-batch 1420/1896:  Train loss: 0.00353117473423481  Test loss: 0.0047685531899333 \n",
      "Epoch: 1/1:  mini-batch 1421/1896:  Train loss: 0.001902946038171649  Test loss: 0.004769960418343544 \n",
      "Epoch: 1/1:  mini-batch 1422/1896:  Train loss: 0.0009977223817259073  Test loss: 0.0047804201021790504 \n",
      "Epoch: 1/1:  mini-batch 1423/1896:  Train loss: 0.0027125380001962185  Test loss: 0.004766892176121473 \n",
      "Epoch: 1/1:  mini-batch 1424/1896:  Train loss: 0.004528637044131756  Test loss: 0.004774963948875666 \n",
      "Epoch: 1/1:  mini-batch 1425/1896:  Train loss: 0.004903389140963554  Test loss: 0.004779189359396696 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1426/1896:  Train loss: 0.004215397406369448  Test loss: 0.004782800562679768 \n",
      "Epoch: 1/1:  mini-batch 1427/1896:  Train loss: 0.004659665282815695  Test loss: 0.004789235070347786 \n",
      "Epoch: 1/1:  mini-batch 1428/1896:  Train loss: 0.0032240883447229862  Test loss: 0.00479681184515357 \n",
      "Epoch: 1/1:  mini-batch 1429/1896:  Train loss: 0.005913700442761183  Test loss: 0.004795826505869627 \n",
      "Epoch: 1/1:  mini-batch 1430/1896:  Train loss: 0.00392056442797184  Test loss: 0.004768667742609978 \n",
      "Epoch: 1/1:  mini-batch 1431/1896:  Train loss: 0.003748082322999835  Test loss: 0.004761761985719204 \n",
      "Epoch: 1/1:  mini-batch 1432/1896:  Train loss: 0.006351528689265251  Test loss: 0.004774998873472214 \n",
      "Epoch: 1/1:  mini-batch 1433/1896:  Train loss: 0.006896433420479298  Test loss: 0.004778274334967136 \n",
      "Epoch: 1/1:  mini-batch 1434/1896:  Train loss: 0.0038913493044674397  Test loss: 0.00479498878121376 \n",
      "Epoch: 1/1:  mini-batch 1435/1896:  Train loss: 0.002274635713547468  Test loss: 0.0047744689509272575 \n",
      "Epoch: 1/1:  mini-batch 1436/1896:  Train loss: 0.00692480243742466  Test loss: 0.004788299091160297 \n",
      "Epoch: 1/1:  mini-batch 1437/1896:  Train loss: 0.004344029352068901  Test loss: 0.004788346588611603 \n",
      "Epoch: 1/1:  mini-batch 1438/1896:  Train loss: 0.0022053830325603485  Test loss: 0.004803062416613102 \n",
      "Epoch: 1/1:  mini-batch 1439/1896:  Train loss: 0.0032817241735756397  Test loss: 0.004880479536950588 \n",
      "Epoch: 1/1:  mini-batch 1440/1896:  Train loss: 0.002297342522069812  Test loss: 0.004969315603375435 \n",
      "Epoch: 1/1:  mini-batch 1441/1896:  Train loss: 0.002612635027617216  Test loss: 0.0048740822821855545 \n",
      "Epoch: 1/1:  mini-batch 1442/1896:  Train loss: 0.0034890519455075264  Test loss: 0.004767262376844883 \n",
      "Epoch: 1/1:  mini-batch 1443/1896:  Train loss: 0.0025340132415294647  Test loss: 0.00479363277554512 \n",
      "Epoch: 1/1:  mini-batch 1444/1896:  Train loss: 0.0017388159176334739  Test loss: 0.004811892285943031 \n",
      "Epoch: 1/1:  mini-batch 1445/1896:  Train loss: 0.006046139169484377  Test loss: 0.004798517562448978 \n",
      "Epoch: 1/1:  mini-batch 1446/1896:  Train loss: 0.0009835115633904934  Test loss: 0.004775252193212509 \n",
      "Epoch: 1/1:  mini-batch 1447/1896:  Train loss: 0.005140166264027357  Test loss: 0.00477402750402689 \n",
      "Epoch: 1/1:  mini-batch 1448/1896:  Train loss: 0.0011234763078391552  Test loss: 0.004800326190888882 \n",
      "Epoch: 1/1:  mini-batch 1449/1896:  Train loss: 0.002570910844951868  Test loss: 0.0048577021807432175 \n",
      "Epoch: 1/1:  mini-batch 1450/1896:  Train loss: 0.0009597270982339978  Test loss: 0.004846284631639719 \n",
      "Epoch: 1/1:  mini-batch 1451/1896:  Train loss: 0.0037406061310321093  Test loss: 0.00480633182451129 \n",
      "Epoch: 1/1:  mini-batch 1452/1896:  Train loss: 0.006239108741283417  Test loss: 0.0047946167178452015 \n",
      "Epoch: 1/1:  mini-batch 1453/1896:  Train loss: 0.0024626674130558968  Test loss: 0.004776385612785816 \n",
      "Epoch: 1/1:  mini-batch 1454/1896:  Train loss: 0.0012522782199084759  Test loss: 0.004762168042361736 \n",
      "Epoch: 1/1:  mini-batch 1455/1896:  Train loss: 0.0008256968576461077  Test loss: 0.004783011041581631 \n",
      "Epoch: 1/1:  mini-batch 1456/1896:  Train loss: 0.005038149189203978  Test loss: 0.0048028770834207535 \n",
      "Epoch: 1/1:  mini-batch 1457/1896:  Train loss: 0.0019366692285984755  Test loss: 0.004785370081663132 \n",
      "Epoch: 1/1:  mini-batch 1458/1896:  Train loss: 0.00224499823525548  Test loss: 0.004786781966686249 \n",
      "Epoch: 1/1:  mini-batch 1459/1896:  Train loss: 0.003193432232365012  Test loss: 0.0048350123688578606 \n",
      "Epoch: 1/1:  mini-batch 1460/1896:  Train loss: 0.0009684902615845203  Test loss: 0.004857129417359829 \n",
      "Epoch: 1/1:  mini-batch 1461/1896:  Train loss: 0.008545486256480217  Test loss: 0.004764898214489222 \n",
      "Epoch: 1/1:  mini-batch 1462/1896:  Train loss: 0.0034597343765199184  Test loss: 0.004827434197068214 \n",
      "Epoch: 1/1:  mini-batch 1463/1896:  Train loss: 0.004540969152003527  Test loss: 0.004836621694266796 \n",
      "Epoch: 1/1:  mini-batch 1464/1896:  Train loss: 0.0029101199470460415  Test loss: 0.004792771302163601 \n",
      "Epoch: 1/1:  mini-batch 1465/1896:  Train loss: 0.005029193125665188  Test loss: 0.004840222653001547 \n",
      "Epoch: 1/1:  mini-batch 1466/1896:  Train loss: 0.003386093769222498  Test loss: 0.004878505133092403 \n",
      "Epoch: 1/1:  mini-batch 1467/1896:  Train loss: 0.0031418949365615845  Test loss: 0.004893195815384388 \n",
      "Epoch: 1/1:  mini-batch 1468/1896:  Train loss: 0.004327543079853058  Test loss: 0.004787735175341368 \n",
      "Epoch: 1/1:  mini-batch 1469/1896:  Train loss: 0.0022355520632117987  Test loss: 0.004783717915415764 \n",
      "Epoch: 1/1:  mini-batch 1470/1896:  Train loss: 0.0004248222685419023  Test loss: 0.0048334333114326 \n",
      "Epoch: 1/1:  mini-batch 1471/1896:  Train loss: 0.0014664886984974146  Test loss: 0.004851332865655422 \n",
      "Epoch: 1/1:  mini-batch 1472/1896:  Train loss: 0.0030915236566215754  Test loss: 0.004821187816560268 \n",
      "Epoch: 1/1:  mini-batch 1473/1896:  Train loss: 0.002104331273585558  Test loss: 0.004796171095222235 \n",
      "Epoch: 1/1:  mini-batch 1474/1896:  Train loss: 0.0022431411780416965  Test loss: 0.00477955536916852 \n",
      "Epoch: 1/1:  mini-batch 1475/1896:  Train loss: 0.0027244980446994305  Test loss: 0.00482161995023489 \n",
      "Epoch: 1/1:  mini-batch 1476/1896:  Train loss: 0.002854194026440382  Test loss: 0.0048502907156944275 \n",
      "Epoch: 1/1:  mini-batch 1477/1896:  Train loss: 0.002051317598670721  Test loss: 0.004908558446913958 \n",
      "Epoch: 1/1:  mini-batch 1478/1896:  Train loss: 0.00530325947329402  Test loss: 0.004820812493562698 \n",
      "Epoch: 1/1:  mini-batch 1479/1896:  Train loss: 0.0034160129725933075  Test loss: 0.0047822557389736176 \n",
      "Epoch: 1/1:  mini-batch 1480/1896:  Train loss: 0.005832653492689133  Test loss: 0.004812168888747692 \n",
      "Epoch: 1/1:  mini-batch 1481/1896:  Train loss: 0.0033700603526085615  Test loss: 0.004795104265213013 \n",
      "Epoch: 1/1:  mini-batch 1482/1896:  Train loss: 0.005478386767208576  Test loss: 0.004771396983414888 \n",
      "Epoch: 1/1:  mini-batch 1483/1896:  Train loss: 0.0044824290089309216  Test loss: 0.0048196641728281975 \n",
      "Epoch: 1/1:  mini-batch 1484/1896:  Train loss: 0.003928078804165125  Test loss: 0.004906277172267437 \n",
      "Epoch: 1/1:  mini-batch 1485/1896:  Train loss: 0.0030874870717525482  Test loss: 0.004896801896393299 \n",
      "Epoch: 1/1:  mini-batch 1486/1896:  Train loss: 0.0023773207794874907  Test loss: 0.004810956306755543 \n",
      "Epoch: 1/1:  mini-batch 1487/1896:  Train loss: 0.004722112324088812  Test loss: 0.004789134953171015 \n",
      "Epoch: 1/1:  mini-batch 1488/1896:  Train loss: 0.003791338764131069  Test loss: 0.004992413800209761 \n",
      "Epoch: 1/1:  mini-batch 1489/1896:  Train loss: 0.004214249085634947  Test loss: 0.005137555301189423 \n",
      "Epoch: 1/1:  mini-batch 1490/1896:  Train loss: 0.0028690488543361425  Test loss: 0.005025952123105526 \n",
      "Epoch: 1/1:  mini-batch 1491/1896:  Train loss: 0.005115799605846405  Test loss: 0.004864554852247238 \n",
      "Epoch: 1/1:  mini-batch 1492/1896:  Train loss: 0.003880724310874939  Test loss: 0.004775378853082657 \n",
      "Epoch: 1/1:  mini-batch 1493/1896:  Train loss: 0.0026120548136532307  Test loss: 0.004822895862162113 \n",
      "Epoch: 1/1:  mini-batch 1494/1896:  Train loss: 0.0023396241012960672  Test loss: 0.004868457093834877 \n",
      "Epoch: 1/1:  mini-batch 1495/1896:  Train loss: 0.006032929290086031  Test loss: 0.004843098111450672 \n",
      "Epoch: 1/1:  mini-batch 1496/1896:  Train loss: 0.0045331865549087524  Test loss: 0.004803326912224293 \n",
      "Epoch: 1/1:  mini-batch 1497/1896:  Train loss: 0.001889469800516963  Test loss: 0.004794924985617399 \n",
      "Epoch: 1/1:  mini-batch 1498/1896:  Train loss: 0.007277308031916618  Test loss: 0.004823197145015001 \n",
      "Epoch: 1/1:  mini-batch 1499/1896:  Train loss: 0.006361756939440966  Test loss: 0.004898701794445515 \n",
      "Epoch: 1/1:  mini-batch 1500/1896:  Train loss: 0.0011682873591780663  Test loss: 0.004910423420369625 \n",
      "Epoch: 1/1:  mini-batch 1501/1896:  Train loss: 0.0019868749659508467  Test loss: 0.0048627969808876514 \n",
      "Epoch: 1/1:  mini-batch 1502/1896:  Train loss: 0.006445565260946751  Test loss: 0.004768410697579384 \n",
      "Epoch: 1/1:  mini-batch 1503/1896:  Train loss: 0.0027525462210178375  Test loss: 0.004774605855345726 \n",
      "Epoch: 1/1:  mini-batch 1504/1896:  Train loss: 0.001041520037688315  Test loss: 0.00479162996634841 \n",
      "Epoch: 1/1:  mini-batch 1505/1896:  Train loss: 0.0077070449478924274  Test loss: 0.004779251292347908 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1506/1896:  Train loss: 0.005984167102724314  Test loss: 0.004801732487976551 \n",
      "Epoch: 1/1:  mini-batch 1507/1896:  Train loss: 0.0006680375081487  Test loss: 0.0048494418151676655 \n",
      "Epoch: 1/1:  mini-batch 1508/1896:  Train loss: 0.005162905901670456  Test loss: 0.004831498488783836 \n",
      "Epoch: 1/1:  mini-batch 1509/1896:  Train loss: 0.004649139009416103  Test loss: 0.004799767397344112 \n",
      "Epoch: 1/1:  mini-batch 1510/1896:  Train loss: 0.0008871564059518278  Test loss: 0.004783857613801956 \n",
      "Epoch: 1/1:  mini-batch 1511/1896:  Train loss: 0.0036296620965003967  Test loss: 0.004783011041581631 \n",
      "Epoch: 1/1:  mini-batch 1512/1896:  Train loss: 0.0058959368616342545  Test loss: 0.004799877293407917 \n",
      "Epoch: 1/1:  mini-batch 1513/1896:  Train loss: 0.0007042231154628098  Test loss: 0.004791339859366417 \n",
      "Epoch: 1/1:  mini-batch 1514/1896:  Train loss: 0.0011187480995431542  Test loss: 0.004768899641931057 \n",
      "Epoch: 1/1:  mini-batch 1515/1896:  Train loss: 0.0031147387344390154  Test loss: 0.004765981808304787 \n",
      "Epoch: 1/1:  mini-batch 1516/1896:  Train loss: 0.0008689456735737622  Test loss: 0.004814730957150459 \n",
      "Epoch: 1/1:  mini-batch 1517/1896:  Train loss: 0.0027436180971562862  Test loss: 0.004817659966647625 \n",
      "Epoch: 1/1:  mini-batch 1518/1896:  Train loss: 0.002737239236012101  Test loss: 0.004780426621437073 \n",
      "Epoch: 1/1:  mini-batch 1519/1896:  Train loss: 0.0026187847834080458  Test loss: 0.0047815535217523575 \n",
      "Epoch: 1/1:  mini-batch 1520/1896:  Train loss: 0.006532648578286171  Test loss: 0.004890815354883671 \n",
      "Epoch: 1/1:  mini-batch 1521/1896:  Train loss: 0.004998522810637951  Test loss: 0.0049207620322704315 \n",
      "Epoch: 1/1:  mini-batch 1522/1896:  Train loss: 0.002861378015950322  Test loss: 0.004784739576280117 \n",
      "Epoch: 1/1:  mini-batch 1523/1896:  Train loss: 0.0026491412427276373  Test loss: 0.004778964910656214 \n",
      "Epoch: 1/1:  mini-batch 1524/1896:  Train loss: 0.004685989115387201  Test loss: 0.0048415642231702805 \n",
      "Epoch: 1/1:  mini-batch 1525/1896:  Train loss: 0.0029604039154946804  Test loss: 0.004866583272814751 \n",
      "Epoch: 1/1:  mini-batch 1526/1896:  Train loss: 0.0011551165953278542  Test loss: 0.004836643114686012 \n",
      "Epoch: 1/1:  mini-batch 1527/1896:  Train loss: 0.0025870271492749453  Test loss: 0.004789137281477451 \n",
      "Epoch: 1/1:  mini-batch 1528/1896:  Train loss: 0.003239793237298727  Test loss: 0.004784064367413521 \n",
      "Epoch: 1/1:  mini-batch 1529/1896:  Train loss: 0.0056054312735795975  Test loss: 0.0047836704179644585 \n",
      "Epoch: 1/1:  mini-batch 1530/1896:  Train loss: 0.003135343547910452  Test loss: 0.004789809696376324 \n",
      "Epoch: 1/1:  mini-batch 1531/1896:  Train loss: 0.0053928871639072895  Test loss: 0.0047865938395261765 \n",
      "Epoch: 1/1:  mini-batch 1532/1896:  Train loss: 0.0018108905060216784  Test loss: 0.004781671799719334 \n",
      "Epoch: 1/1:  mini-batch 1533/1896:  Train loss: 0.0043791658245027065  Test loss: 0.004813681356608868 \n",
      "Epoch: 1/1:  mini-batch 1534/1896:  Train loss: 0.006766064092516899  Test loss: 0.004908637143671513 \n",
      "Epoch: 1/1:  mini-batch 1535/1896:  Train loss: 0.0035064886324107647  Test loss: 0.004963807296007872 \n",
      "Epoch: 1/1:  mini-batch 1536/1896:  Train loss: 0.004696790594607592  Test loss: 0.004828177392482758 \n",
      "Epoch: 1/1:  mini-batch 1537/1896:  Train loss: 0.0043672071769833565  Test loss: 0.004788495600223541 \n",
      "Epoch: 1/1:  mini-batch 1538/1896:  Train loss: 0.0009906243067234755  Test loss: 0.004829010926187038 \n",
      "Epoch: 1/1:  mini-batch 1539/1896:  Train loss: 0.0042881290428340435  Test loss: 0.004817165434360504 \n",
      "Epoch: 1/1:  mini-batch 1540/1896:  Train loss: 0.005051261745393276  Test loss: 0.004774586297571659 \n",
      "Epoch: 1/1:  mini-batch 1541/1896:  Train loss: 0.0029243009630590677  Test loss: 0.004892469383776188 \n",
      "Epoch: 1/1:  mini-batch 1542/1896:  Train loss: 0.0030233629513531923  Test loss: 0.004928593523800373 \n",
      "Epoch: 1/1:  mini-batch 1543/1896:  Train loss: 0.00231772568076849  Test loss: 0.004837107844650745 \n",
      "Epoch: 1/1:  mini-batch 1544/1896:  Train loss: 0.002743692835792899  Test loss: 0.004805947188287973 \n",
      "Epoch: 1/1:  mini-batch 1545/1896:  Train loss: 0.004342219326645136  Test loss: 0.004803353920578957 \n",
      "Epoch: 1/1:  mini-batch 1546/1896:  Train loss: 0.0058095199055969715  Test loss: 0.0047969575971364975 \n",
      "Epoch: 1/1:  mini-batch 1547/1896:  Train loss: 0.00420747697353363  Test loss: 0.004803772084414959 \n",
      "Epoch: 1/1:  mini-batch 1548/1896:  Train loss: 0.0016681586857885122  Test loss: 0.004849387798458338 \n",
      "Epoch: 1/1:  mini-batch 1549/1896:  Train loss: 0.0006795390509068966  Test loss: 0.004864594433456659 \n",
      "Epoch: 1/1:  mini-batch 1550/1896:  Train loss: 0.005157137755304575  Test loss: 0.0048401555977761745 \n",
      "Epoch: 1/1:  mini-batch 1551/1896:  Train loss: 0.007683567702770233  Test loss: 0.004844869486987591 \n",
      "Epoch: 1/1:  mini-batch 1552/1896:  Train loss: 0.005356820300221443  Test loss: 0.004826453980058432 \n",
      "Epoch: 1/1:  mini-batch 1553/1896:  Train loss: 0.001699264976195991  Test loss: 0.004780375398695469 \n",
      "Epoch: 1/1:  mini-batch 1554/1896:  Train loss: 0.0018798467935994267  Test loss: 0.004782186821103096 \n",
      "Epoch: 1/1:  mini-batch 1555/1896:  Train loss: 0.0021485143806785345  Test loss: 0.004817277193069458 \n",
      "Epoch: 1/1:  mini-batch 1556/1896:  Train loss: 0.00349650951102376  Test loss: 0.0048109483905136585 \n",
      "Epoch: 1/1:  mini-batch 1557/1896:  Train loss: 0.0006016144761815667  Test loss: 0.004774221684783697 \n",
      "Epoch: 1/1:  mini-batch 1558/1896:  Train loss: 0.0037833321839571  Test loss: 0.0047763255424797535 \n",
      "Epoch: 1/1:  mini-batch 1559/1896:  Train loss: 0.00396376196295023  Test loss: 0.004878397099673748 \n",
      "Epoch: 1/1:  mini-batch 1560/1896:  Train loss: 0.0018435566453263164  Test loss: 0.005063704214990139 \n",
      "Epoch: 1/1:  mini-batch 1561/1896:  Train loss: 0.005404235795140266  Test loss: 0.005155532620847225 \n",
      "Epoch: 1/1:  mini-batch 1562/1896:  Train loss: 0.008391817100346088  Test loss: 0.004954110831022263 \n",
      "Epoch: 1/1:  mini-batch 1563/1896:  Train loss: 0.005718364380300045  Test loss: 0.004821686074137688 \n",
      "Epoch: 1/1:  mini-batch 1564/1896:  Train loss: 0.0003243455139454454  Test loss: 0.004956471733748913 \n",
      "Epoch: 1/1:  mini-batch 1565/1896:  Train loss: 0.005679048132151365  Test loss: 0.005149455275386572 \n",
      "Epoch: 1/1:  mini-batch 1566/1896:  Train loss: 0.0010706224711611867  Test loss: 0.005053237546235323 \n",
      "Epoch: 1/1:  mini-batch 1567/1896:  Train loss: 0.002578144893050194  Test loss: 0.004785562865436077 \n",
      "Epoch: 1/1:  mini-batch 1568/1896:  Train loss: 0.0021437732502818108  Test loss: 0.00488753616809845 \n",
      "Epoch: 1/1:  mini-batch 1569/1896:  Train loss: 0.0027373817283660173  Test loss: 0.00506244134157896 \n",
      "Epoch: 1/1:  mini-batch 1570/1896:  Train loss: 0.0005279645556584001  Test loss: 0.005065618082880974 \n",
      "Epoch: 1/1:  mini-batch 1571/1896:  Train loss: 0.00286336662247777  Test loss: 0.004937092773616314 \n",
      "Epoch: 1/1:  mini-batch 1572/1896:  Train loss: 0.00879809632897377  Test loss: 0.004866358358412981 \n",
      "Epoch: 1/1:  mini-batch 1573/1896:  Train loss: 0.0007433376740664244  Test loss: 0.004796341992914677 \n",
      "Epoch: 1/1:  mini-batch 1574/1896:  Train loss: 0.005178743042051792  Test loss: 0.004776697605848312 \n",
      "Epoch: 1/1:  mini-batch 1575/1896:  Train loss: 0.009938368573784828  Test loss: 0.00480790575966239 \n",
      "Epoch: 1/1:  mini-batch 1576/1896:  Train loss: 0.0018154822755604982  Test loss: 0.004936748184263706 \n",
      "Epoch: 1/1:  mini-batch 1577/1896:  Train loss: 0.0009775017388164997  Test loss: 0.004921638406813145 \n",
      "Epoch: 1/1:  mini-batch 1578/1896:  Train loss: 0.005725890398025513  Test loss: 0.00479169562458992 \n",
      "Epoch: 1/1:  mini-batch 1579/1896:  Train loss: 0.0015543426852673292  Test loss: 0.004793713800609112 \n",
      "Epoch: 1/1:  mini-batch 1580/1896:  Train loss: 0.005563736893236637  Test loss: 0.004890771582722664 \n",
      "Epoch: 1/1:  mini-batch 1581/1896:  Train loss: 0.0032294702250510454  Test loss: 0.005018893163651228 \n",
      "Epoch: 1/1:  mini-batch 1582/1896:  Train loss: 0.0033042654395103455  Test loss: 0.005077962297946215 \n",
      "Epoch: 1/1:  mini-batch 1583/1896:  Train loss: 0.0019505773670971394  Test loss: 0.004883200861513615 \n",
      "Epoch: 1/1:  mini-batch 1584/1896:  Train loss: 0.0034999120980501175  Test loss: 0.0047644153237342834 \n",
      "Epoch: 1/1:  mini-batch 1585/1896:  Train loss: 0.003480271901935339  Test loss: 0.004785088822245598 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1586/1896:  Train loss: 0.0009217644692398608  Test loss: 0.004804074298590422 \n",
      "Epoch: 1/1:  mini-batch 1587/1896:  Train loss: 0.009523711167275906  Test loss: 0.004851414822041988 \n",
      "Epoch: 1/1:  mini-batch 1588/1896:  Train loss: 0.004460614174604416  Test loss: 0.00490991398692131 \n",
      "Epoch: 1/1:  mini-batch 1589/1896:  Train loss: 0.004584907554090023  Test loss: 0.00485222227871418 \n",
      "Epoch: 1/1:  mini-batch 1590/1896:  Train loss: 0.004524166230112314  Test loss: 0.004770769737660885 \n",
      "Epoch: 1/1:  mini-batch 1591/1896:  Train loss: 0.0017343894578516483  Test loss: 0.004791511222720146 \n",
      "Epoch: 1/1:  mini-batch 1592/1896:  Train loss: 0.004662990570068359  Test loss: 0.0050041875801980495 \n",
      "Epoch: 1/1:  mini-batch 1593/1896:  Train loss: 0.0021485041361302137  Test loss: 0.005250847898423672 \n",
      "Epoch: 1/1:  mini-batch 1594/1896:  Train loss: 0.00452951667830348  Test loss: 0.005107410252094269 \n",
      "Epoch: 1/1:  mini-batch 1595/1896:  Train loss: 0.0059034149162471294  Test loss: 0.004929837770760059 \n",
      "Epoch: 1/1:  mini-batch 1596/1896:  Train loss: 0.007613892666995525  Test loss: 0.004773895721882582 \n",
      "Epoch: 1/1:  mini-batch 1597/1896:  Train loss: 0.004022454377263784  Test loss: 0.005056777969002724 \n",
      "Epoch: 1/1:  mini-batch 1598/1896:  Train loss: 0.006353328470140696  Test loss: 0.005401181522756815 \n",
      "Epoch: 1/1:  mini-batch 1599/1896:  Train loss: 0.0030877101235091686  Test loss: 0.005106888711452484 \n",
      "Epoch: 1/1:  mini-batch 1600/1896:  Train loss: 0.008052853867411613  Test loss: 0.004772045649588108 \n",
      "Epoch: 1/1:  mini-batch 1601/1896:  Train loss: 0.004022859036922455  Test loss: 0.005331504158675671 \n",
      "Epoch: 1/1:  mini-batch 1602/1896:  Train loss: 0.003975625149905682  Test loss: 0.005765968002378941 \n",
      "Epoch: 1/1:  mini-batch 1603/1896:  Train loss: 0.005296837538480759  Test loss: 0.0053533948957920074 \n",
      "Epoch: 1/1:  mini-batch 1604/1896:  Train loss: 0.0048195007257163525  Test loss: 0.00485437735915184 \n",
      "Epoch: 1/1:  mini-batch 1605/1896:  Train loss: 0.003841185010969639  Test loss: 0.0048675537109375 \n",
      "Epoch: 1/1:  mini-batch 1606/1896:  Train loss: 0.002712518908083439  Test loss: 0.005249873735010624 \n",
      "Epoch: 1/1:  mini-batch 1607/1896:  Train loss: 0.004374704323709011  Test loss: 0.005256937816739082 \n",
      "Epoch: 1/1:  mini-batch 1608/1896:  Train loss: 0.004236165434122086  Test loss: 0.004979035817086697 \n",
      "Epoch: 1/1:  mini-batch 1609/1896:  Train loss: 0.003373364917933941  Test loss: 0.004770468920469284 \n",
      "Epoch: 1/1:  mini-batch 1610/1896:  Train loss: 0.0069224415346980095  Test loss: 0.004922767169773579 \n",
      "Epoch: 1/1:  mini-batch 1611/1896:  Train loss: 0.0010757179697975516  Test loss: 0.0051158107817173 \n",
      "Epoch: 1/1:  mini-batch 1612/1896:  Train loss: 0.004768225830048323  Test loss: 0.0051724957302212715 \n",
      "Epoch: 1/1:  mini-batch 1613/1896:  Train loss: 0.007881243713200092  Test loss: 0.0049530742689967155 \n",
      "Epoch: 1/1:  mini-batch 1614/1896:  Train loss: 0.0043007428757846355  Test loss: 0.004776875488460064 \n",
      "Epoch: 1/1:  mini-batch 1615/1896:  Train loss: 0.0031564622186124325  Test loss: 0.004829776473343372 \n",
      "Epoch: 1/1:  mini-batch 1616/1896:  Train loss: 0.009069384075701237  Test loss: 0.004859399050474167 \n",
      "Epoch: 1/1:  mini-batch 1617/1896:  Train loss: 0.0030330950394272804  Test loss: 0.004809585399925709 \n",
      "Epoch: 1/1:  mini-batch 1618/1896:  Train loss: 0.0064162868075072765  Test loss: 0.00477319210767746 \n",
      "Epoch: 1/1:  mini-batch 1619/1896:  Train loss: 0.0044788033701479435  Test loss: 0.00478778500109911 \n",
      "Epoch: 1/1:  mini-batch 1620/1896:  Train loss: 0.003959361929446459  Test loss: 0.004942689090967178 \n",
      "Epoch: 1/1:  mini-batch 1621/1896:  Train loss: 0.006210507825016975  Test loss: 0.004956209100782871 \n",
      "Epoch: 1/1:  mini-batch 1622/1896:  Train loss: 0.005055965390056372  Test loss: 0.004828939214348793 \n",
      "Epoch: 1/1:  mini-batch 1623/1896:  Train loss: 0.0021960907615721226  Test loss: 0.004772271029651165 \n",
      "Epoch: 1/1:  mini-batch 1624/1896:  Train loss: 0.004184677731245756  Test loss: 0.0048012398183345795 \n",
      "Epoch: 1/1:  mini-batch 1625/1896:  Train loss: 0.004098387900739908  Test loss: 0.004799056798219681 \n",
      "Epoch: 1/1:  mini-batch 1626/1896:  Train loss: 0.002320142462849617  Test loss: 0.0047745415940880775 \n",
      "Epoch: 1/1:  mini-batch 1627/1896:  Train loss: 0.004729767795652151  Test loss: 0.004794727079570293 \n",
      "Epoch: 1/1:  mini-batch 1628/1896:  Train loss: 0.005114112980663776  Test loss: 0.00479928869754076 \n",
      "Epoch: 1/1:  mini-batch 1629/1896:  Train loss: 0.002676250645890832  Test loss: 0.004807686433196068 \n",
      "Epoch: 1/1:  mini-batch 1630/1896:  Train loss: 0.006122850812971592  Test loss: 0.004784535616636276 \n",
      "Epoch: 1/1:  mini-batch 1631/1896:  Train loss: 0.004398985765874386  Test loss: 0.004771114327013493 \n",
      "Epoch: 1/1:  mini-batch 1632/1896:  Train loss: 0.002267576288431883  Test loss: 0.004768545739352703 \n",
      "Epoch: 1/1:  mini-batch 1633/1896:  Train loss: 0.004030541051179171  Test loss: 0.004765516147017479 \n",
      "Epoch: 1/1:  mini-batch 1634/1896:  Train loss: 0.005139410961419344  Test loss: 0.004772944841533899 \n",
      "Epoch: 1/1:  mini-batch 1635/1896:  Train loss: 0.002296984661370516  Test loss: 0.004793792963027954 \n",
      "Epoch: 1/1:  mini-batch 1636/1896:  Train loss: 0.0055815428495407104  Test loss: 0.0048843566328287125 \n",
      "Epoch: 1/1:  mini-batch 1637/1896:  Train loss: 0.0008618938154540956  Test loss: 0.004896481987088919 \n",
      "Epoch: 1/1:  mini-batch 1638/1896:  Train loss: 0.006927896291017532  Test loss: 0.004813567269593477 \n",
      "Epoch: 1/1:  mini-batch 1639/1896:  Train loss: 0.0026074585039168596  Test loss: 0.004790694918483496 \n",
      "Epoch: 1/1:  mini-batch 1640/1896:  Train loss: 0.002913304604589939  Test loss: 0.004826306831091642 \n",
      "Epoch: 1/1:  mini-batch 1641/1896:  Train loss: 0.0033295415341854095  Test loss: 0.004791243467479944 \n",
      "Epoch: 1/1:  mini-batch 1642/1896:  Train loss: 0.004502411931753159  Test loss: 0.004770012106746435 \n",
      "Epoch: 1/1:  mini-batch 1643/1896:  Train loss: 0.0010489070555195212  Test loss: 0.004843845032155514 \n",
      "Epoch: 1/1:  mini-batch 1644/1896:  Train loss: 0.006219829898327589  Test loss: 0.0049039144068956375 \n",
      "Epoch: 1/1:  mini-batch 1645/1896:  Train loss: 0.003120959969237447  Test loss: 0.0049016778357326984 \n",
      "Epoch: 1/1:  mini-batch 1646/1896:  Train loss: 0.004976631607860327  Test loss: 0.004908578936010599 \n",
      "Epoch: 1/1:  mini-batch 1647/1896:  Train loss: 0.002556790364906192  Test loss: 0.004780375398695469 \n",
      "Epoch: 1/1:  mini-batch 1648/1896:  Train loss: 0.0063673327676951885  Test loss: 0.004786400124430656 \n",
      "Epoch: 1/1:  mini-batch 1649/1896:  Train loss: 0.005487728863954544  Test loss: 0.004894109442830086 \n",
      "Epoch: 1/1:  mini-batch 1650/1896:  Train loss: 0.0045982045121490955  Test loss: 0.004911424126476049 \n",
      "Epoch: 1/1:  mini-batch 1651/1896:  Train loss: 0.0051423972472548485  Test loss: 0.004821214359253645 \n",
      "Epoch: 1/1:  mini-batch 1652/1896:  Train loss: 0.00963209755718708  Test loss: 0.00482957111671567 \n",
      "Epoch: 1/1:  mini-batch 1653/1896:  Train loss: 0.004808993078768253  Test loss: 0.004989616572856903 \n",
      "Epoch: 1/1:  mini-batch 1654/1896:  Train loss: 0.004670482594519854  Test loss: 0.0050054267048835754 \n",
      "Epoch: 1/1:  mini-batch 1655/1896:  Train loss: 0.005916614085435867  Test loss: 0.004874879959970713 \n",
      "Epoch: 1/1:  mini-batch 1656/1896:  Train loss: 0.004399552941322327  Test loss: 0.0047899279743433 \n",
      "Epoch: 1/1:  mini-batch 1657/1896:  Train loss: 0.005156048107892275  Test loss: 0.004826887510716915 \n",
      "Epoch: 1/1:  mini-batch 1658/1896:  Train loss: 0.003245957661420107  Test loss: 0.004893024452030659 \n",
      "Epoch: 1/1:  mini-batch 1659/1896:  Train loss: 0.005791601724922657  Test loss: 0.00480395182967186 \n",
      "Epoch: 1/1:  mini-batch 1660/1896:  Train loss: 0.0044110179878771305  Test loss: 0.0048240479081869125 \n",
      "Epoch: 1/1:  mini-batch 1661/1896:  Train loss: 0.0016528459964320064  Test loss: 0.004965629894286394 \n",
      "Epoch: 1/1:  mini-batch 1662/1896:  Train loss: 0.007648013066500425  Test loss: 0.004988337866961956 \n",
      "Epoch: 1/1:  mini-batch 1663/1896:  Train loss: 0.004193019587546587  Test loss: 0.004797071218490601 \n",
      "Epoch: 1/1:  mini-batch 1664/1896:  Train loss: 0.004889195319265127  Test loss: 0.004812383558601141 \n",
      "Epoch: 1/1:  mini-batch 1665/1896:  Train loss: 0.006289642304182053  Test loss: 0.005148710682988167 \n",
      "Epoch: 1/1:  mini-batch 1666/1896:  Train loss: 0.0013774519320577383  Test loss: 0.005152488127350807 \n",
      "Epoch: 1/1:  mini-batch 1667/1896:  Train loss: 0.0053248899057507515  Test loss: 0.0048684049397706985 \n",
      "Epoch: 1/1:  mini-batch 1668/1896:  Train loss: 0.004487904254347086  Test loss: 0.004805653356015682 \n",
      "Epoch: 1/1:  mini-batch 1669/1896:  Train loss: 0.004308545961976051  Test loss: 0.005173885263502598 \n",
      "Epoch: 1/1:  mini-batch 1670/1896:  Train loss: 0.00594101008027792  Test loss: 0.005574579816311598 \n",
      "Epoch: 1/1:  mini-batch 1671/1896:  Train loss: 0.007400200702250004  Test loss: 0.005354695022106171 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1672/1896:  Train loss: 0.0029975075740367174  Test loss: 0.004862543661147356 \n",
      "Epoch: 1/1:  mini-batch 1673/1896:  Train loss: 0.005642716772854328  Test loss: 0.004820921458303928 \n",
      "Epoch: 1/1:  mini-batch 1674/1896:  Train loss: 0.0022306921891868114  Test loss: 0.005026023834943771 \n",
      "Epoch: 1/1:  mini-batch 1675/1896:  Train loss: 0.0018002334982156754  Test loss: 0.005101914517581463 \n",
      "Epoch: 1/1:  mini-batch 1676/1896:  Train loss: 0.006797243375331163  Test loss: 0.004862518049776554 \n",
      "Epoch: 1/1:  mini-batch 1677/1896:  Train loss: 0.0006526741781271994  Test loss: 0.004843803122639656 \n",
      "Epoch: 1/1:  mini-batch 1678/1896:  Train loss: 0.007625189144164324  Test loss: 0.004853090271353722 \n",
      "Epoch: 1/1:  mini-batch 1679/1896:  Train loss: 0.002274448052048683  Test loss: 0.004855145234614611 \n",
      "Epoch: 1/1:  mini-batch 1680/1896:  Train loss: 0.005798876751214266  Test loss: 0.004836387000977993 \n",
      "Epoch: 1/1:  mini-batch 1681/1896:  Train loss: 0.0026550842449069023  Test loss: 0.004801006056368351 \n",
      "Epoch: 1/1:  mini-batch 1682/1896:  Train loss: 0.006989326328039169  Test loss: 0.004790653940290213 \n",
      "Epoch: 1/1:  mini-batch 1683/1896:  Train loss: 0.0065502808429300785  Test loss: 0.004839574918150902 \n",
      "Epoch: 1/1:  mini-batch 1684/1896:  Train loss: 0.003736959770321846  Test loss: 0.004836725071072578 \n",
      "Epoch: 1/1:  mini-batch 1685/1896:  Train loss: 0.0053242277354002  Test loss: 0.004796974360942841 \n",
      "Epoch: 1/1:  mini-batch 1686/1896:  Train loss: 0.004463176243007183  Test loss: 0.004784491844475269 \n",
      "Epoch: 1/1:  mini-batch 1687/1896:  Train loss: 0.004079753998667002  Test loss: 0.004822425544261932 \n",
      "Epoch: 1/1:  mini-batch 1688/1896:  Train loss: 0.006535135209560394  Test loss: 0.004793628118932247 \n",
      "Epoch: 1/1:  mini-batch 1689/1896:  Train loss: 0.0019587052520364523  Test loss: 0.0047895354218780994 \n",
      "Epoch: 1/1:  mini-batch 1690/1896:  Train loss: 0.005940379574894905  Test loss: 0.004827998578548431 \n",
      "Epoch: 1/1:  mini-batch 1691/1896:  Train loss: 0.0074707213789224625  Test loss: 0.00483744079247117 \n",
      "Epoch: 1/1:  mini-batch 1692/1896:  Train loss: 0.00470058573409915  Test loss: 0.004903900437057018 \n",
      "Epoch: 1/1:  mini-batch 1693/1896:  Train loss: 0.005407660733908415  Test loss: 0.005011074244976044 \n",
      "Epoch: 1/1:  mini-batch 1694/1896:  Train loss: 0.003147485665977001  Test loss: 0.004849941004067659 \n",
      "Epoch: 1/1:  mini-batch 1695/1896:  Train loss: 0.0018548163352534175  Test loss: 0.004772739950567484 \n",
      "Epoch: 1/1:  mini-batch 1696/1896:  Train loss: 0.005661812145262957  Test loss: 0.005052851513028145 \n",
      "Epoch: 1/1:  mini-batch 1697/1896:  Train loss: 0.0013882810017094016  Test loss: 0.005313790403306484 \n",
      "Epoch: 1/1:  mini-batch 1698/1896:  Train loss: 0.0030210171826183796  Test loss: 0.005269131623208523 \n",
      "Epoch: 1/1:  mini-batch 1699/1896:  Train loss: 0.005801009014248848  Test loss: 0.004868635442107916 \n",
      "Epoch: 1/1:  mini-batch 1700/1896:  Train loss: 0.0029278493020683527  Test loss: 0.004830602556467056 \n",
      "Epoch: 1/1:  mini-batch 1701/1896:  Train loss: 0.001204577274620533  Test loss: 0.00500493124127388 \n",
      "Epoch: 1/1:  mini-batch 1702/1896:  Train loss: 0.007537652272731066  Test loss: 0.0049097007140517235 \n",
      "Epoch: 1/1:  mini-batch 1703/1896:  Train loss: 0.004360967315733433  Test loss: 0.0047875153832137585 \n",
      "Epoch: 1/1:  mini-batch 1704/1896:  Train loss: 0.0022509670816361904  Test loss: 0.004869788885116577 \n",
      "Epoch: 1/1:  mini-batch 1705/1896:  Train loss: 0.0031499092001467943  Test loss: 0.004918903578072786 \n",
      "Epoch: 1/1:  mini-batch 1706/1896:  Train loss: 0.006421639584004879  Test loss: 0.0051167672500014305 \n",
      "Epoch: 1/1:  mini-batch 1707/1896:  Train loss: 0.002379937097430229  Test loss: 0.005051087588071823 \n",
      "Epoch: 1/1:  mini-batch 1708/1896:  Train loss: 0.006398038938641548  Test loss: 0.004858016036450863 \n",
      "Epoch: 1/1:  mini-batch 1709/1896:  Train loss: 0.005261920392513275  Test loss: 0.004843811504542828 \n",
      "Epoch: 1/1:  mini-batch 1710/1896:  Train loss: 0.005910162348300219  Test loss: 0.005301846191287041 \n",
      "Epoch: 1/1:  mini-batch 1711/1896:  Train loss: 0.00417138310149312  Test loss: 0.00540932547301054 \n",
      "Epoch: 1/1:  mini-batch 1712/1896:  Train loss: 0.005467639770358801  Test loss: 0.004951904993504286 \n",
      "Epoch: 1/1:  mini-batch 1713/1896:  Train loss: 0.004120031371712685  Test loss: 0.004767934791743755 \n",
      "Epoch: 1/1:  mini-batch 1714/1896:  Train loss: 0.0073108552023768425  Test loss: 0.005029920022934675 \n",
      "Epoch: 1/1:  mini-batch 1715/1896:  Train loss: 0.005369206890463829  Test loss: 0.005272700451314449 \n",
      "Epoch: 1/1:  mini-batch 1716/1896:  Train loss: 0.005774206947535276  Test loss: 0.0050270408391952515 \n",
      "Epoch: 1/1:  mini-batch 1717/1896:  Train loss: 0.002564519876614213  Test loss: 0.004791448824107647 \n",
      "Epoch: 1/1:  mini-batch 1718/1896:  Train loss: 0.006344014313071966  Test loss: 0.0049209194257855415 \n",
      "Epoch: 1/1:  mini-batch 1719/1896:  Train loss: 0.002912323921918869  Test loss: 0.005168239586055279 \n",
      "Epoch: 1/1:  mini-batch 1720/1896:  Train loss: 0.0033138308208435774  Test loss: 0.004996003583073616 \n",
      "Epoch: 1/1:  mini-batch 1721/1896:  Train loss: 0.0036696288734674454  Test loss: 0.004771608859300613 \n",
      "Epoch: 1/1:  mini-batch 1722/1896:  Train loss: 0.007169218268245459  Test loss: 0.005513460375368595 \n",
      "Epoch: 1/1:  mini-batch 1723/1896:  Train loss: 0.0050694565288722515  Test loss: 0.005818732548505068 \n",
      "Epoch: 1/1:  mini-batch 1724/1896:  Train loss: 0.003964332863688469  Test loss: 0.005356983281672001 \n",
      "Epoch: 1/1:  mini-batch 1725/1896:  Train loss: 0.00986409280449152  Test loss: 0.0048397500067949295 \n",
      "Epoch: 1/1:  mini-batch 1726/1896:  Train loss: 0.002143803983926773  Test loss: 0.0048401374369859695 \n",
      "Epoch: 1/1:  mini-batch 1727/1896:  Train loss: 0.00415109982714057  Test loss: 0.005027074366807938 \n",
      "Epoch: 1/1:  mini-batch 1728/1896:  Train loss: 0.004085375927388668  Test loss: 0.005035343114286661 \n",
      "Epoch: 1/1:  mini-batch 1729/1896:  Train loss: 0.007116059307008982  Test loss: 0.004866216331720352 \n",
      "Epoch: 1/1:  mini-batch 1730/1896:  Train loss: 0.0023098145611584187  Test loss: 0.00508616678416729 \n",
      "Epoch: 1/1:  mini-batch 1731/1896:  Train loss: 0.005485455505549908  Test loss: 0.005125532392412424 \n",
      "Epoch: 1/1:  mini-batch 1732/1896:  Train loss: 0.0029919391963630915  Test loss: 0.004967944696545601 \n",
      "Epoch: 1/1:  mini-batch 1733/1896:  Train loss: 0.005421385634690523  Test loss: 0.004842843860387802 \n",
      "Epoch: 1/1:  mini-batch 1734/1896:  Train loss: 0.002812698483467102  Test loss: 0.004764108918607235 \n",
      "Epoch: 1/1:  mini-batch 1735/1896:  Train loss: 0.0038951050955802202  Test loss: 0.0048491256311535835 \n",
      "Epoch: 1/1:  mini-batch 1736/1896:  Train loss: 0.0022651338949799538  Test loss: 0.004923536442220211 \n",
      "Epoch: 1/1:  mini-batch 1737/1896:  Train loss: 0.004225794225931168  Test loss: 0.004831405356526375 \n",
      "Epoch: 1/1:  mini-batch 1738/1896:  Train loss: 0.005179696716368198  Test loss: 0.004842826165258884 \n",
      "Epoch: 1/1:  mini-batch 1739/1896:  Train loss: 0.006075060460716486  Test loss: 0.004907493479549885 \n",
      "Epoch: 1/1:  mini-batch 1740/1896:  Train loss: 0.0038605951704084873  Test loss: 0.0048081884160637856 \n",
      "Epoch: 1/1:  mini-batch 1741/1896:  Train loss: 0.007524160202592611  Test loss: 0.004809272475540638 \n",
      "Epoch: 1/1:  mini-batch 1742/1896:  Train loss: 0.007699594367295504  Test loss: 0.005221376195549965 \n",
      "Epoch: 1/1:  mini-batch 1743/1896:  Train loss: 0.0012137176236137748  Test loss: 0.005391787737607956 \n",
      "Epoch: 1/1:  mini-batch 1744/1896:  Train loss: 0.00613895058631897  Test loss: 0.005011156667023897 \n",
      "Epoch: 1/1:  mini-batch 1745/1896:  Train loss: 0.004602578468620777  Test loss: 0.004853260237723589 \n",
      "Epoch: 1/1:  mini-batch 1746/1896:  Train loss: 0.009845511987805367  Test loss: 0.0052701206877827644 \n",
      "Epoch: 1/1:  mini-batch 1747/1896:  Train loss: 0.0050816903822124004  Test loss: 0.005335860885679722 \n",
      "Epoch: 1/1:  mini-batch 1748/1896:  Train loss: 0.0068346112966537476  Test loss: 0.004843190312385559 \n",
      "Epoch: 1/1:  mini-batch 1749/1896:  Train loss: 0.004564553964883089  Test loss: 0.004774757660925388 \n",
      "Epoch: 1/1:  mini-batch 1750/1896:  Train loss: 0.0031426993664354086  Test loss: 0.004933399148285389 \n",
      "Epoch: 1/1:  mini-batch 1751/1896:  Train loss: 0.005397032015025616  Test loss: 0.004917796701192856 \n",
      "Epoch: 1/1:  mini-batch 1752/1896:  Train loss: 0.004689019173383713  Test loss: 0.004852724261581898 \n",
      "Epoch: 1/1:  mini-batch 1753/1896:  Train loss: 0.00583137571811676  Test loss: 0.004771173931658268 \n",
      "Epoch: 1/1:  mini-batch 1754/1896:  Train loss: 0.008188311941921711  Test loss: 0.004778848960995674 \n",
      "Epoch: 1/1:  mini-batch 1755/1896:  Train loss: 0.0026236772537231445  Test loss: 0.004802187439054251 \n",
      "Epoch: 1/1:  mini-batch 1756/1896:  Train loss: 0.005006781779229641  Test loss: 0.004812060389667749 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1757/1896:  Train loss: 0.004862588830292225  Test loss: 0.0048034945502877235 \n",
      "Epoch: 1/1:  mini-batch 1758/1896:  Train loss: 0.0009541392792016268  Test loss: 0.004820345900952816 \n",
      "Epoch: 1/1:  mini-batch 1759/1896:  Train loss: 0.004309709649533033  Test loss: 0.004802390933036804 \n",
      "Epoch: 1/1:  mini-batch 1760/1896:  Train loss: 0.006905294023454189  Test loss: 0.004775945097208023 \n",
      "Epoch: 1/1:  mini-batch 1761/1896:  Train loss: 0.0008986586472019553  Test loss: 0.004787026438862085 \n",
      "Epoch: 1/1:  mini-batch 1762/1896:  Train loss: 0.003372243605554104  Test loss: 0.004773027263581753 \n",
      "Epoch: 1/1:  mini-batch 1763/1896:  Train loss: 0.0054164063185453415  Test loss: 0.004778695292770863 \n",
      "Epoch: 1/1:  mini-batch 1764/1896:  Train loss: 0.0061357468366622925  Test loss: 0.004820522852241993 \n",
      "Epoch: 1/1:  mini-batch 1765/1896:  Train loss: 0.0072736539877951145  Test loss: 0.0047977156937122345 \n",
      "Epoch: 1/1:  mini-batch 1766/1896:  Train loss: 0.003220359794795513  Test loss: 0.004799170419573784 \n",
      "Epoch: 1/1:  mini-batch 1767/1896:  Train loss: 0.0030719898641109467  Test loss: 0.0048474278301000595 \n",
      "Epoch: 1/1:  mini-batch 1768/1896:  Train loss: 0.004263683687895536  Test loss: 0.004831061232835054 \n",
      "Epoch: 1/1:  mini-batch 1769/1896:  Train loss: 0.00480231549590826  Test loss: 0.004768822342157364 \n",
      "Epoch: 1/1:  mini-batch 1770/1896:  Train loss: 0.006601907778531313  Test loss: 0.004778211936354637 \n",
      "Epoch: 1/1:  mini-batch 1771/1896:  Train loss: 0.006078172009438276  Test loss: 0.00479425024241209 \n",
      "Epoch: 1/1:  mini-batch 1772/1896:  Train loss: 0.0045813643373548985  Test loss: 0.004804215393960476 \n",
      "Epoch: 1/1:  mini-batch 1773/1896:  Train loss: 0.005971572361886501  Test loss: 0.004815415944904089 \n",
      "Epoch: 1/1:  mini-batch 1774/1896:  Train loss: 0.004530895967036486  Test loss: 0.004811794031411409 \n",
      "Epoch: 1/1:  mini-batch 1775/1896:  Train loss: 0.0039171380922198296  Test loss: 0.00478022824972868 \n",
      "Epoch: 1/1:  mini-batch 1776/1896:  Train loss: 0.0038916575722396374  Test loss: 0.004912249743938446 \n",
      "Epoch: 1/1:  mini-batch 1777/1896:  Train loss: 0.0019370978698134422  Test loss: 0.005134657025337219 \n",
      "Epoch: 1/1:  mini-batch 1778/1896:  Train loss: 0.004172329790890217  Test loss: 0.005188526585698128 \n",
      "Epoch: 1/1:  mini-batch 1779/1896:  Train loss: 0.003193777985870838  Test loss: 0.004889287985861301 \n",
      "Epoch: 1/1:  mini-batch 1780/1896:  Train loss: 0.0034752015490084887  Test loss: 0.004789586178958416 \n",
      "Epoch: 1/1:  mini-batch 1781/1896:  Train loss: 0.005643189884722233  Test loss: 0.005079265683889389 \n",
      "Epoch: 1/1:  mini-batch 1782/1896:  Train loss: 0.006950215436518192  Test loss: 0.005141842179000378 \n",
      "Epoch: 1/1:  mini-batch 1783/1896:  Train loss: 0.0024587104562669992  Test loss: 0.004912896081805229 \n",
      "Epoch: 1/1:  mini-batch 1784/1896:  Train loss: 0.007521932478994131  Test loss: 0.00478185061365366 \n",
      "Epoch: 1/1:  mini-batch 1785/1896:  Train loss: 0.010143439285457134  Test loss: 0.005193510092794895 \n",
      "Epoch: 1/1:  mini-batch 1786/1896:  Train loss: 0.003905113786458969  Test loss: 0.005472683347761631 \n",
      "Epoch: 1/1:  mini-batch 1787/1896:  Train loss: 0.007895207963883877  Test loss: 0.005197820253670216 \n",
      "Epoch: 1/1:  mini-batch 1788/1896:  Train loss: 0.006608388852328062  Test loss: 0.004794164095073938 \n",
      "Epoch: 1/1:  mini-batch 1789/1896:  Train loss: 0.0062086693942546844  Test loss: 0.004999034106731415 \n",
      "Epoch: 1/1:  mini-batch 1790/1896:  Train loss: 0.005203490145504475  Test loss: 0.005431658588349819 \n",
      "Epoch: 1/1:  mini-batch 1791/1896:  Train loss: 0.0030609758105129004  Test loss: 0.005336865782737732 \n",
      "Epoch: 1/1:  mini-batch 1792/1896:  Train loss: 0.0049572899006307125  Test loss: 0.004818360786885023 \n",
      "Epoch: 1/1:  mini-batch 1793/1896:  Train loss: 0.009647984988987446  Test loss: 0.004882564768195152 \n",
      "Epoch: 1/1:  mini-batch 1794/1896:  Train loss: 0.004547644406557083  Test loss: 0.0053401365876197815 \n",
      "Epoch: 1/1:  mini-batch 1795/1896:  Train loss: 0.0014651839155703783  Test loss: 0.005493453703820705 \n",
      "Epoch: 1/1:  mini-batch 1796/1896:  Train loss: 0.006796245463192463  Test loss: 0.004966394044458866 \n",
      "Epoch: 1/1:  mini-batch 1797/1896:  Train loss: 0.00235767406411469  Test loss: 0.004812001716345549 \n",
      "Epoch: 1/1:  mini-batch 1798/1896:  Train loss: 0.006976783275604248  Test loss: 0.005221846513450146 \n",
      "Epoch: 1/1:  mini-batch 1799/1896:  Train loss: 0.0024954804684966803  Test loss: 0.005236154422163963 \n",
      "Epoch: 1/1:  mini-batch 1800/1896:  Train loss: 0.005895094946026802  Test loss: 0.004818071145564318 \n",
      "Epoch: 1/1:  mini-batch 1801/1896:  Train loss: 0.0025480298791080713  Test loss: 0.004845104180276394 \n",
      "Epoch: 1/1:  mini-batch 1802/1896:  Train loss: 0.0043334318324923515  Test loss: 0.004986203275620937 \n",
      "Epoch: 1/1:  mini-batch 1803/1896:  Train loss: 0.00784512422978878  Test loss: 0.005143800750374794 \n",
      "Epoch: 1/1:  mini-batch 1804/1896:  Train loss: 0.004334121942520142  Test loss: 0.004883004352450371 \n",
      "Epoch: 1/1:  mini-batch 1805/1896:  Train loss: 0.0063000889495015144  Test loss: 0.004765229765325785 \n",
      "Epoch: 1/1:  mini-batch 1806/1896:  Train loss: 0.0027921281289309263  Test loss: 0.00485636480152607 \n",
      "Epoch: 1/1:  mini-batch 1807/1896:  Train loss: 0.002835901454091072  Test loss: 0.004873728379607201 \n",
      "Epoch: 1/1:  mini-batch 1808/1896:  Train loss: 0.006555395666509867  Test loss: 0.004792618099600077 \n",
      "Epoch: 1/1:  mini-batch 1809/1896:  Train loss: 0.006163950078189373  Test loss: 0.004781249910593033 \n",
      "Epoch: 1/1:  mini-batch 1810/1896:  Train loss: 0.004446408245712519  Test loss: 0.004801148548722267 \n",
      "Epoch: 1/1:  mini-batch 1811/1896:  Train loss: 0.0012411823263391852  Test loss: 0.004806779325008392 \n",
      "Epoch: 1/1:  mini-batch 1812/1896:  Train loss: 0.0032231463119387627  Test loss: 0.004830441437661648 \n",
      "Epoch: 1/1:  mini-batch 1813/1896:  Train loss: 0.0022301736753433943  Test loss: 0.004856940358877182 \n",
      "Epoch: 1/1:  mini-batch 1814/1896:  Train loss: 0.0036184072960168123  Test loss: 0.00481764879077673 \n",
      "Epoch: 1/1:  mini-batch 1815/1896:  Train loss: 0.003923551645129919  Test loss: 0.004818860441446304 \n",
      "Epoch: 1/1:  mini-batch 1816/1896:  Train loss: 0.001964792376384139  Test loss: 0.004831144120544195 \n",
      "Epoch: 1/1:  mini-batch 1817/1896:  Train loss: 0.005935118068009615  Test loss: 0.004891301970928907 \n",
      "Epoch: 1/1:  mini-batch 1818/1896:  Train loss: 0.0064155058935284615  Test loss: 0.004894009325653315 \n",
      "Epoch: 1/1:  mini-batch 1819/1896:  Train loss: 0.00508832884952426  Test loss: 0.0048961341381073 \n",
      "Epoch: 1/1:  mini-batch 1820/1896:  Train loss: 0.005625766236335039  Test loss: 0.004788625985383987 \n",
      "Epoch: 1/1:  mini-batch 1821/1896:  Train loss: 0.004799888469278812  Test loss: 0.004858320578932762 \n",
      "Epoch: 1/1:  mini-batch 1822/1896:  Train loss: 0.003435363294556737  Test loss: 0.004963034763932228 \n",
      "Epoch: 1/1:  mini-batch 1823/1896:  Train loss: 0.002741957316175103  Test loss: 0.00498201185837388 \n",
      "Epoch: 1/1:  mini-batch 1824/1896:  Train loss: 0.0008596144616603851  Test loss: 0.004877680446952581 \n",
      "Epoch: 1/1:  mini-batch 1825/1896:  Train loss: 0.0044160326942801476  Test loss: 0.004821166396141052 \n",
      "Epoch: 1/1:  mini-batch 1826/1896:  Train loss: 0.0021585903596132994  Test loss: 0.0048714978620409966 \n",
      "Epoch: 1/1:  mini-batch 1827/1896:  Train loss: 0.0034986226819455624  Test loss: 0.0048223864287137985 \n",
      "Epoch: 1/1:  mini-batch 1828/1896:  Train loss: 0.004265858791768551  Test loss: 0.004802688956260681 \n",
      "Epoch: 1/1:  mini-batch 1829/1896:  Train loss: 0.002288662828505039  Test loss: 0.004770987667143345 \n",
      "Epoch: 1/1:  mini-batch 1830/1896:  Train loss: 0.0014121427666395903  Test loss: 0.004798954352736473 \n",
      "Epoch: 1/1:  mini-batch 1831/1896:  Train loss: 0.00803998950868845  Test loss: 0.0049842046573758125 \n",
      "Epoch: 1/1:  mini-batch 1832/1896:  Train loss: 0.0016317949630320072  Test loss: 0.0049645379185676575 \n",
      "Epoch: 1/1:  mini-batch 1833/1896:  Train loss: 0.0024099252186715603  Test loss: 0.004827557131648064 \n",
      "Epoch: 1/1:  mini-batch 1834/1896:  Train loss: 0.0040431576780974865  Test loss: 0.004784822929650545 \n",
      "Epoch: 1/1:  mini-batch 1835/1896:  Train loss: 0.0031435759738087654  Test loss: 0.004905986599624157 \n",
      "Epoch: 1/1:  mini-batch 1836/1896:  Train loss: 0.0020731010008603334  Test loss: 0.0049326177686452866 \n",
      "Epoch: 1/1:  mini-batch 1837/1896:  Train loss: 0.00605401024222374  Test loss: 0.0048408289439976215 \n",
      "Epoch: 1/1:  mini-batch 1838/1896:  Train loss: 0.003026386024430394  Test loss: 0.004790334030985832 \n",
      "Epoch: 1/1:  mini-batch 1839/1896:  Train loss: 0.004541371483355761  Test loss: 0.004877293482422829 \n",
      "Epoch: 1/1:  mini-batch 1840/1896:  Train loss: 0.00438101077452302  Test loss: 0.004936494864523411 \n",
      "Epoch: 1/1:  mini-batch 1841/1896:  Train loss: 0.004887649789452553  Test loss: 0.0048593031242489815 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1:  mini-batch 1842/1896:  Train loss: 0.002020345302298665  Test loss: 0.004784177057445049 \n",
      "Epoch: 1/1:  mini-batch 1843/1896:  Train loss: 0.002373423660174012  Test loss: 0.004763449542224407 \n",
      "Epoch: 1/1:  mini-batch 1844/1896:  Train loss: 0.002376907505095005  Test loss: 0.00478415098041296 \n",
      "Epoch: 1/1:  mini-batch 1845/1896:  Train loss: 0.0009389137267135084  Test loss: 0.004836788401007652 \n",
      "Epoch: 1/1:  mini-batch 1846/1896:  Train loss: 0.0074425991624593735  Test loss: 0.004935845732688904 \n",
      "Epoch: 1/1:  mini-batch 1847/1896:  Train loss: 0.0045584095641970634  Test loss: 0.004886028356850147 \n",
      "Epoch: 1/1:  mini-batch 1848/1896:  Train loss: 0.0034998999908566475  Test loss: 0.004767477512359619 \n",
      "Epoch: 1/1:  mini-batch 1849/1896:  Train loss: 0.005005964543670416  Test loss: 0.004781232215464115 \n",
      "Epoch: 1/1:  mini-batch 1850/1896:  Train loss: 0.0072241295129060745  Test loss: 0.004818407818675041 \n",
      "Epoch: 1/1:  mini-batch 1851/1896:  Train loss: 0.00379864196293056  Test loss: 0.0048005604185163975 \n",
      "Epoch: 1/1:  mini-batch 1852/1896:  Train loss: 0.004579375497996807  Test loss: 0.00482467096298933 \n",
      "Epoch: 1/1:  mini-batch 1853/1896:  Train loss: 0.002290359465405345  Test loss: 0.004843366798013449 \n",
      "Epoch: 1/1:  mini-batch 1854/1896:  Train loss: 0.004077138379216194  Test loss: 0.004797539673745632 \n",
      "Epoch: 1/1:  mini-batch 1855/1896:  Train loss: 0.010080722160637379  Test loss: 0.004763365723192692 \n",
      "Epoch: 1/1:  mini-batch 1856/1896:  Train loss: 0.0036555486731231213  Test loss: 0.004769532475620508 \n",
      "Epoch: 1/1:  mini-batch 1857/1896:  Train loss: 0.001746955793350935  Test loss: 0.004766556434333324 \n",
      "Epoch: 1/1:  mini-batch 1858/1896:  Train loss: 0.007954507134854794  Test loss: 0.004823843948543072 \n",
      "Epoch: 1/1:  mini-batch 1859/1896:  Train loss: 0.0012978828744962811  Test loss: 0.004826340824365616 \n",
      "Epoch: 1/1:  mini-batch 1860/1896:  Train loss: 0.0033094300888478756  Test loss: 0.004803810268640518 \n",
      "Epoch: 1/1:  mini-batch 1861/1896:  Train loss: 0.006315043196082115  Test loss: 0.004860064946115017 \n",
      "Epoch: 1/1:  mini-batch 1862/1896:  Train loss: 0.006455093622207642  Test loss: 0.004832798615098 \n",
      "Epoch: 1/1:  mini-batch 1863/1896:  Train loss: 0.004103395156562328  Test loss: 0.00480908015742898 \n",
      "Epoch: 1/1:  mini-batch 1864/1896:  Train loss: 0.009572142735123634  Test loss: 0.004780844785273075 \n",
      "Epoch: 1/1:  mini-batch 1865/1896:  Train loss: 0.002036204095929861  Test loss: 0.004787207581102848 \n",
      "Epoch: 1/1:  mini-batch 1866/1896:  Train loss: 0.0049946666695177555  Test loss: 0.004822018556296825 \n",
      "Epoch: 1/1:  mini-batch 1867/1896:  Train loss: 0.001814289833419025  Test loss: 0.004794443026185036 \n",
      "Epoch: 1/1:  mini-batch 1868/1896:  Train loss: 0.004950108006596565  Test loss: 0.004804976284503937 \n",
      "Epoch: 1/1:  mini-batch 1869/1896:  Train loss: 0.0026241212617605925  Test loss: 0.004814441315829754 \n",
      "Epoch: 1/1:  mini-batch 1870/1896:  Train loss: 0.002083522966131568  Test loss: 0.00481061264872551 \n",
      "Epoch: 1/1:  mini-batch 1871/1896:  Train loss: 0.003456204431131482  Test loss: 0.004880728665739298 \n",
      "Epoch: 1/1:  mini-batch 1872/1896:  Train loss: 0.004989878740161657  Test loss: 0.004980971105396748 \n",
      "Epoch: 1/1:  mini-batch 1873/1896:  Train loss: 0.00437608128413558  Test loss: 0.004811159335076809 \n",
      "Epoch: 1/1:  mini-batch 1874/1896:  Train loss: 0.0022246851585805416  Test loss: 0.0048225936479866505 \n",
      "Epoch: 1/1:  mini-batch 1875/1896:  Train loss: 0.0011734329164028168  Test loss: 0.004937958903610706 \n",
      "Epoch: 1/1:  mini-batch 1876/1896:  Train loss: 0.0034065088257193565  Test loss: 0.004862253554165363 \n",
      "Epoch: 1/1:  mini-batch 1877/1896:  Train loss: 0.0064702508971095085  Test loss: 0.004798281006515026 \n",
      "Epoch: 1/1:  mini-batch 1878/1896:  Train loss: 0.0044261799193918705  Test loss: 0.004883699119091034 \n",
      "Epoch: 1/1:  mini-batch 1879/1896:  Train loss: 0.004836458712816238  Test loss: 0.004973122850060463 \n",
      "Epoch: 1/1:  mini-batch 1880/1896:  Train loss: 0.0009607668034732342  Test loss: 0.004905326291918755 \n",
      "Epoch: 1/1:  mini-batch 1881/1896:  Train loss: 0.0051905810832977295  Test loss: 0.004780533257871866 \n",
      "Epoch: 1/1:  mini-batch 1882/1896:  Train loss: 0.00316393980756402  Test loss: 0.004794342443346977 \n",
      "Epoch: 1/1:  mini-batch 1883/1896:  Train loss: 0.004023945890367031  Test loss: 0.004870901349931955 \n",
      "Epoch: 1/1:  mini-batch 1884/1896:  Train loss: 0.0018625643569976091  Test loss: 0.004852965474128723 \n",
      "Epoch: 1/1:  mini-batch 1885/1896:  Train loss: 0.005149805918335915  Test loss: 0.004762772470712662 \n",
      "Epoch: 1/1:  mini-batch 1886/1896:  Train loss: 0.00444773118942976  Test loss: 0.004903649911284447 \n",
      "Epoch: 1/1:  mini-batch 1887/1896:  Train loss: 0.0065140500664711  Test loss: 0.005395512096583843 \n",
      "Epoch: 1/1:  mini-batch 1888/1896:  Train loss: 0.005308156833052635  Test loss: 0.005367989651858807 \n",
      "Epoch: 1/1:  mini-batch 1889/1896:  Train loss: 0.005738290026783943  Test loss: 0.004822331480681896 \n",
      "Epoch: 1/1:  mini-batch 1890/1896:  Train loss: 0.004481471609324217  Test loss: 0.0050815497525036335 \n",
      "Epoch: 1/1:  mini-batch 1891/1896:  Train loss: 0.005438261199742556  Test loss: 0.005823313258588314 \n",
      "Epoch: 1/1:  mini-batch 1892/1896:  Train loss: 0.0033114003017544746  Test loss: 0.005521751474589109 \n",
      "Epoch: 1/1:  mini-batch 1893/1896:  Train loss: 0.004140349105000496  Test loss: 0.004838463384658098 \n",
      "Epoch: 1/1:  mini-batch 1894/1896:  Train loss: 0.0034684636630117893  Test loss: 0.004987460095435381 \n",
      "Epoch: 1/1:  mini-batch 1895/1896:  Train loss: 0.006104800850152969  Test loss: 0.005641432013362646 \n",
      "Epoch: 1/1:  mini-batch 1896/1896:  Train loss: 0.0039028893224895  Test loss: 0.005611116997897625 \n"
     ]
    }
   ],
   "source": [
    "completed = 0\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "ended = False\n",
    "epoch = 0\n",
    "sess = tf.Session()\n",
    "# Initialize variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "while (epoch < epochs) and not(ended):\n",
    "    completed = int(epoch * 100/epochs)\n",
    "    if completed >= perc:\n",
    "        print(str(perc) + \" % completed\")\n",
    "        perc = int(epoch * 100/epochs)\n",
    "\n",
    "    batch_completed = 0\n",
    "    mini_batch = 1\n",
    "    mini_batch_train_losses = []\n",
    "    mini_batch_test_losses = []\n",
    "    while batch_completed < (len(X_train) - batch_size):\n",
    "        train_X = X_train[batch_completed:(batch_completed + batch_size)]\n",
    "        train_Y = Y_train[batch_completed:(batch_completed + batch_size)]\n",
    "        loss = train(sess, mlp, train_X, train_Y)\n",
    "        # Runs out of memory while evaluating on the complete test set. Only batch_size used for evaluating\n",
    "        # This step should be randomized\n",
    "        test_loss = get_loss(sess, mlp, X_test[0:batch_size], Y_test[0:batch_size])\n",
    "        print('Epoch: {}/{}: '.format(epoch+1, epochs), 'mini-batch {}/{}: '.format(mini_batch, num_batches), \"Train loss: {} \".format(loss), \"Test loss: {} \".format(test_loss))\n",
    "        batch_completed += batch_size\n",
    "        mini_batch += 1\n",
    "        mini_batch_train_losses.append(loss)\n",
    "        mini_batch_test_losses.append(test_loss)\n",
    "    epoch_train_losses.append(sum(mini_batch_train_losses)/len(mini_batch_train_losses))\n",
    "    epoch_test_losses.append(sum(mini_batch_test_losses)/len(mini_batch_test_losses))\n",
    "    # Early stopping check (callback should be used instead):\n",
    "    if (epoch > 0) and (epoch_test_losses[epoch] > epoch_test_losses[epoch - 1]):\n",
    "        saver.save(sess, \"supervised_feedfoward_aircraft_center_continuous\")\n",
    "        ended = True\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'supervised_feedfoward_aircraft_center_continuous'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"supervised_feedfoward_aircraft_center_continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.00000e+00, 1.00000e+00, 0.00000e+00, 5.00000e+00, 5.00000e+00,\n",
       "        3.00000e+00, 2.00000e+00, 8.00000e+00, 9.00000e+00, 2.10000e+01,\n",
       "        5.20000e+01, 8.70000e+01, 1.63000e+02, 3.04000e+02, 7.35000e+02,\n",
       "        1.21400e+03, 4.97600e+03, 2.05526e+05, 2.49140e+04, 3.13400e+03,\n",
       "        6.86000e+02, 3.44000e+02, 1.67000e+02, 1.19000e+02, 6.60000e+01,\n",
       "        4.90000e+01, 3.40000e+01, 1.90000e+01, 1.70000e+01, 6.00000e+00,\n",
       "        1.30000e+01, 4.00000e+00, 2.00000e+00, 1.00000e+00, 2.00000e+00,\n",
       "        2.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00, 1.00000e+00]),\n",
       " array([0.   , 0.025, 0.05 , 0.075, 0.1  , 0.125, 0.15 , 0.175, 0.2  ,\n",
       "        0.225, 0.25 , 0.275, 0.3  , 0.325, 0.35 , 0.375, 0.4  , 0.425,\n",
       "        0.45 , 0.475, 0.5  , 0.525, 0.55 , 0.575, 0.6  , 0.625, 0.65 ,\n",
       "        0.675, 0.7  , 0.725, 0.75 , 0.775, 0.8  , 0.825, 0.85 , 0.875,\n",
       "        0.9  , 0.925, 0.95 , 0.975, 1.   ]),\n",
       " <a list of 40 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_train[:, 0], bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000e+00, 0.00000e+00, 2.00000e+00, 3.00000e+00, 3.00000e+00,\n",
       "        3.00000e+00, 6.00000e+00, 1.30000e+01, 9.00000e+00, 1.60000e+01,\n",
       "        1.70000e+01, 1.70000e+01, 3.50000e+01, 3.30000e+01, 7.90000e+01,\n",
       "        1.08000e+02, 2.09000e+02, 2.49000e+02, 5.31700e+03, 5.41520e+04,\n",
       "        1.76315e+05, 5.72600e+03, 1.19000e+02, 4.80000e+01, 4.10000e+01,\n",
       "        2.50000e+01, 3.20000e+01, 2.20000e+01, 2.00000e+01, 1.40000e+01,\n",
       "        1.20000e+01, 7.00000e+00, 1.60000e+01, 8.00000e+00, 8.00000e+00,\n",
       "        6.00000e+00, 3.00000e+00, 1.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([0.00534759, 0.0302139 , 0.05508021, 0.07994652, 0.10481283,\n",
       "        0.12967914, 0.15454545, 0.17941176, 0.20427807, 0.22914439,\n",
       "        0.2540107 , 0.27887701, 0.30374332, 0.32860963, 0.35347594,\n",
       "        0.37834225, 0.40320856, 0.42807487, 0.45294118, 0.47780749,\n",
       "        0.5026738 , 0.52754011, 0.55240642, 0.57727273, 0.60213904,\n",
       "        0.62700535, 0.65187166, 0.67673797, 0.70160428, 0.72647059,\n",
       "        0.7513369 , 0.77620321, 0.80106952, 0.82593583, 0.85080214,\n",
       "        0.87566845, 0.90053476, 0.92540107, 0.95026738, 0.97513369,\n",
       "        1.        ]),\n",
       " <a list of 40 Patch objects>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_train[:, 1], bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.94000e+02, 2.41000e+02, 2.76000e+02, 1.81000e+02, 1.43000e+02,\n",
       "        1.11000e+02, 8.30000e+01, 8.00000e+01, 9.30000e+01, 1.66000e+02,\n",
       "        1.35000e+02, 1.16000e+02, 1.72000e+02, 2.20000e+02, 4.09000e+02,\n",
       "        6.24000e+02, 1.03700e+03, 2.12700e+03, 5.38700e+03, 6.74050e+04,\n",
       "        1.49206e+05, 6.14700e+03, 2.59400e+03, 1.37700e+03, 8.20000e+02,\n",
       "        4.34000e+02, 2.55000e+02, 1.94000e+02, 1.34000e+02, 1.44000e+02,\n",
       "        1.23000e+02, 1.21000e+02, 1.10000e+02, 1.14000e+02, 1.32000e+02,\n",
       "        2.13000e+02, 2.75000e+02, 3.56000e+02, 3.46000e+02, 3.01000e+02]),\n",
       " array([0.   , 0.025, 0.05 , 0.075, 0.1  , 0.125, 0.15 , 0.175, 0.2  ,\n",
       "        0.225, 0.25 , 0.275, 0.3  , 0.325, 0.35 , 0.375, 0.4  , 0.425,\n",
       "        0.45 , 0.475, 0.5  , 0.525, 0.55 , 0.575, 0.6  , 0.625, 0.65 ,\n",
       "        0.675, 0.7  , 0.725, 0.75 , 0.775, 0.8  , 0.825, 0.85 , 0.875,\n",
       "        0.9  , 0.925, 0.95 , 0.975, 1.   ]),\n",
       " <a list of 40 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Y_train[:, 2], bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4466704 , 0.50179165, 0.50129693])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.05163017e-01, -8.24806288e+01,  1.62931364e-02])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_train * (y_max - y_min) + y_min, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ts_start and ts_end are integers: minute index\n",
    "state_dim = [2 * half_x_length + 1, 2 * half_y_length + 1, 2 * half_z_length + 1]\n",
    "def test_timestep_predictions(ts_start, ts_end):\n",
    "    X, Y = get_X_Y(flight_data, [i for i in range(0, 11)])\n",
    "    keys = list(X.keys())\n",
    "    X = [X[key] for key in keys]\n",
    "    X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "    Y = [Y[key] for key in keys]\n",
    "    Y = np.array(Y)\n",
    "    Y_actual = ((np.array(Y) - y_min)/(y_max - y_min))\n",
    "    X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "    Y_pred = get_predicted_action(sess, mlp, X)\n",
    "    Y_pred_transformed = Y_pred * (y_max - y_min) + y_min\n",
    "    loss = get_loss(sess, mlp, X, Y_actual)\n",
    "    Y_pred = {keys[i]: Y_pred[i, :] for i in range(len(keys))}\n",
    "    Y_pred_transformed = {keys[i]: Y_pred_transformed[i, :] for i in range(len(keys))}\n",
    "    return Y_pred, Y_pred_transformed, loss\n",
    "\n",
    "Y_pred, Y_pred_transformed, loss = test_timestep_predictions(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (1, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (2, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (3, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (4, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (5, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (6, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (6, 'AB128D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (7, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (7, 'AA1949_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (7, 'AB128D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A0312D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A04E0E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A6455D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A88498_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'A9ADC0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'AA1949_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'AB128D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (8, 'ABA574_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '345115_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '394A09_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '39BD20_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '3C656E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '3C65A4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '400551_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '400610_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '400614_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '4006B0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '4243FE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '424575_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '440360_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '45AC4D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '484368_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '4B1883_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '738041_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '780A15_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, '896187_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A002AF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A004C4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A0275C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A0312D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A04E0E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A07341_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A0817C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A09F37_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A0A8DF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A0B888_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A10C4E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A11005_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A11D3C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A146F9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A14B1A_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A18EEF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A19300_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A1EDE9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A1F19B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A26B80_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A26FC9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A2C44B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A2C8B5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A2DCEE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A2FBF1_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A3183D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A320EF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A336F0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A33B52_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A3AD6D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A3E9D1_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A411DF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A414E8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A437CE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A45ADD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A55090_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A584C4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A5EA5C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A6049B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A63F8B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A6455D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A654CB_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A65C7B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A66239_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A68DAF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A69988_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A6A3C2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A6A4CB_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A6AB27_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A70EC9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A72B6B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A7BB1F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A7F6B0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A801D3_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A83564_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A838E2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A84548_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A85FF5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A86A28_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A8847F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A88498_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A90EA8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A923A7_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A923A9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A92DDF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A97DE4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A98CBC_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'A9ADC0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AA1949_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AAE3AD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AAF1FA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB0AD7_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB128D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB2BA3_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB398A_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB5502_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB6CA2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB6CBD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB6FDD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB83E8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB8FD0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB9B0D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AB9BAA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ABA574_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ABA7FA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ABA800_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ABC6E0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AC135F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AC832E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ACBA1B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ACD296_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AD4804_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AD4F05_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AD566C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AD6922_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AD6BDC_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ADB782_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ADCA15_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'ADEE2F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'AE4F13_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C021FD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C04864_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C049E9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C060B5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C065A4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (9, 'C06B01_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '345115_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '394A09_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '39BD20_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '3C656E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '3C65A4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '400551_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '400610_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '400614_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '4006B0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '42417C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '4243FE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '424575_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '440360_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '45AC4D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '484368_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '4B1883_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '738041_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '780A15_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, '896187_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A002AF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A004C4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A02443_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0275C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0312D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A04E0E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A07341_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0817C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A09F37_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0A8DF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0B888_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A0C6D9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A10C4E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A11005_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A11D3C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A146F9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A14B1A_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A18EEF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A19300_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A1EDE9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A1F19B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A26B80_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A26FC9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A2C094_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A2C44B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A2C8B5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A2DCEE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A2FBF1_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A3183D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A320EF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A336F0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A33B52_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A36659_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A3AD6D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A3E9D1_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A411DF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A414E8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A437CE_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A45ADD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A537ED_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A55090_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A584C4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A5EA5C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A6049B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A63F8B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A6455D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A654CB_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A65C7B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A66239_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A68DAF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A69988_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A6A3C2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A6A4CB_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A6AB27_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A70EC9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A72B6B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A79F25_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A7BB1F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A7F6B0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A801D3_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A83564_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A838E2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A84548_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A85FF5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A86A28_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A8847F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A88498_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A90EA8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A923A7_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A923A9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A92DDF_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A97DE4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A98CBC_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A9ADC0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'A9C5C5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AA1949_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AAE3AD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AAF1FA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB0AD7_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB128D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB2BA3_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB398A_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB5502_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB6CA2_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB6CBD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB6FDD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB83E8_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB8FD0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB9B0D_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AB9BAA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ABA574_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ABA7FA_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ABA800_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ABC6E0_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AC135F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AC22C5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AC832E_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ACBA1B_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ACD296_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AD4804_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AD4F05_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AD566C_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AD6922_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AD6BDC_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ADB782_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ADCA15_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'ADEE2F_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'AE4F13_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C021FD_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C04864_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C049E9_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C060B5_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C065A4_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32),\n",
       " (10, 'C06B01_0'): array([0.43952274, 0.49671006, 0.47517362], dtype=float32)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_timestep_predictions(ts_start, ts_end):\n",
    "#     X, Y = get_X_Y([ts_start])\n",
    "#     keys = list(X.keys())\n",
    "#     X = [X[key] for key in keys]\n",
    "#     X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "#     Y = [Y[key] for key in keys]\n",
    "#     Y = np.array(Y)\n",
    "#     Y_actual = ((np.array(Y) - y_min)/(y_max - y_min))\n",
    "#     X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "#     Y_pred = get_predicted_action(sess, mlp, X)\n",
    "#     Y_pred_transformed = Y_pred * (y_max - y_min) + y_min\n",
    "#     loss = get_loss(sess, mlp, X, Y_actual)\n",
    "#     Y_pred = {keys[i]: Y_pred[i, :] for i in range(len(keys))}\n",
    "#     Y_pred_transformed = {keys[i]: Y_pred_transformed[i, :] for i in range(len(keys))}\n",
    "#     ts_df = flight_data[flight_data['ts'] == ts_start]\n",
    "#     for key in Y_pred_transformed.keys():\n",
    "#         distance = ts_df[ts_df['id'] == key[1]]['speed'] * 60 * 4.63 / 9 # metres\n",
    "#         lat, lon = destination(ts_df[ts_df['id'] == key[1]]['lat'], ts_df[ts_df['id'] == key[1]]['lon'], ts_df[ts_df['id'] == key[1]]['azimuth'], distance)\n",
    "#         speed = ts_df[ts_df['id'] == key[1]]['speed'] + Y_pred_transformed[key][0]\n",
    "#         altitude = ts_df[ts_df['id'] == key[1]]['altitude'] + Y_pred_transformed[key][1]\n",
    "#         heading = ts_df[ts_df['id'] == key[1]]['heading'] + Y_pred_transformed[key][2]\n",
    "\n",
    "# Y_pred, Y_pred_transformed_loss = simulate_timestep_predictions(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_timestep_predictions(flight_data, ts_start, ts_end):\n",
    "    for ts in range(ts_start, ts_end + 1):\n",
    "        X, Y = get_X_Y(flight_data, [ts])\n",
    "        keys = list(X.keys())\n",
    "        X = [X[key] for key in keys]\n",
    "        X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "        Y = [Y[key] for key in keys]\n",
    "        Y = np.array(Y)\n",
    "        Y_actual = ((np.array(Y) - y_min)/(y_max - y_min))\n",
    "        X = np.concatenate([x.reshape((1, 8250)) for x in X], axis = 0)\n",
    "        Y_pred = get_predicted_action(sess, mlp, X)\n",
    "        Y_pred_transformed = Y_pred * (y_max - y_min) + y_min\n",
    "        loss = get_loss(sess, mlp, X, Y_actual)\n",
    "        Y_pred = {keys[i]: Y_pred[i, :] for i in range(len(keys))}\n",
    "        Y_pred_transformed = {keys[i]: Y_pred_transformed[i, :] for i in range(len(keys))}\n",
    "        ts_df = flight_data[flight_data['ts'] == ts]\n",
    "        # if ts == ts_start:\n",
    "        for key in Y_pred_transformed.keys():\n",
    "            distance = ts_df[ts_df['id'] == key[1]]['ground_speed'].iloc[0] * 60 * 4.63 / 9 # metres\n",
    "            lat, lon = destination(ts_df[ts_df['id'] == key[1]]['lat'].iloc[0], ts_df[ts_df['id'] == key[1]]['lon'].iloc[0], ts_df[ts_df['id'] == key[1]]['azimuth'].iloc[0], distance)\n",
    "            speed = ts_df[ts_df['id'] == key[1]]['ground_speed'].iloc[0] + Y_pred_transformed[key][0]\n",
    "            altitude = ts_df[ts_df['id'] == key[1]]['altitude'].iloc[0] + Y_pred_transformed[key][1]\n",
    "            heading = ts_df[ts_df['id'] == key[1]]['azimuth'].iloc[0] + Y_pred_transformed[key][2]\n",
    "            \n",
    "            # Updating values of next time steps\n",
    "            flight_data.loc[(flight_data['ts'] == (key[0]+1)) & (flight_data['id'] == key[1]), 'ground_speed'] = speed\n",
    "            flight_data.loc[(flight_data['ts'] == (key[0]+1)) & (flight_data['id'] == key[1]), 'altitude'] = altitude\n",
    "            flight_data.loc[(flight_data['ts'] == (key[0]+1)) & (flight_data['id'] == key[1]), 'azimuth'] = (heading % (2*math.pi))\n",
    "            flight_data.loc[(flight_data['ts'] == (key[0]+1)) & (flight_data['id'] == key[1]), 'lat'] = lat\n",
    "            flight_data.loc[(flight_data['ts'] == (key[0]+1)) & (flight_data['id'] == key[1]), 'lon'] = lon\n",
    "        \n",
    "        flight_data['x'] = (x_length * (flight_data['lat'] - lat_min - 0.0001)/(lat_max - lat_min)).apply(math.ceil)\n",
    "        flight_data['y'] = (y_length * (flight_data['lon'] - lon_min - 0.0001)/(lon_max - lon_min)).apply(math.ceil)\n",
    "        flight_data['z'] = (flight_data['altitude']/alt_bucket_range).apply(math.floor)\n",
    "        # Dead aircraft: out of x range, y range or z range\n",
    "        exit_aircraft = flight_data[(flight_data['ts'] == (ts+1)) & ((flight_data['x'] < 0) | (flight_data['x'] >= x_length) | (flight_data['y'] < 0) | (flight_data['y'] >= y_length))]\n",
    "        crashed = flight_data[(flight_data['ts'] == (ts+1)) & (flight_data['z'] < 0) | (flight_data['z'] >= z_length) | (flight_data['altitude'] < 0) & ((flight_data['lat'] < 40) | (flight_data['lat'] > 41.25) | (flight_data['lon'] < -74.375) | (flight_data['lon'] > -73.125))]\n",
    "        remove_ids = set(exit_aircraft['id'].tolist() + crashed['id'].tolist())\n",
    "        flight_data = flight_data[flight_data['id'].apply(lambda x: not(x in remove_ids))]\n",
    "        print((ts, exit_aircraft.shape[0], crashed.shape[0]))\n",
    "\n",
    "\n",
    "# else:\n",
    "#     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/pandas-0.24.2-py3.7-linux-x86_64.egg/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "/home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/atc/anaconda3/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 0, 0)\n",
      "(2, 0, 0)\n",
      "(3, 0, 0)\n",
      "(4, 0, 0)\n",
      "(5, 0, 0)\n",
      "(6, 0, 0)\n",
      "(7, 0, 0)\n",
      "(8, 2, 2)\n",
      "(9, 0, 5)\n",
      "(10, 0, 6)\n"
     ]
    }
   ],
   "source": [
    "simulate_timestep_predictions(flight_data, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- All predictions correspond to average (almost no action)\n",
    "- Forecasting using the model leads to several aircraft crashes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
